<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://miykael.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://miykael.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-10-02T16:25:52+00:00</updated><id>https://miykael.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal homepage of Michael P. Notter.
</subtitle><entry><title type="html">Advanced Deep Learning - Custom Neural Networks with TensorFlow</title><link href="https://miykael.github.io/blog/2023/04_tensorflow_advanced/" rel="alternate" type="text/html" title="Advanced Deep Learning - Custom Neural Networks with TensorFlow" /><published>2023-10-23T15:00:00+00:00</published><updated>2023-10-23T15:00:00+00:00</updated><id>https://miykael.github.io/blog/2023/04_tensorflow_advanced</id><content type="html" xml:base="https://miykael.github.io/blog/2023/04_tensorflow_advanced/"><![CDATA[<p>In this final part of our series, we’ll explore advanced TensorFlow concepts by building a sophisticated regression model. While Part 2 introduced basic neural networks for classification, we’ll now tackle regression and demonstrate TensorFlow’s powerful features for model customization and optimization. However, as in part 3, the purpose of this tutorial is to highlight the flexibility and capabilities of TensorFlow. Therefore, this showcase is mostly about introducing you to those advanced routines and not about how to create the best regression model.</p>

<p>The complete code for this tutorial can be found in the <a href="/assets/scripts/04_tensorflow_advanced.py">04_tensorflow_advanced.py</a> script.</p>

<h3 id="why-advanced-neural-networks">Why Advanced Neural Networks?</h3>

<p>Complex real-world problems often require:</p>
<ul>
  <li>Custom model architectures</li>
  <li>Advanced optimization strategies</li>
  <li>Robust training procedures</li>
  <li>Model performance monitoring</li>
</ul>

<p>TensorFlow provides all these capabilities, and we’ll learn how to use them effectively.</p>

<p>As always, first, let’s import the scientific Python packages we need.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</code></pre></div></div>

<h2 id="1-dataset-preparation">1. Dataset Preparation</h2>

<p>For the regression task we will use a small dataset about abalone marine snails. The dataset contains 8
features from 4177 snails. In our regression task, we will use 7 of these features, to predict the number of
rings a snail has (which determines their age).</p>

<p>The abalone dataset is a classic regression problem where we try to predict the age of abalone (sea snails) based on physical measurements. While this might seem niche, it represents common challenges in regression:</p>
<ul>
  <li>Multiple input features of different types</li>
  <li>A continuous target variable</li>
  <li>Natural variability in the data</li>
  <li>Non-linear relationships between features</li>
</ul>

<p>So let’s go ahead and bring the data into an appropriate shape.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load and prepare dataset
</span><span class="n">url</span> <span class="o">=</span> <span class="sh">'</span><span class="s">https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data</span><span class="sh">'</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">Sex</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Length</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Diameter</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Height</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Whole weight</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">Shucked weight</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Viscera weight</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Shell weight</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Rings</span><span class="sh">'</span>
<span class="p">]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Convert categorical data to numerical
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Shape of dataset: </span><span class="si">{</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shape of dataset: (4177, 11)
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/04_tensorflow_dataset_table.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 1: Overview of the abalone dataset.
</div>

<p>Next, let’s split the dataset into a train and test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split dataset into train and test set
</span><span class="n">df_tr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df_te</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">df_tr</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>

<span class="c1"># Separate target from features and convert to float32
</span><span class="n">x_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">df_tr</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Rings</span><span class="sh">'</span><span class="p">])).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">x_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">df_te</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">Rings</span><span class="sh">'</span><span class="p">])).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">y_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">df_tr</span><span class="p">[</span><span class="sh">'</span><span class="s">Rings</span><span class="sh">'</span><span class="p">]).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>
<span class="n">y_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">asarray</span><span class="p">(</span><span class="n">df_te</span><span class="p">[</span><span class="sh">'</span><span class="s">Rings</span><span class="sh">'</span><span class="p">]).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Size of training and test set: </span><span class="si">{</span><span class="n">df_tr</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s"> | </span><span class="si">{</span><span class="n">df_te</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Size of training and test set: (3342, 11) | (835, 11)
</code></pre></div></div>

<p>The abalone dataset dimensions represent:</p>
<ul>
  <li><strong>4,177 total samples</strong>: Split into 3,342 training and 835 test samples</li>
  <li><strong>11 features</strong>: Including both physical measurements and categorical data:
    <ul>
      <li>Physical attributes (length, diameter, height, weights)</li>
      <li>Categorical sex information (encoded as one-hot vectors)</li>
    </ul>
  </li>
  <li><strong>Target variable</strong>: Number of rings (age indicator) to predict</li>
</ul>

<p>An important step for any machine learning project is appropriate features scaling. Now, we could use something
like <code class="language-plaintext highlighter-rouge">scipy</code> or <code class="language-plaintext highlighter-rouge">scikit-learn</code> to do this task. But let’s see how this can also be done directly with
TensorFlow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Normalize data with a keras layer
</span><span class="n">normalizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Normalization</span><span class="p">(</span><span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Train the layer to establish normalization parameters
</span><span class="n">normalizer</span><span class="p">.</span><span class="nf">adapt</span><span class="p">(</span><span class="n">x_tr</span><span class="p">)</span>

<span class="c1"># Verify normalization parameters
</span><span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean parameters:</span><span class="se">\n</span><span class="si">{</span><span class="n">normalizer</span><span class="p">.</span><span class="n">adapt_mean</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Variance parameters:</span><span class="se">\n</span><span class="si">{</span><span class="n">normalizer</span><span class="p">.</span><span class="n">adapt_variance</span><span class="p">.</span><span class="nf">numpy</span><span class="p">()</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Mean parameters:
[0.5240649  0.4077229  0.13945538 0.82737887 0.35884637 0.18079534
 0.23809911 0.313884   0.3255536  0.36056268]
Variance parameters:
[0.01422794 0.00970037 0.00150639 0.23864637 0.04889446 0.0120052
 0.01888644 0.21536086 0.21956848 0.23055716]
</code></pre></div></div>

<h2 id="2-model-creation">2. Model Creation</h2>

<p>Unlike our previous tutorial where we used the Sequential API, here we’ll use TensorFlow’s Functional API. The Functional API provides several key advantages:</p>

<ol>
  <li><strong>Multiple Inputs/Outputs</strong>: Can handle multiple input/output streams</li>
  <li><strong>Layer Sharing</strong>: Reuse layers across different parts of the model</li>
  <li><strong>Non-Sequential Flow</strong>: Create models with branches or multiple paths</li>
  <li><strong>Complex Architectures</strong>: Easily implement advanced patterns like skip connections</li>
  <li><strong>Better Visualization</strong>: Clearer view of data flow between layers</li>
</ol>

<p>Here’s how we build a model using the Functional API:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create layers and connect them with functional API
</span><span class="n">input_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x_tr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>

<span class="c1"># Normalize inputs using our pre-trained normalization layer
</span><span class="n">x</span> <span class="o">=</span> <span class="nf">normalizer</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>

<span class="c1"># Build hidden layers with explicit connections
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Stabilizes training
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>                <span class="c1"># Non-linear activation
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>          <span class="c1"># Prevents overfitting
</span>
<span class="c1"># Second dense layer with similar structure but fewer neurons
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">4</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Output layer for regression (no activation function)
</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># Create model by specifying inputs and outputs
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>

<span class="c1"># Check model size
</span><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">(</span><span class="n">show_trainable</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model"
_________________________________________________________________
Layer (type)                 Output Shape               Param #
=================================================================
input_1 (InputLayer)        [(None, 10)]               0
normalization (Normalizati   (None, 10)                21
on)
dense (Dense)                (None, 8)                 88
batch_normalization (Batch   (None, 8)                 32
Normalization)
re_lu (ReLU)                 (None, 8)                 0
dropout (Dropout)            (None, 8)                 0
dense_1 (Dense)              (None, 4)                 36
batch_normalization_1 (Bat   (None, 4)                 16
chNormalization)
re_lu_1 (ReLU)               (None, 4)                 0
dropout_1 (Dropout)          (None, 4)                 0
dense_2 (Dense)              (None, 1)                 5
=================================================================
Total params: 198 (796.00 Byte)
Trainable params: 153 (612.00 Byte)
Non-trainable params: 45 (184.00 Byte)
_________________________________________________________________
</code></pre></div></div>

<p>Notice how each layer is explicitly connected using function calls (e.g., <code class="language-plaintext highlighter-rouge">layers.Dense(8)(x)</code>). This syntax makes the data flow clear and allows for complex branching patterns that aren’t possible with the Sequential API.</p>

<p>Now that the model is ready, let’s go ahead and compile it. During this process we can specify an appropriate
optimizer as well as relevant metrics that we want to keep track of.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compile model
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nc">MeanSquaredError</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">MSE</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nc">MeanAbsoluteError</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">MAE</span><span class="sh">'</span><span class="p">)],</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Before we move over to training the model, let’s first create a few useful callbacks. Callbacks are powerful tools that can:</p>
<ul>
  <li>Save the best model during training</li>
  <li>Stop training early if no improvement is seen</li>
  <li>Adjust learning rate dynamically</li>
  <li>Log training metrics for later analysis</li>
</ul>

<p>These callbacks can be used to perform some interesting tasks before, during or after a batch, an epoch or training in general. We’ll implement several of these to create a robust training pipeline.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Save best performing model (based on validation loss) in checkpoint
</span><span class="n">model_checkpoint_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">ModelCheckpoint</span><span class="p">(</span>
    <span class="n">filepath</span><span class="o">=</span><span class="sh">'</span><span class="s">model_backup</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Store training history in csv file
</span><span class="n">history_logger</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">CSVLogger</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">history_log.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="bp">False</span>
<span class="p">)</span>

<span class="c1"># Reduce learning rate on plateau
</span><span class="n">reduce_lr_on_plateau</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">ReduceLROnPlateau</span><span class="p">(</span>
        <span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">,</span> <span class="n">factor</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">min_lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="c1"># Use early stopping to stop learning once it doesn't improve anymore
</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="3-training">3. Training</h2>

<p>The data is ready, the model is setup - we’re good to go!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span>
        <span class="n">model_checkpoint_callback</span><span class="p">,</span>
        <span class="n">history_logger</span><span class="p">,</span>
        <span class="n">reduce_lr_on_plateau</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="p">,</span>
    <span class="p">],</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/200
42/42 [==============================] - 2s 36ms/step - loss: 110.4277 - MAE: 9.9720 - val_loss: 104.8957 - val_MAE: 9.7636 - lr: 0.0010
Epoch 2/200
42/42 [==============================] - 1s 29ms/step - loss: 106.3978 - MAE: 9.7741 - val_loss: 101.7641 - val_MAE: 9.6255 - lr: 0.0010
Epoch 3/200
42/42 [==============================] - 1s 29ms/step - loss: 102.8426 - MAE: 9.5989 - val_loss: 99.2429 - val_MAE: 9.5080 - lr: 0.0010
...
Epoch 199/200
42/42 [==============================] - 0s 8ms/step - loss: 9.5845 - MAE: 2.2097 - val_loss: 6.4356 - val_MAE: 1.7255 - lr: 0.0010
Epoch 200/200
42/42 [==============================] - 0s 8ms/step - loss: 8.9521 - MAE: 2.1254 - val_loss: 6.4109 - val_MAE: 1.7223 - lr: 0.0010
</code></pre></div></div>

<p>Once the model is trained we can go ahead and investigate performance during training. Instead of using the
<code class="language-plaintext highlighter-rouge">history</code> variable, let’s load the same information from the CSV saved by the checkpoint.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_history</span><span class="p">(</span><span class="n">history_file</span><span class="o">=</span><span class="sh">'</span><span class="s">history_log.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">''</span><span class="p">):</span>
    <span class="c1"># Load training history from CSV
</span>    <span class="n">history_log</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">history_file</span><span class="p">)</span>

    <span class="c1"># Create subplots for loss and MAE metrics
</span>    <span class="n">_</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1"># Plot loss metrics
</span>    <span class="n">history_log</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">history_log</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">)].</span><span class="nf">plot</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Loss during training</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="c1"># Plot MAE metrics
</span>    <span class="n">history_log</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">history_log</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">MAE</span><span class="sh">'</span><span class="p">)].</span><span class="nf">plot</span><span class="p">(</span>
        <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">MAE during training</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Configure axis labels and scales
</span>    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">MAE</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">axs</span><span class="p">)):</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [#]</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">axs</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">set_yscale</span><span class="p">(</span><span class="sh">'</span><span class="s">log</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># Use log scale for better visualization of changes
</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plot training history
</span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history_file</span><span class="o">=</span><span class="sh">'</span><span class="s">history_log.csv</span><span class="sh">'</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Training overview</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/04_tensorflow_training_history.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 2: Training progress showing loss and Mean Absolute Error (MAE) metrics over epochs. The logarithmic scale helps visualize improvements across different orders of magnitude.
</div>

<h2 id="analyzing-model-performance">Analyzing Model Performance</h2>

<p>Let’s examine our model’s performance from multiple angles:</p>
<ul>
  <li>Training history to check for overfitting</li>
  <li>Prediction accuracy across different value ranges</li>
  <li>Feature importance through a sensitivity analysis</li>
  <li>Comparison with simpler baseline models</li>
</ul>

<p>This multi-faceted analysis helps us understand both where our model succeeds and where it might need improvement.</p>

<p>Looking at the training history above, we can see that:</p>
<ol>
  <li>The model converges smoothly without major fluctuations</li>
  <li>Validation metrics closely follow training metrics, suggesting no significant overfitting</li>
  <li>Both MSE and MAE show consistent improvement throughout training</li>
</ol>

<h2 id="4-inference">4. Inference</h2>

<p>The model training seems to have worked well. Let’s now go ahead and test the model. For this, let’s first load
the best model - saved by the callback during training. This doesn’t need to be the same as the model at the
end of the training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load best model
</span><span class="n">model_best</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="sh">'</span><span class="s">model_backup</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Evaluate best model on test set
</span><span class="n">train_results</span> <span class="o">=</span> <span class="n">model_best</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">test_results</span> <span class="o">=</span> <span class="n">model_best</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Train - Loss: {:.3f} | MAE: {:.3f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">*</span><span class="n">train_results</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Test  - Loss: {:.3f} | MAE: {:.3f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">*</span><span class="n">test_results</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Train - Loss: 6.264 | MAE: 1.696
Test  - Loss: 7.393 | MAE: 1.778
</code></pre></div></div>

<p>Let’s break down these final performance metrics:</p>
<ul>
  <li><strong>Training Metrics</strong>:
    <ul>
      <li>Loss (6.264): Measures overall prediction error</li>
      <li>MAE (1.696): Average deviation of ~1.7 rings in age predictions</li>
    </ul>
  </li>
  <li><strong>Test Metrics</strong>:
    <ul>
      <li>Loss (7.393): ~18% higher than training, indicating some overfitting</li>
      <li>MAE (1.778): Predictions off by ~1.8 rings on average</li>
    </ul>
  </li>
  <li><strong>Practical Impact</strong>: For abalone age prediction, being off by less than 2 rings is acceptable for most applications</li>
</ul>

<h2 id="5-architecture-fine-tuning">5. Architecture fine-tuning</h2>

<p>Let’s now go a step further and create a setup with which we can fine-tune the model architecture. While there
are different frameworks, such as <a href="https://keras.io/keras_tuner/">KerasTuner</a> or
<a href="https://www.adriangb.com/scikeras/stable/">Sci-Kears</a> - let’s perform a more manual approach.</p>

<p>For this we need two things: <em>First</em>, a function that creates the model and sets the compiler, and <em>second</em> a
parameter grid.</p>

<h3 id="function-to-dynamically-create-a-model">Function to dynamically create a model</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_and_compile_model</span><span class="p">(</span>
    <span class="n">hidden</span><span class="o">=</span><span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>           <span class="c1"># List defining sizes of hidden layers
</span>    <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span>       <span class="c1"># Activation function for hidden layers
</span>    <span class="n">use_batch</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>          <span class="c1"># Whether to use batch normalization
</span>    <span class="n">dropout_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>          <span class="c1"># Dropout rate for regularization
</span>    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>     <span class="c1"># Initial learning rate
</span>    <span class="n">optimizers</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>       <span class="c1"># Choice of optimizer
</span>    <span class="n">kernel_init</span><span class="o">=</span><span class="sh">'</span><span class="s">he_normal</span><span class="sh">'</span><span class="p">,</span> <span class="c1"># Weight initialization strategy
</span>    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="c1"># Weight regularization method
</span><span class="p">):</span>
    <span class="c1"># Create input layer
</span>    <span class="n">input_layer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x_tr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>

    <span class="c1"># Normalize input data
</span>    <span class="n">x</span> <span class="o">=</span> <span class="nf">normalizer</span><span class="p">(</span><span class="n">input_layer</span><span class="p">)</span>

    <span class="c1"># Build hidden layers
</span>    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">h</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">hidden</span><span class="p">):</span>

        <span class="c1"># Add batch normalization if requested
</span>        <span class="k">if</span> <span class="n">use_batch</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add dense layer with specific parameters
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span>
            <span class="n">h</span><span class="p">,</span>
            <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>
            <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span>
            <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>
        <span class="p">)(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Add dropout layer if requested
</span>        <span class="k">if</span> <span class="n">dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">dropout_rate</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Add output layer (no activation for regression)
</span>    <span class="n">output_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

    <span class="c1"># Create and compile model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">input_layer</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output_layer</span><span class="p">)</span>

    <span class="c1"># Configure optimizer based on selection
</span>    <span class="k">if</span> <span class="n">optimizers</span> <span class="o">==</span> <span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">optimizers</span> <span class="o">==</span> <span class="sh">'</span><span class="s">rmsprop</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">optimizers</span> <span class="o">==</span> <span class="sh">'</span><span class="s">sgd</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Compile model with loss and metrics
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
        <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="nc">MeanSquaredError</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">MSE</span><span class="sh">'</span><span class="p">),</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">keras</span><span class="p">.</span><span class="n">metrics</span><span class="p">.</span><span class="nc">MeanAbsoluteError</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">MAE</span><span class="sh">'</span><span class="p">)],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1"># Use function to create model and report summary overview
</span><span class="n">model</span> <span class="o">=</span> <span class="nf">build_and_compile_model</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "model_1"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_2 (InputLayer)        [(None, 10)]              0
normalization (Normalizati  (None, 10)                21
on)
batch_normalization_2 (Bat  (None, 10)                40
chNormalization)
dense_3 (Dense)             (None, 8)                 88
batch_normalization_3 (Bat  (None, 8)                 32
chNormalization)
dense_4 (Dense)             (None, 4)                 36
dense_5 (Dense)             (None, 1)                 5
=================================================================
Total params: 222 (892.00 Byte)
Trainable params: 165 (660.00 Byte)
Non-trainable params: 57 (232.00 Byte)
_________________________________________________________________
</code></pre></div></div>

<p>Next step is the creation of the parameter grid. First, let’s establish the different parameters we could
explore.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define exploration grid
</span><span class="n">hidden</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="n">activation</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">elu</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">selu</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">tanh</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">]</span>
<span class="n">use_batch</span> <span class="o">=</span> <span class="p">[</span><span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">]</span>
<span class="n">dropout_rate</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">]</span>
<span class="n">optimizers</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">rmsprop</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">sgd</span><span class="sh">'</span><span class="p">]</span>
<span class="n">kernel_init</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">'</span><span class="s">he_normal</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">he_uniform</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">glorot_normal</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">glorot_uniform</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">uniform</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">normal</span><span class="sh">'</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="sh">'</span><span class="s">l1</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">l2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">l1_l2</span><span class="sh">'</span><span class="p">]</span>
<span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">128</span><span class="p">]</span>
</code></pre></div></div>

<p>Now, let’s put all of this into a parameter grid.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create parameter grid
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span>
    <span class="n">hidden</span><span class="o">=</span><span class="n">hidden</span><span class="p">,</span>  <span class="c1"># Network architectures from simple to complex
</span>    <span class="n">activation</span><span class="o">=</span><span class="n">activation</span><span class="p">,</span>  <span class="c1"># Different non-linearities for different data patterns
</span>    <span class="n">use_batch</span><span class="o">=</span><span class="n">use_batch</span><span class="p">,</span>  <span class="c1"># Batch normalization for training stability
</span>    <span class="n">dropout_rate</span><span class="o">=</span><span class="n">dropout_rate</span><span class="p">,</span>  <span class="c1"># Regularization to prevent overfitting
</span>    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>  <span class="c1"># Controls step size during optimization
</span>    <span class="n">optimizers</span><span class="o">=</span><span class="n">optimizers</span><span class="p">,</span>  <span class="c1"># Different optimization strategies
</span>    <span class="n">kernel_init</span><span class="o">=</span><span class="n">kernel_init</span><span class="p">,</span>  <span class="c1"># Weight initialization methods
</span>    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">kernel_regularizer</span><span class="p">,</span>  <span class="c1"># Weight penalties to prevent overfitting
</span>    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_sizes</span><span class="p">,</span>  <span class="c1"># Training batch sizes for different memory/speed tradeoffs
</span><span class="p">)</span>

<span class="c1"># Go through the parameter grid
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ParameterGrid</span>

<span class="n">grid</span> <span class="o">=</span> <span class="nc">ParameterGrid</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Exploring </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span><span class="si">}</span><span class="s"> network architectures.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Exploring 17280 network architectures.
</code></pre></div></div>

<p>Ok… these are definitely too many grid points. So let’s shuffle the grid and explore a few iterations to get
a better sense of what works and what doesn’t.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Number of grid points to explore
</span><span class="n">nth</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Establish grid indecies, shuffle them and keep the first N-th entries
</span><span class="n">grid_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">grid</span><span class="p">))</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">grid_idx</span><span class="p">)</span>
<span class="n">grid_idx</span> <span class="o">=</span> <span class="n">grid_idx</span><span class="p">[:</span><span class="n">nth</span><span class="p">]</span>
</code></pre></div></div>

<p>Now, we’re good to go!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loop through grid points
</span><span class="k">for</span> <span class="n">gixd</span> <span class="ow">in</span> <span class="n">grid_idx</span><span class="p">:</span>

    <span class="c1"># Select grid point
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">grid</span><span class="p">[</span><span class="n">gixd</span><span class="p">]</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Exploring: </span><span class="si">{</span><span class="n">g</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Build and compile model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nf">build_and_compile_model</span><span class="p">(</span>
        <span class="n">hidden</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">hidden</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">activation</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">use_batch</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">use_batch</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">dropout_rate</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">dropout_rate</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">learning_rate</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">optimizers</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">optimizers</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">kernel_init</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">kernel_init</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">kernel_regularizer</span><span class="sh">'</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="c1"># Save best performing model (based on validation loss) in checkpoint
</span>    <span class="n">backup_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">model_backup_</span><span class="si">{</span><span class="n">gixd</span><span class="si">}</span><span class="sh">'</span>
    <span class="n">model_checkpoint_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">ModelCheckpoint</span><span class="p">(</span>
        <span class="n">filepath</span><span class="o">=</span><span class="n">backup_path</span><span class="p">,</span>
        <span class="n">save_weights_only</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
        <span class="n">monitor</span><span class="o">=</span><span class="sh">'</span><span class="s">val_loss</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">min</span><span class="sh">'</span><span class="p">,</span>
        <span class="n">save_best_only</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Store training history in csv file
</span>    <span class="n">history_file</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">history_log_</span><span class="si">{</span><span class="n">gixd</span><span class="si">}</span><span class="s">.csv</span><span class="sh">'</span>
    <span class="n">history_logger</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">CSVLogger</span><span class="p">(</span>
        <span class="n">history_file</span><span class="p">,</span> <span class="n">separator</span><span class="o">=</span><span class="sh">'</span><span class="s">,</span><span class="sh">'</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="bp">False</span>
    <span class="p">)</span>

    <span class="c1"># Setup callbacks
</span>    <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">model_checkpoint_callback</span><span class="p">,</span>
        <span class="n">history_logger</span><span class="p">,</span>
        <span class="n">reduce_lr_on_plateau</span><span class="p">,</span>
        <span class="n">early_stopping</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># Train model
</span>    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x_tr</span><span class="p">,</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">g</span><span class="p">[</span><span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">],</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span>
    <span class="p">)</span>

    <span class="c1"># Load best model
</span>    <span class="n">model_best</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="n">backup_path</span><span class="p">)</span>

    <span class="c1"># Evaluate best model on test set
</span>    <span class="n">train_results</span> <span class="o">=</span> <span class="n">model_best</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">test_results</span> <span class="o">=</span> <span class="n">model_best</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\t</span><span class="s">Train - Loss: {:.3f} | MAE: {:.3f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">*</span><span class="n">train_results</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\t</span><span class="s">Test  - Loss: {:.3f} | MAE: {:.3f}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">*</span><span class="n">test_results</span><span class="p">))</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="se">\t</span><span class="s">Model Parameters: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="nf">count_params</span><span class="p">()</span><span class="si">}</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Exploring: {'use_batch': False, 'optimizers': 'sgd', 'learning_rate': 0.0001,
            'kernel_regularizer': None, 'kernel_init': 'glorot_uniform', 'hidden': [8],
            'dropout_rate': 0, 'batch_size': 128, 'activation': 'selu'}
    Train - Loss: 5.294 | MAE: 1.643
    Test  - Loss: 7.148 | MAE: 1.807
    Model Parameters: 118

Exploring: {'use_batch': False, 'optimizers': 'sgd', 'learning_rate': 0.001,
            'kernel_regularizer': None, 'kernel_init': 'glorot_uniform', 'hidden': [8, 4],
            'dropout_rate': 0.5, 'batch_size': 128, 'activation': 'tanh'}
    Train - Loss: 5.290 | MAE: 1.583
    Test  - Loss: 6.250 | MAE: 1.730
    Model Parameters: 150

Exploring: {'use_batch': True, 'optimizers': 'sgd', 'learning_rate': 0.0001,
            'kernel_regularizer': None, 'kernel_init': 'uniform', 'hidden': [8, 4],
            'dropout_rate': 0.5, 'batch_size': 32, 'activation': 'relu'}
Epoch 195: early stopping
    Train - Loss: 8.660 | MAE: 2.050
    Test  - Loss: 10.039 | MAE: 2.144
    Model Parameters: 222

Exploring: {'use_batch': True, 'optimizers': 'rmsprop', 'learning_rate': 0.0001,
            'kernel_regularizer': None, 'kernel_init': 'he_uniform', 'hidden': [8, 16, 8],
            'dropout_rate': 0.5, 'batch_size': 128, 'activation': 'tanh'}
    Train - Loss: 24.535 | MAE: 3.957
    Test  - Loss: 26.605 | MAE: 4.079
    Model Parameters: 534

Exploring: {'use_batch': False, 'optimizers': 'adam', 'learning_rate': 0.0001,
            'kernel_regularizer': 'l2', 'kernel_init': 'glorot_uniform', 'hidden': [8, 16, 8],
            'dropout_rate': 0.5, 'batch_size': 128, 'activation': 'selu'}
    Train - Loss: 6.947 | MAE: 1.715
    Test  - Loss: 8.042 | MAE: 1.851
    Model Parameters: 398
</code></pre></div></div>

<h2 id="6-fine-tuning-investigation">6. Fine-tuning investigation</h2>

<p>Once the grid points were explored we can go ahead and investigate the best models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop through grid points
</span><span class="k">for</span> <span class="n">gixd</span> <span class="ow">in</span> <span class="n">grid_idx</span><span class="p">:</span>

    <span class="c1"># Select grid point
</span>    <span class="n">g</span> <span class="o">=</span> <span class="n">grid</span><span class="p">[</span><span class="n">gixd</span><span class="p">]</span>

    <span class="c1"># Restore best model
</span>    <span class="n">backup_path</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">model_backup_</span><span class="si">{</span><span class="n">gixd</span><span class="si">}</span><span class="sh">'</span>
    <span class="n">model_best</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nf">load_model</span><span class="p">(</span><span class="n">backup_path</span><span class="p">)</span>

    <span class="c1"># Evaluate best model on test set
</span>    <span class="n">train_results</span> <span class="o">=</span> <span class="n">model_best</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">test_results</span> <span class="o">=</span> <span class="n">model_best</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Store information in table
</span>    <span class="n">df_score</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
    <span class="n">df_score</span><span class="p">[</span><span class="sh">'</span><span class="s">loss_tr</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">df_score</span><span class="p">[</span><span class="sh">'</span><span class="s">MAE_tr</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">df_score</span><span class="p">[</span><span class="sh">'</span><span class="s">loss_te</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_results</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">df_score</span><span class="p">[</span><span class="sh">'</span><span class="s">MAE_te</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">test_results</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">df_score</span><span class="p">[</span><span class="sh">'</span><span class="s">idx</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gixd</span>

    <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">df_score</span><span class="p">.</span><span class="nf">to_frame</span><span class="p">().</span><span class="n">T</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span><span class="n">results</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nf">sort_values</span><span class="p">(</span><span class="sh">'</span><span class="s">loss_te</span><span class="sh">'</span><span class="p">)</span>
<span class="n">results</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/04_tensorflow_results_table.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 3: Results table for the best performing model architecture, showing both loss and MAE on training and validation sets. The consistent convergence suggests stable learning without overfitting.
</div>

<p>From this table, you could now perform a multitude of follow-up investigations. For example, take a look at the
loss evolution during training:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get idx of best model
</span><span class="n">gixd_best</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># Load history file of best training
</span><span class="n">history_file</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">'</span><span class="s">history_log_</span><span class="si">{</span><span class="n">gixd_best</span><span class="si">}</span><span class="s">.csv</span><span class="sh">'</span>

<span class="c1"># Plot training curves
</span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history_file</span><span class="o">=</span><span class="n">history_file</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Training overview of best model</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/04_tensorflow_architecture_comparison.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 4: Training metrics for the best performing model architecture, showing both loss and MAE on training and validation sets. The consistent convergence suggests stable learning without overfitting.
</div>

<h3 id="advanced-deep-learning-pitfalls">Advanced Deep Learning Pitfalls</h3>

<p>When working with complex neural networks and regression tasks, be aware of these advanced challenges:</p>

<p><strong>Gradient Issues</strong></p>
<ul>
  <li>Vanishing/exploding gradients in deep networks</li>
  <li>Unstable training with certain architectures</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use gradient clipping to prevent explosions
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span>
    <span class="n">clipnorm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span>
<span class="p">)</span>

<span class="c1"># Add batch normalization to help with gradient flow
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Learning Rate Dynamics</strong></p>
<ul>
  <li>Static learning rates often suboptimal</li>
  <li>Different layers may need different rates</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Implement learning rate schedule
</span><span class="n">initial_learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">decay_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">decay_rate</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">schedules</span><span class="p">.</span><span class="nc">ExponentialDecay</span><span class="p">(</span>
    <span class="n">initial_learning_rate</span><span class="p">,</span>
    <span class="n">decay_steps</span><span class="o">=</span><span class="n">decay_steps</span><span class="p">,</span>
    <span class="n">decay_rate</span><span class="o">=</span><span class="n">decay_rate</span>
<span class="p">)</span>

<span class="c1"># Or use adaptive learning rate with warmup
</span><span class="n">warmup_steps</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="k">def</span> <span class="nf">warmup_cosine_decay</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
    <span class="n">warmup_rate</span> <span class="o">=</span> <span class="n">initial_learning_rate</span> <span class="o">*</span> <span class="n">step</span> <span class="o">/</span> <span class="n">warmup_steps</span>
    <span class="n">cosine_rate</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">experimental</span><span class="p">.</span><span class="nc">CosineDecay</span><span class="p">(</span>
        <span class="n">initial_learning_rate</span><span class="p">,</span> <span class="n">decay_steps</span>
    <span class="p">)(</span><span class="n">step</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">step</span> <span class="o">&lt;</span> <span class="n">warmup_steps</span><span class="p">,</span> <span class="n">warmup_rate</span><span class="p">,</span> <span class="n">cosine_rate</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Complex Loss Functions</strong></p>
<ul>
  <li>Multiple objectives need careful weighting</li>
  <li>Custom losses require gradient consideration</li>
  <li>Handle edge cases and numerical stability</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">WeightedMSE</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">losses</span><span class="p">.</span><span class="n">Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">feature_weights</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">feature_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">constant</span><span class="p">(</span><span class="n">feature_weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
        <span class="c1"># Add small epsilon to prevent numerical issues
</span>        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">clip_by_value</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">squared_errors</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">square</span><span class="p">(</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span>
        <span class="n">weighted_errors</span> <span class="o">=</span> <span class="n">squared_errors</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">feature_weights</span>
        <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">weighted_errors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Data Pipeline Bottlenecks</strong></p>
<ul>
  <li>I/O can become training bottleneck</li>
  <li>Memory constraints with large datasets</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Efficient data pipeline with prefetching
</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">((</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">prefetch</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">AUTOTUNE</span><span class="p">)</span>

<span class="c1"># For large datasets, use generators
</span><span class="k">def</span> <span class="nf">data_generator</span><span class="p">():</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">x_tr</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">x_tr</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">],</span> <span class="n">y_tr</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span>
</code></pre></div></div>

<p><strong>Model Architecture Complexity</strong></p>
<ul>
  <li>Deeper isn’t always better</li>
  <li>Skip connections can help with gradient flow</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of residual connection
</span><span class="k">def</span> <span class="nf">residual_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">filters</span><span class="p">):</span>
    <span class="n">shortcut</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="n">filters</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Add</span><span class="p">()([</span><span class="n">shortcut</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>  <span class="c1"># Skip connection
</span>    <span class="k">return</span> <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Regularization Strategy</strong></p>
<ul>
  <li>Different layers may need different regularization</li>
  <li>Combine multiple regularization techniques</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Comprehensive regularization strategy
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span>
    <span class="mi">64</span><span class="p">,</span>
    <span class="n">kernel_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="nf">l1_l2</span><span class="p">(</span><span class="n">l1</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">l2</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">),</span>
    <span class="n">activity_regularizer</span><span class="o">=</span><span class="n">keras</span><span class="p">.</span><span class="n">regularizers</span><span class="p">.</span><span class="nf">l1</span><span class="p">(</span><span class="mf">1e-5</span><span class="p">)</span>
<span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Model Debugging</strong></p>
<ul>
  <li>Add metrics to monitor internal states</li>
  <li>Use callbacks for detailed inspection</li>
  <li>Clear unused variables and models</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Clear memory after training experiments
</span><span class="kn">import</span> <span class="n">gc</span>

<span class="k">def</span> <span class="nf">cleanup_memory</span><span class="p">():</span>
    <span class="c1"># Delete unused variables
</span>    <span class="k">del</span> <span class="n">unused_model</span>
    <span class="c1"># Force garbage collection
</span>    <span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
    <span class="c1"># Clear TensorFlow session
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="nf">clear_session</span><span class="p">()</span>

<span class="c1"># Monitor layer states during training
</span><span class="k">class</span> <span class="nc">LayerStateCallback</span><span class="p">(</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">logs</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">layer_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">layer</span><span class="p">.</span><span class="n">output</span> <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">]</span>
        <span class="n">inspection_model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span>
            <span class="n">outputs</span><span class="o">=</span><span class="n">layer_outputs</span>
        <span class="p">)</span>
        <span class="c1"># Monitor layer statistics during training
</span>        <span class="n">layer_states</span> <span class="o">=</span> <span class="n">inspection_model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_val</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">layer_idx</span><span class="p">,</span> <span class="n">states</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">layer_states</span><span class="p">):</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Layer </span><span class="si">{</span><span class="n">layer_idx</span><span class="si">}</span><span class="s"> stats:</span><span class="sh">"</span><span class="p">,</span>
                    <span class="sa">f</span><span class="sh">"</span><span class="s">mean=</span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">,</span><span class="sh">"</span><span class="p">,</span>
                    <span class="sa">f</span><span class="sh">"</span><span class="s">std=</span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">states</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Clean up inspection model
</span>        <span class="k">del</span> <span class="n">inspection_model</span>
        <span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
</code></pre></div></div>

<p>These advanced considerations become crucial when:</p>
<ul>
  <li>Working with complex architectures</li>
  <li>Training on large datasets</li>
  <li>Optimizing for specific performance metrics</li>
  <li>Deploying models in production environments</li>
  <li>Debugging training issues</li>
</ul>

<h2 id="summary-and-series-conclusion">Summary and Series Conclusion</h2>

<p>In this final tutorial, we’ve covered:</p>
<ul>
  <li>Building a regression model using TensorFlow’s functional API</li>
  <li>Implementing custom normalization layers</li>
  <li>Using callbacks for training optimization</li>
  <li>Comparing different model approaches</li>
</ul>

<p><strong>Key takeaways:</strong></p>
<ol>
  <li>Complex architectures aren’t always better</li>
  <li>Proper training procedures are crucial</li>
  <li>Model comparison helps choose the best approach</li>
  <li>Advanced features require careful tuning</li>
  <li>Different architectures suit different problems</li>
</ol>

<p>Throughout this series, we’ve progressed from basic classification to advanced regression, covering both traditional machine learning and deep learning approaches. We’ve seen how Scikit-learn and TensorFlow complement each other, each offering unique strengths for different types of problems.</p>

<p><a href="/blog/2023/03_scikit_advanced">← Previous: Advanced Machine Learning</a> or
<a href="/blog/2023/01_scikit_simple">Return to Series Overview →</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Implementing sophisticated neural networks for regression tasks]]></summary></entry><entry><title type="html">Advanced Machine Learning - Regression Pipelines in Scikit-learn</title><link href="https://miykael.github.io/blog/2023/03_scikit_advanced/" rel="alternate" type="text/html" title="Advanced Machine Learning - Regression Pipelines in Scikit-learn" /><published>2023-10-23T14:00:00+00:00</published><updated>2023-10-23T14:00:00+00:00</updated><id>https://miykael.github.io/blog/2023/03_scikit_advanced</id><content type="html" xml:base="https://miykael.github.io/blog/2023/03_scikit_advanced/"><![CDATA[<p>In this third part of our series, we’ll explore more sophisticated machine learning techniques using <a href="https://scikit-learn.org/stable/">Scikit-learn</a>. While Parts 1 and 2 focused on classification, we’ll now tackle regression problems and learn how to build complex preprocessing pipelines. We’ll use the California Housing dataset to demonstrate these concepts.</p>

<p>The complete code for this tutorial can be found in the <a href="/assets/scripts/03_scikit_advanced.py">03_scikit_advanced.py</a> script.</p>

<p><strong>Note</strong>: The purpose of this post is to highlight the flexibility and capabilities of scikit-learn’s advanced features. Therefore, this tutorial focuses on introducing you to those advanced routines rather than creating the optimal regression model.</p>

<h3 id="why-advanced-preprocessing">Why Advanced Preprocessing?</h3>

<p>Real-world data rarely comes in a clean, ready-to-use format. Data scientists often spend more time preparing data than training models. Common preprocessing steps include:</p>
<ul>
  <li><strong>Missing value imputation</strong>: Filling missing data points</li>
  <li><strong>Feature encoding</strong>: Converting categorical variables to numerical format</li>
  <li><strong>Feature scaling</strong>: Normalizing features to comparable ranges</li>
  <li><strong>Feature selection</strong>: Identifying most relevant variables</li>
  <li><strong>Feature engineering</strong>: Creating new features from existing ones</li>
</ul>

<p>Scikit-learn provides powerful tools to handle these challenges systematically. Let’s see how to combine them effectively into a preprocessing pipeline that can handle all these issues automatically.</p>

<p>As always, first, let’s import the scientific Python packages we need.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Standard scientific Python imports
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
</code></pre></div></div>

<h2 id="1-load-dataset">1. Load Dataset</h2>

<p>The California Housing dataset contains information about houses in California districts. It’s a perfect dataset for demonstrating advanced preprocessing because it includes:</p>
<ul>
  <li>Both numerical and categorical features</li>
  <li>Missing values that need handling</li>
  <li>Features on different scales</li>
  <li>Complex relationships between variables</li>
</ul>

<p>The dataset itself contains information about the houses, including features like total area, lot shape, neighborhood information, overall quality, year built, etc. And the target feature that we would like to predict is the <code class="language-plaintext highlighter-rouge">SalePrice</code>.</p>

<p>Let’s load the data and take a look:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load dataset
</span><span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">housing</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">fetch_openml</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">house_prices</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Extract feature matrix X and target vector y
</span><span class="n">X</span> <span class="o">=</span> <span class="n">housing</span><span class="p">[</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">].</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="sh">'</span><span class="s">Id</span><span class="sh">'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">housing</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dimension of X: </span><span class="si">{</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s">Dimension of y: </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dimension of X: (1460, 79)
Dimension of y: (1460,)
</code></pre></div></div>

<p>The house price dataset contains:</p>
<ul>
  <li><strong>1,460 samples</strong>: Each representing a different house sale</li>
  <li><strong>79 features</strong>: A mix of numerical and categorical characteristics including:
    <ul>
      <li>Property specifications (size, rooms, year built)</li>
      <li>Location details (neighborhood, zoning)</li>
      <li>Quality ratings (overall condition, materials)</li>
    </ul>
  </li>
  <li><strong>Target values</strong>: Continuous house sale prices in dollars</li>
</ul>

<p>As you can see, we have 1460 samples (houses), each containing 79 features (i.e. characteristics). Let’s examine the first few entries to better understand our data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show first few entries and columns of the dataset
</span><span class="n">X</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:</span><span class="mi">5</span><span class="p">]</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/03_scikit_dataframe_01.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 1: First 5 rows and 5 columns of the California Housing dataset.
</div>

<p>If we look closer at the feature matrix X, we can see that of those 79 features, 36 are of type float and 43
are of type ‘object’ (i.e. categorical features), and that some entries are missing. Plus, the target feature
<code class="language-plaintext highlighter-rouge">SalePrice</code> has a right skewed value distribution.</p>

<p>Therefore, if possible, our pipeline should be able to handle all of this peculiarities. Even better, let’s try
to setup a pipeline that helps us to find the optimal way how to preprocess this dataset.</p>

<h2 id="2-feature-analysis">2. Feature Analysis</h2>

<p>Before building our pipeline, let’s understand what we’re working with:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Quick overview of feature types
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature types:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">dtypes</span><span class="p">.</span><span class="nf">value_counts</span><span class="p">())</span>

<span class="c1"># Check for missing values
</span><span class="n">missing_values</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Features with missing values:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">missing_values</span><span class="p">[</span><span class="n">missing_values</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">].</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Feature types:
object     43
int64      33
float64     3
Name: count, dtype: int64

Features with missing values:
PoolQC          1453
MiscFeature     1406
Alley           1369
Fence           1179
FireplaceQu      690
LotFrontage      259
GarageType        81
GarageYrBlt       81
GarageFinish      81
GarageQual        81
GarageCond        81
BsmtExposure      38
BsmtFinType2      38
BsmtFinType1      37
BsmtCond          37
BsmtQual          37
MasVnrArea         8
MasVnrType         8
Electrical         1
dtype: int64
</code></pre></div></div>

<p>And visualizing the target variable distribution:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Analyze target variable distribution
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">histplot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Distribution of House Prices</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Price</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/03_scikit_price_distribution.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 2: Data quality analysis showing the distribution of missing values and data types across features in the California Housing dataset.
</div>

<p>This analysis reveals several important preprocessing needs:</p>
<ol>
  <li>We have both numerical (float) and categorical (object) features</li>
  <li>Several features have missing values</li>
  <li>Our target variable (house prices) shows right skew</li>
  <li>Features are on very different scales (e.g., year vs. price)</li>
</ol>

<p>These insights will guide our pipeline design.</p>

<h2 id="3-split-data-into-train-and-test-set">3. Split data into train and test set</h2>

<p>As always, let’s first go ahead and split the dataset into train and test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="4-building-the-pipeline">4. Building the Pipeline</h2>

<p>One of Scikit-learn’s most powerful features is its Pipeline API. We’ll create a pipeline that:</p>
<ol>
  <li>Handles missing values differently for numerical and categorical features</li>
  <li>Applies appropriate scaling to numerical features</li>
  <li>Properly encodes categorical features</li>
  <li>Optionally reduces dimensionality</li>
  <li>Fits our chosen regression model</li>
</ol>

<p>So let’s setup a pipeline that performs these different pre-processing routines: Transformation
of categorical data to numerical data, data imputer for missing values, data scaling, potential dimensionality
reduction, etc.</p>

<h3 id="41-handling-categorical-data">4.1. Handling categorical data</h3>

<p>First, let’s create a small pipeline that takes categorical data, fills missing values with <code class="language-plaintext highlighter-rouge">'missing'</code> and
than applies one-hot encoding on these categorical features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="n">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="c1"># Define preprocessing pipeline for categorical features
</span><span class="n">categorical_preprocessor</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># Fill missing values with 'missing' string
</span>        <span class="p">(</span><span class="sh">'</span><span class="s">imputer_cat</span><span class="sh">'</span><span class="p">,</span> <span class="nc">SimpleImputer</span><span class="p">(</span><span class="n">fill_value</span><span class="o">=</span><span class="sh">'</span><span class="s">missing</span><span class="sh">'</span><span class="p">,</span> <span class="n">strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">constant</span><span class="sh">'</span><span class="p">)),</span>

        <span class="c1"># Convert categorical strings to one-hot encoded vectors
</span>        <span class="c1"># handle_unknown='ignore' prevents errors with new categories at prediction time
</span>        <span class="p">(</span><span class="sh">'</span><span class="s">onehot</span><span class="sh">'</span><span class="p">,</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">handle_unknown</span><span class="o">=</span><span class="sh">'</span><span class="s">ignore</span><span class="sh">'</span><span class="p">)),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="42-handling-numerical-data">4.2. Handling numerical data</h3>

<p>To handle numerical data we will use a slightly more advanced processing pipeline (to showcase some scikit-learn
feature, not because it’s the best thing to do). So let’s first fill missing values with e.g. the mean of the
feature, potentially apply a polynomial expansion to module non-linear relationships, apply a scaler and then
potentially apply dimensionality reduction via PCA and/or by selecting only the “most relevant” features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span>
<span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_regression</span><span class="p">,</span> <span class="n">mutual_info_regression</span>
<span class="kn">from</span> <span class="n">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">FeatureUnion</span>

<span class="c1"># Create feature reduction pipeline combining PCA and feature selection
</span><span class="n">dim_reduction</span> <span class="o">=</span> <span class="nc">FeatureUnion</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="sh">'</span><span class="s">pca</span><span class="sh">'</span><span class="p">,</span> <span class="nc">PCA</span><span class="p">()),</span>  <span class="c1"># Dimensionality reduction using principal component analysis
</span>        <span class="p">(</span><span class="sh">'</span><span class="s">feat_selecter</span><span class="sh">'</span><span class="p">,</span> <span class="nc">SelectKBest</span><span class="p">()),</span>  <span class="c1"># Select top K features based on statistical tests
</span>    <span class="p">]</span>
<span class="p">)</span>

<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">PolynomialFeatures</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">,</span>
                                   <span class="n">RobustScaler</span><span class="p">,</span> <span class="n">PowerTransformer</span><span class="p">)</span>

<span class="c1"># Package all relevant preprocessing routines for numerical data into one pipeline
</span><span class="n">numeric_preprocessor</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># Handle missing values in numerical features
</span>        <span class="p">(</span><span class="sh">'</span><span class="s">imputer_numeric</span><span class="sh">'</span><span class="p">,</span> <span class="nc">SimpleImputer</span><span class="p">(</span>
            <span class="n">missing_values</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">,</span>  <span class="c1"># Identify NaN values
</span>            <span class="n">strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">)),</span>  <span class="c1"># Replace with column mean
</span>
        <span class="p">(</span><span class="sh">'</span><span class="s">polytrans</span><span class="sh">'</span><span class="p">,</span> <span class="nc">PolynomialFeatures</span><span class="p">()),</span>  <span class="c1"># Create interaction terms between features
</span>        <span class="p">(</span><span class="sh">'</span><span class="s">scaler</span><span class="sh">'</span><span class="p">,</span> <span class="nc">StandardScaler</span><span class="p">()),</span>  <span class="c1"># Normalize features to zero mean and unit variance
</span>        <span class="p">(</span><span class="sh">'</span><span class="s">dim_reduction</span><span class="sh">'</span><span class="p">,</span> <span class="n">dim_reduction</span><span class="p">),</span>  <span class="c1"># Apply dimensionality reduction
</span>    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="43-combining-preprocessing-pipelines">4.3. Combining preprocessing pipelines</h3>

<p>Now that we have a preprocessing pipeline for the categorical and numerical features, let’s combine them into
one preprocessing pipeline.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.compose</span> <span class="kn">import</span> <span class="n">ColumnTransformer</span>

<span class="n">preprocessor</span> <span class="o">=</span> <span class="nc">ColumnTransformer</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="sh">'</span><span class="s">numerical</span><span class="sh">'</span><span class="p">,</span> <span class="n">numeric_preprocessor</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="sh">'</span><span class="s">number</span><span class="sh">'</span><span class="p">).</span><span class="n">columns</span><span class="p">),</span>
        <span class="p">(</span><span class="sh">'</span><span class="s">categorical</span><span class="sh">'</span><span class="p">,</span> <span class="n">categorical_preprocessor</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="sh">'</span><span class="s">number</span><span class="sh">'</span><span class="p">).</span><span class="n">columns</span><span class="p">),</span>
    <span class="p">],</span>
    <span class="n">remainder</span><span class="o">=</span><span class="sh">'</span><span class="s">passthrough</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="44-add-regression-model">4.4. Add regression model</h3>

<p>After the data is preprocessed we want to hand it over to a regression estimator. For this purpose, let’s chose
a ridge regression.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>

<span class="n">ridge</span> <span class="o">=</span> <span class="nc">Ridge</span><span class="p">()</span>

<span class="n">pipe</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="p">[</span>
    <span class="p">(</span><span class="sh">'</span><span class="s">preprocessor</span><span class="sh">'</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">),</span>
    <span class="p">(</span><span class="sh">'</span><span class="s">ridge</span><span class="sh">'</span><span class="p">,</span> <span class="nc">Ridge</span><span class="p">())</span>
<span class="p">])</span>
</code></pre></div></div>

<p>As such, the pipeline would be finished. But because we know that our target feature <code class="language-plaintext highlighter-rouge">SalePrice</code> is right
skewed, we should ideally apply a log-transformation before fitting the model. Instead of doing this
transformation manually (and reverting it at the end), we can also use scikit-learn’s
<code class="language-plaintext highlighter-rouge">TransformedTargetRegressor</code> to do that on the fly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.compose</span> <span class="kn">import</span> <span class="n">TransformedTargetRegressor</span>

<span class="n">regressor</span> <span class="o">=</span> <span class="nc">TransformedTargetRegressor</span><span class="p">(</span>
    <span class="n">regressor</span><span class="o">=</span><span class="n">pipe</span><span class="p">,</span> <span class="n">func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">,</span> <span class="n">inverse_func</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">expm1</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="5-parameter-grid">5. Parameter grid</h2>

<p>Before training our model, we should also define a parameter grid that allows us to fine-tune the processing
and model parameters. Given our complex routine, we actually have a lot of parameter that we can play around
with.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ParameterGrid</span>

<span class="c1"># Shorten key identifier by separating common prefix
</span><span class="n">prefix</span> <span class="o">=</span> <span class="sh">'</span><span class="s">regressor__preprocessor__</span><span class="sh">'</span>

<span class="c1"># Create parametergrid
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>

    <span class="c1"># Explore imputers
</span>    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__imputer_numeric__add_indicator</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__imputer_numeric__strategy</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">median</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">most_frequent</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">constant</span><span class="sh">'</span><span class="p">],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">categorical__imputer_cat__add_indicator</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="bp">True</span><span class="p">,</span> <span class="bp">False</span><span class="p">],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">categorical__imputer_cat__strategy</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">most_frequent</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">constant</span><span class="sh">'</span><span class="p">],</span>

    <span class="c1"># Explore numerical preprocessors
</span>    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__polytrans__degree</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__polytrans__interaction_only</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="bp">False</span><span class="p">,</span> <span class="bp">True</span><span class="p">],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__dim_reduction__pca</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">drop</span><span class="sh">'</span><span class="p">,</span> <span class="nc">PCA</span><span class="p">(</span><span class="mf">0.9</span><span class="p">),</span> <span class="nc">PCA</span><span class="p">(</span><span class="mf">0.99</span><span class="p">)],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__dim_reduction__feat_selecter__k</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="sh">'</span><span class="s">all</span><span class="sh">'</span><span class="p">],</span>
    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__dim_reduction__feat_selecter__score_func</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span>
        <span class="n">f_regression</span><span class="p">,</span> <span class="n">mutual_info_regression</span><span class="p">],</span>

    <span class="c1"># Explore scalers
</span>    <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">prefix</span><span class="si">}</span><span class="s">numerical__scaler</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="nc">StandardScaler</span><span class="p">(),</span> <span class="nc">RobustScaler</span><span class="p">(),</span> <span class="nc">PowerTransformer</span><span class="p">()],</span>

    <span class="c1"># Explore regressor
</span>    <span class="sh">'</span><span class="s">regressor__ridge__alpha</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span>
<span class="p">}</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="nc">ParameterGrid</span><span class="p">(</span><span class="n">param_grid</span><span class="p">)))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>101376
</code></pre></div></div>

<p>As you can see, we have more than 100’000 different parameter combinations that we could explore. So using a
<code class="language-plaintext highlighter-rouge">GridSearchCV</code> routine and checking all of them individually would take way too much time. Luckily,
scikit-learn also provides a <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> routine, with which you can randomly explore a few parameter
grid combinations.</p>

<p>Furthermore, both <code class="language-plaintext highlighter-rouge">GridSearchCV</code> and <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> routines also allow you to change the performance
metric with which the model performs is scored. So let’s take <code class="language-plaintext highlighter-rouge">'neg_mean_absolute_percentage_error'</code> (for more
see <a href="https://scikit-learn.org/stable/modules/model_evaluation.html">here</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="nc">RandomizedSearchCV</span><span class="p">(</span>
    <span class="n">regressor</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="p">,</span>
    <span class="n">n_iter</span><span class="o">=</span><span class="mi">250</span><span class="p">,</span>
    <span class="n">refit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">neg_mean_absolute_percentage_error</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="6-train-model">6. Train model</h2>

<p>Everything is ready, so let’s go ahead and train the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">res</span> <span class="o">=</span> <span class="n">random_search</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Fitting 2 folds for each of 250 candidates, totalling 500 fits
</code></pre></div></div>

<h2 id="7-performance-investigation-after-randomizedsearchcv">7. Performance investigation after RandomizedSearchCV</h2>

<p>Once the model has explored a fixed number of grid points, we can go ahead and look at their performance. The
easiest is to just put everything into a pandas DataFrame and sort the entries by the best test score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create dataframe with results
</span><span class="n">df_res</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># Remove columns that are not relevant for the analysis
</span><span class="n">df_res</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">~</span><span class="n">df_res</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">time|split[0-9]*|rank|params</span><span class="sh">'</span><span class="p">)]</span>

<span class="c1"># Rename columns to make them more readable
</span><span class="n">new_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">param_regressor__</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="sh">'</span><span class="s">param_regressor</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">c</span> <span class="k">else</span> <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_res</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">new_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="sh">'</span><span class="s">preprocessor__</span><span class="sh">'</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">if</span> <span class="sh">'</span><span class="s">preprocessor__</span><span class="sh">'</span> <span class="ow">in</span> <span class="n">c</span> <span class="k">else</span> <span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">new_columns</span><span class="p">]</span>
<span class="n">df_res</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">new_columns</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="s">Top 10 parameter combinations:</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">df_res</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/03_scikit_dataframe_02.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 3: Results of RandomizedSearchCV showing top 10 parameter combinations ranked by mean test score. Each row represents a different combination of preprocessing and model parameters, helping identify the most effective configuration for the housing price prediction model.
</div>

<p>If you explore this table a bit you can better judge which parameter variations in your grid search are
actually useful and which ones aren’t. In this example we will not focus on this and directly continue with
computing the model performance on the training and test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate model performance on training and test set
</span><span class="n">score_tr</span> <span class="o">=</span> <span class="o">-</span><span class="n">random_search</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">score_te</span> <span class="o">=</span> <span class="o">-</span><span class="n">random_search</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">Prediction accuracy on train data: </span><span class="si">{</span><span class="n">score_tr</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n\
</span><span class="s">Prediction accuracy on test data:  </span><span class="si">{</span><span class="n">score_te</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Prediction accuracy on train data: 7.08%
Prediction accuracy on test data:  9.09%
</code></pre></div></div>

<p>Let’s interpret these regression metrics in practical terms:</p>
<ul>
  <li><strong>Train Error</strong>: On average, predictions deviate by about 7-8% from true house prices
    <ul>
      <li>For a $300,000 house, this means predictions are typically within ±$21,000-24,000</li>
    </ul>
  </li>
  <li><strong>Test Error</strong>: Slightly higher error on unseen data
    <ul>
      <li>For a $300,000 house, predictions are typically within ±$24,000-27,000</li>
    </ul>
  </li>
  <li><strong>Error Difference</strong>: Small gap indicates good generalization</li>
  <li><strong>Context</strong>: For house price prediction, ~8-9% error is relatively good considering market volatility</li>
</ul>

<p>Great, the score seems reasonably good! But now that we know better which preprocessing routine seems to be the
best (thanks to <code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code>), let’s go ahead and further fine-tune the ridge model.</p>

<h2 id="8-fine-tune-best-preprocessing-pipeline">8. Fine tune best preprocessing pipeline</h2>

<p>To further fine tune the best preprocessing pipeline, we can just load the ‘best estimator’ from the
<code class="language-plaintext highlighter-rouge">RandomizedSearchCV</code> exploration and specify a new parameter grid that we want to explore - this time with the
<code class="language-plaintext highlighter-rouge">GridSearchCV</code> routine (so that we look at all grid points).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Select best estimator
</span><span class="n">best_estimator</span> <span class="o">=</span> <span class="n">random_search</span><span class="p">.</span><span class="n">best_estimator_</span>

<span class="c1"># Specify new parameter grid to explore
</span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">regressor__ridge__alpha</span><span class="sh">'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">51</span><span class="p">)}</span>
</code></pre></div></div>

<p>To showcase one more additional thing, let’s go ahead and use a nested cross-validation routine to improve the
generalization power of our model. In other words, in contrast to the previous approach where we separated the
test from the train set only once, we will now also apply a cross validation approach on this split as well.
Together with the cross validation in the grid search, we therefore use cross validation twice, hence the name
“nested”.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Establish the two cross validations
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="n">inner_cv</span> <span class="o">=</span> <span class="nc">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">outer_cv</span> <span class="o">=</span> <span class="nc">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>And now, let’s combine all of this with and run the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">cross_validate</span>

<span class="c1"># Create grid search object with parameter grid and inner cross validation
</span><span class="n">grid_search</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span>
    <span class="n">best_estimator</span><span class="p">,</span>
    <span class="n">param_grid</span><span class="p">,</span>
    <span class="n">refit</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="n">inner_cv</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="sh">'</span><span class="s">neg_mean_absolute_percentage_error</span><span class="sh">'</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Train model with outer cross validation (and return estimators for post-model investigation)
</span><span class="n">cv_results</span> <span class="o">=</span> <span class="nf">cross_validate</span><span class="p">(</span>
    <span class="n">grid_search</span><span class="p">,</span>
    <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
    <span class="n">cv</span><span class="o">=</span><span class="n">outer_cv</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">return_estimator</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Once the model has finished training, we can extract the different scores from the most outer loop and print
their average score, as well as the standard deviation over the folds. Plus, the same thing can also be done
for the most optimal ridge model parameter ‘alpha’. These information can give us some insights about the model
generalization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_nested</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">cv_results</span><span class="p">)</span>
<span class="n">cv_train_scores</span> <span class="o">=</span> <span class="o">-</span><span class="n">df_nested</span><span class="p">[</span><span class="sh">'</span><span class="s">train_score</span><span class="sh">'</span><span class="p">]</span>
<span class="n">cv_test_scores</span> <span class="o">=</span> <span class="o">-</span><span class="n">df_nested</span><span class="p">[</span><span class="sh">'</span><span class="s">test_score</span><span class="sh">'</span><span class="p">]</span>
<span class="n">cv_alphas</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span><span class="p">.</span><span class="n">best_params_</span><span class="p">[</span><span class="sh">'</span><span class="s">regressor__ridge__alpha</span><span class="sh">'</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">df_nested</span><span class="p">[</span><span class="sh">'</span><span class="s">estimator</span><span class="sh">'</span><span class="p">]]</span>
<span class="nf">print</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">Generalization score with hyperparameters tuning:</span><span class="se">\n</span><span class="sh">"</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">  Train Score:    </span><span class="si">{</span><span class="n">cv_train_scores</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">% +/- </span><span class="si">{</span><span class="n">cv_train_scores</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n</span><span class="sh">"</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">  Test Score:     </span><span class="si">{</span><span class="n">cv_test_scores</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">% +/- </span><span class="si">{</span><span class="n">cv_test_scores</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n</span><span class="sh">"</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">  Optimal Alpha: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">cv_alphas</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> +/- </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="nf">std</span><span class="p">(</span><span class="n">cv_alphas</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="se">\n</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Generalization score with hyperparameters tuning:
  Train Score:    7.6% +/- 0.9%
  Test Score:     9.0% +/- 0.2%
  Optimal Alpha: 29.6 +/- 23.8
</code></pre></div></div>

<h1 id="9-feature-importance-investigation-with-permutation-testing">9. Feature importance investigation with permutation testing</h1>

<p>Some model provide some insights about feature importance (i.e. which features the model uses most for the
prediction). However, this is sometimes prone to multiple issues. A better approach is to use a permutation
approach. This approach performs the same model fitting (in this case based on the best model with the best
hyper parameters) but during each iteration randomly shuffles a given feature and investigates how this
perturbates the final score.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Select the best estimator with the best hyper parameter
</span><span class="n">final_estimator</span> <span class="o">=</span> <span class="n">grid_search</span><span class="p">.</span><span class="nf">set_params</span><span class="p">(</span>
    <span class="n">estimator__regressor__ridge__alpha</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">cv_alphas</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Fit this estimator to the initial training set
</span><span class="n">_</span> <span class="o">=</span> <span class="n">final_estimator</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
</code></pre></div></div>

<p>Now that the model is ready and trained, we can go ahead and perform the feature importance investigation via
permutation testing. To showcase one additional feature, let’s actually perform this routine twice, once while
focusing on the <code class="language-plaintext highlighter-rouge">r2</code> of the model, and once while focusing on the <code class="language-plaintext highlighter-rouge">neg_mean_absolute_percentage_error</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>

<span class="n">scoring</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">r2</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">neg_mean_absolute_percentage_error</span><span class="sh">'</span><span class="p">]</span>
<span class="n">result</span> <span class="o">=</span> <span class="nf">permutation_importance</span><span class="p">(</span>
    <span class="n">final_estimator</span><span class="p">,</span>
    <span class="n">X_te</span><span class="p">,</span>
    <span class="n">y_te</span><span class="p">,</span>
    <span class="n">n_repeats</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">scoring</span><span class="o">=</span><span class="n">scoring</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p>Once everything is computed, we can go ahead and plot the feature importance for each feature, separated by the
two different scoring metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">scoring</span><span class="p">):</span>

    <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">result</span><span class="p">[</span><span class="n">s</span><span class="p">].</span><span class="n">importances_mean</span><span class="p">.</span><span class="nf">argsort</span><span class="p">()</span>

    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">boxplot</span><span class="p">(</span>
        <span class="n">result</span><span class="p">[</span><span class="n">s</span><span class="p">].</span><span class="n">importances</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">vert</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">X_te</span><span class="p">.</span><span class="n">columns</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Permutation Importances (test set) | %s</span><span class="sh">"</span> <span class="o">%</span> <span class="n">s</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/03_scikit_feature_importance.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 4: Feature importance analysis using permutation importance method. Box plots show the impact of each feature on model performance across multiple permutations, with larger values indicating more important features.
</div>

<h2 id="summary-and-next-steps">Summary and Next Steps</h2>

<p>In this tutorial, we’ve covered advanced scikit-learn concepts:</p>
<ul>
  <li>Building complex preprocessing pipelines</li>
  <li>Handling mixed data types</li>
  <li>Feature selection and engineering automatically</li>
  <li>Implementing grid search with cross-validation</li>
  <li>Model comparison and evaluation</li>
  <li>Analyzing feature importance</li>
</ul>

<p><strong>Key takeaways:</strong></p>
<ol>
  <li>Preprocessing pipelines make complex workflows manageable</li>
  <li>Grid search helps find optimal parameters systematically</li>
  <li>Feature selection can improve model performance</li>
  <li>Understanding feature importance aids model interpretation</li>
  <li>Cross-validation provides robust performance estimates</li>
</ol>

<p>In Part 4, we’ll explore advanced neural network architectures with TensorFlow, building on both the neural network concepts from Part 2 and the preprocessing techniques we’ve learned here.</p>

<p><a href="/blog/2023/02_tensorflow_simple">← Previous: Deep Learning Fundamentals</a> or
<a href="/blog/2023/04_tensorflow_advanced">Next: Advanced Deep Learning →</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Exploring complex regression problems and preprocessing pipelines]]></summary></entry><entry><title type="html">Deep Learning Fundamentals - Building Neural Networks with TensorFlow</title><link href="https://miykael.github.io/blog/2023/02_tensorflow_simple/" rel="alternate" type="text/html" title="Deep Learning Fundamentals - Building Neural Networks with TensorFlow" /><published>2023-10-23T13:00:00+00:00</published><updated>2023-10-23T13:00:00+00:00</updated><id>https://miykael.github.io/blog/2023/02_tensorflow_simple</id><content type="html" xml:base="https://miykael.github.io/blog/2023/02_tensorflow_simple/"><![CDATA[<p>In this second part of our machine learning series, we’ll implement the same MNIST classification task using <a href="https://www.tensorflow.org/">TensorFlow</a>. While Scikit-learn excels at classical machine learning, TensorFlow shines when building neural networks. We’ll see how deep learning approaches differ from traditional methods and learn the basic concepts of neural network architecture.</p>

<p>The complete code for this tutorial can be found in the <a href="/assets/scripts/02_tensorflow_simple.py">02_tensorflow_simple.py</a> script.</p>

<h3 id="why-neural-networks">Why Neural Networks?</h3>

<p>While our Scikit-learn models performed well in Part 1, neural networks offer several key advantages for image classification:</p>
<ul>
  <li><strong>Automatic feature learning</strong>: No need to manually engineer features</li>
  <li><strong>Scalability</strong>: Can handle much larger datasets efficiently</li>
  <li><strong>Complex pattern recognition</strong>: Especially good at finding hierarchical patterns in data</li>
  <li><strong>State-of-the-art performance</strong>: Currently the best approach for many computer vision tasks</li>
</ul>

<p>Let’s see these advantages in action by building our own neural network for digit classification.</p>

<p>Let’s start by importing the necessary packages:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</code></pre></div></div>

<h2 id="1-load-and-prepare-dataset">1. Load and Prepare Dataset</h2>

<p>Unlike Scikit-learn, TensorFlow’s MNIST dataset comes in a slightly different format. We’ll keep the images in their original 2D shape (28x28 pixels) since neural networks can work directly with this structure - another advantage over traditional methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model parameters
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># One class for each digit (0-9)
</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Height, width, and channels (1 for grayscale)
</span>
<span class="c1"># Load dataset, already pre-split into train and test set
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="c1"># Scale pixel values to range [0,1] - this helps with training stability
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># Add channel dimension required by Conv2D layers
# Shape changes from (samples, height, width) to (samples, height, width, channels)
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">x_train shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">x_test shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_train shape: (60000, 28, 28, 1)
x_test shape: (10000, 28, 28, 1)
</code></pre></div></div>

<p>Our dataset dimensions represent:</p>
<ul>
  <li><strong>60,000 training samples</strong>: Much larger than scikit-learn’s version for better learning</li>
  <li><strong>28x28 pixels</strong>: Higher resolution images than Part 1’s 8x8 grid</li>
  <li><strong>1 channel</strong>: Grayscale images (RGB would be 3 channels)</li>
  <li><strong>10,000 test samples</strong>: Large test set for robust evaluation</li>
</ul>

<p>The final dimension (1) represents the color channel. Since MNIST contains grayscale images, we only need one channel, unlike RGB images which would have 3 channels.</p>

<p>Now that the data is loaded and scaled to appropriate range, we can go ahead and create the neural network
model. Given that our input are images, let’s go ahead and train a convolutional neural network. There are
multiple ways how we can set this up.</p>

<h2 id="2-create-neural-network-model">2. Create Neural Network Model</h2>

<p>For image classification, we’ll use a Convolutional Neural Network (CNN). CNNs are specifically designed to work with image data through specialized layers:</p>

<ul>
  <li><strong>Convolutional layers</strong>: Extract spatial features like edges, textures, and shapes</li>
  <li><strong>Pooling layers</strong>: Reduce spatial dimensions while preserving important features</li>
  <li><strong>Dense layers</strong>: Combine extracted features for final classification</li>
  <li><strong>Dropout layers</strong>: Prevent overfitting by randomly deactivating neurons during training</li>
</ul>

<p>There are multiple ways to define a model in TensorFlow. Let’s explore two common approaches:</p>

<h3 id="21-sequential-api">2.1. Sequential API</h3>
<p>The Sequential API is the simplest way to build neural networks - layers are stacked linearly, one after another:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define model architecture using Sequential API
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># First Convolutional Block (32 filters, each 3x3 in size, detect basic patterns)
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">),</span>

        <span class="c1"># MaxPooling2D: Reduces spatial dimensions by half while preserving features
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>

        <span class="c1"># Second Convolutional Block (64 filters, detect more complex patterns)
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

        <span class="c1"># MaxPooling2D: Reduces spatial dimensions by half while preserving features
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>

        <span class="c1"># Flatten 3D feature maps to 1D feature vector
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>

        <span class="c1"># Dense layers for final classification
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Prevents overfitting by randomly dropping 50% of connections
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>  <span class="c1"># Hidden layer combines features
</span>
        <span class="c1"># Output layer for classification
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="22-layer-by-layer-sequential-api">2.2. Layer-by-Layer Sequential API</h3>
<p>For more explicit control, we can separate each layer and activation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># More precise and sequential approach
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The two models are functionally identical, but the layer-by-layer approach offers several advantages:</p>
<ul>
  <li>Makes it easier to insert additional layers like BatchNormalization</li>
  <li>Provides more explicit activation functions</li>
  <li>Makes the data flow more transparent</li>
  <li>Allows finer control over layer parameters</li>
</ul>

<p>Next to this sequential API, there’s also a functional API.We’ll explore this more flexible approach in our advanced TensorFlow tutorial, which allows for:</p>
<ul>
  <li>Multiple inputs and outputs</li>
  <li>Layer sharing</li>
  <li>Non-sequential layer connections</li>
  <li>Complex architectures like residual networks</li>
</ul>

<p>Once the model is created, you can use the <code class="language-plaintext highlighter-rouge">summary()</code> method to get an overview of the network’s architecture
and the number of trainable and non-trainable parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "sequential"
_________________________________________________________________
Layer (type)                    Output Shape              Param #
=================================================================
conv2d (Conv2D)                 (None, 26, 26, 32)        320
re_lu (ReLU)                    (None, 26, 26, 32)        0
max_pooling2d (MaxPooling2D)    (None, 13, 13, 32)        0
conv2d_1 (Conv2D)               (None, 11, 11, 64)        18496
re_lu_1 (ReLU)                  (None, 11, 11, 64)        0
max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 64)          0
flatten (Flatten)               (None, 1600)              0
dropout (Dropout)               (None, 1600)              0
dense (Dense)                   (None, 32)                51232
re_lu_2 (ReLU)                  (None, 32)                0
dropout_1 (Dropout)             (None, 32)                0
dense_1 (Dense)                 (None, 10)                330
softmax (Softmax)               (None, 10)                0
=================================================================
Total params: 70,378 (274.91 KB)
Trainable params: 70,378 (274.91 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre></div></div>

<p>This summary tells us several important things:</p>
<ol>
  <li>Our model has 70,378 trainable parameters - relatively small by modern standards</li>
  <li>The input image (28x28x1) is progressively reduced in size through pooling (see the Output Shape column)</li>
  <li>The final dense layer has 10 outputs - one for each digit class</li>
  <li>Most parameters are in the dense layers, not the convolutional layers</li>
</ol>

<h2 id="3-train-tensorflow-model">3. Train TensorFlow model</h2>

<p>Before we can train the model we need to provide a few additional information:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">batch_size</code>: How many samples the model should look at once before performing the gradient descent.</li>
  <li><code class="language-plaintext highlighter-rouge">epochs</code>: For how many times the model should go through the full dataset.</li>
  <li><code class="language-plaintext highlighter-rouge">loss</code>: Which loss function the model should optimize for.</li>
  <li><code class="language-plaintext highlighter-rouge">metrics</code>: Which performance metrics the model should keep track of. By default this includes the loss metric.</li>
  <li><code class="language-plaintext highlighter-rouge">optimizer</code>: Which optimizer strategy the model should use. This could involve additional optimzation
parameters, such as the learning rate.</li>
  <li><code class="language-plaintext highlighter-rouge">validation_split</code> or <code class="language-plaintext highlighter-rouge">validation_data</code>: This parameter allows you to automatically split the training set
into a training and validation set (with <code class="language-plaintext highlighter-rouge">validation_split</code>) or you can also provide a specific validation
set with <code class="language-plaintext highlighter-rouge">validation_data</code>.</li>
</ul>

<p>Finding the right parameters for any of that, as well as establishing the right model architecture, is the
black arts of any deep learning practitioners. For this example, let’s just go with some proven default
parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training configuration
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># Number of samples processed before model update
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>       <span class="c1"># Number of complete passes through the dataset
</span>
<span class="c1"># Compile model with appropriate loss function and optimizer
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># Appropriate for integer labels
</span>    <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>                        <span class="c1"># Adaptive learning rate optimizer
</span>    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">]</span>                     <span class="c1"># Track accuracy during training
</span><span class="p">)</span>

<span class="c1"># Train the model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span>  <span class="c1"># Use 10% of training data for validation
</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/10
422/422 [==============================] - 4s 9ms/step - loss: 0.5902 - accuracy: 0.8117 - val_loss: 0.1014 - val_accuracy: 0.9700
Epoch 2/10
422/422 [==============================] - 4s 8ms/step - loss: 0.2183 - accuracy: 0.9364 - val_loss: 0.0674 - val_accuracy: 0.9808
Epoch 3/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1663 - accuracy: 0.9512 - val_loss: 0.0499 - val_accuracy: 0.9860
Epoch 4/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1390 - accuracy: 0.9599 - val_loss: 0.0462 - val_accuracy: 0.9875
Epoch 5/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1166 - accuracy: 0.9674 - val_loss: 0.0433 - val_accuracy: 0.9888
Epoch 6/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1046 - accuracy: 0.9693 - val_loss: 0.0370 - val_accuracy: 0.9902
Epoch 7/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0950 - accuracy: 0.9722 - val_loss: 0.0394 - val_accuracy: 0.9892
Epoch 8/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0891 - accuracy: 0.9742 - val_loss: 0.0400 - val_accuracy: 0.9895
Epoch 9/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0865 - accuracy: 0.9750 - val_loss: 0.0342 - val_accuracy: 0.9907
Epoch 10/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0775 - accuracy: 0.9773 - val_loss: 0.0355 - val_accuracy: 0.9905
</code></pre></div></div>

<p>Let’s analyze the training progression:</p>
<ul>
  <li><strong>Initial Performance (Epoch 1)</strong>:
    <ul>
      <li>Training: 81.17% accuracy, loss of 0.5902</li>
      <li>Validation: 97.00% accuracy, loss of 0.1014</li>
      <li>Shows rapid initial learning</li>
    </ul>
  </li>
  <li><strong>Final Performance (Epoch 10)</strong>:
    <ul>
      <li>Training: 97.73% accuracy, loss of 0.0775</li>
      <li>Validation: 99.05% accuracy, loss of 0.0355</li>
      <li>Excellent convergence with validation outperforming training</li>
    </ul>
  </li>
  <li><strong>Key Observations</strong>:
    <ul>
      <li>Consistent improvement across epochs</li>
      <li>Lower validation loss than training loss suggests good generalization</li>
      <li>Final accuracy exceeds our Scikit-learn model from Part 1</li>
      <li>No signs of overfitting as validation metrics remain stable</li>
    </ul>
  </li>
</ul>

<h2 id="4-model-investigation">4. Model investigation</h2>

<p>If we stored the <code class="language-plaintext highlighter-rouge">model.fit()</code> output in a <code class="language-plaintext highlighter-rouge">history</code> variable, we can easily access and visualize the different
model metrics during training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Store history in a dataframe
</span><span class="n">df_history</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>

<span class="c1"># Visualize training history
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">df_history</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">df_history</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">)].</span><span class="nf">plot</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Loss during training</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df_history</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">df_history</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)].</span><span class="nf">plot</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Accuracy during training</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [#]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [#]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/02_tensorflow_training_history.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 1: Training metrics over time showing model loss (left) and Mean Absolute Error (right) for both training and validation sets. The logarithmic scale helps visualize improvement across different magnitudes.
</div>

<p>Once the model is trained we can also compute its score on the test set. For this we can use the <code class="language-plaintext highlighter-rouge">evaluate()</code>
method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test loss:     </span><span class="si">{</span><span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test accuracy: </span><span class="si">{</span><span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test loss:     0.032
Test accuracy: 98.93%
</code></pre></div></div>

<p>And if you’re interested in the individual predictions, you can use the <code class="language-plaintext highlighter-rouge">predict()</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(10000, 10)
</code></pre></div></div>

<p>Given that our last layer uses a softmax activation, we actually don’t get just the class label back, but the
probability score for each class. To get to the class prediction, we therefore need to apply an argmax routine.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transform class probabilities to prediction labels
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create confusion matrix
</span><span class="n">cm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c1"># Visualize confusion matrix
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/02_tensorflow_confusion_matrix.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" />
</div>
<div class="caption">
    Figure 2: Confusion matrix showing model predictions across all digit classes. The strong diagonal pattern indicates high accuracy across all digits.
</div>

<h2 id="5-model-parameters">5. Model parameters</h2>

<p>And if you’re interested in the model parameters of the trained neural network, you can directly access them
via <code class="language-plaintext highlighter-rouge">model.layers</code>. One advantage of neural networks is their ability to learn hierarchical features. Let’s examine what our first convolutional layer learned:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract first hidden convolutional layers
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Transform the layer weights to a numpy array
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># Visualize the 32 kernels from the first convolutional layer
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(</span><span class="n">axs</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Kernel </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">weights</span><span class="p">[...,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/02_tensorflow_conv_kernels.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 3: Visualization of the 32 convolutional kernels learned by the first layer. Each 3x3 kernel acts as a feature detector, learning to identify basic patterns like edges, corners, and textures that are useful for digit recognition.
</div>

<h3 id="common-deep-learning-pitfalls">Common Deep Learning Pitfalls</h3>
<p>When starting with TensorFlow and neural networks, watch out for these common issues:</p>

<p><strong>Data Preparation</strong></p>
<ul>
  <li>(Almost) always scale input data (like we did with <code class="language-plaintext highlighter-rouge">/255.0</code>)</li>
  <li>Check for missing or invalid values</li>
  <li>Ensure consistent data types</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of proper data preparation
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
</code></pre></div></div>

<p><strong>Model Architecture</strong></p>
<ul>
  <li>Start simple, add complexity only if needed</li>
  <li>Match output layer to your task (softmax for classification)</li>
  <li>Use appropriate layer sizes</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of clear, progressive architecture
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># 10 classes
</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>Training Issues</strong></p>
<ul>
  <li>Monitor training metrics (loss not decreasing)</li>
  <li>Watch for overfitting (validation loss increasing)</li>
  <li>Use appropriate batch sizes</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Add validation monitoring during training
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</code></pre></div></div>

<p><strong>Memory Management</strong></p>
<ul>
  <li>Clear unnecessary variables</li>
  <li>Use appropriate data types</li>
  <li>Watch batch sizes on limited hardware</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Free memory after training
</span><span class="kn">import</span> <span class="n">gc</span>
<span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="n">keras</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="nf">clear_session</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="summary-and-next-steps">Summary and Next Steps</h2>

<p>In this tutorial, we’ve introduced neural networks using TensorFlow:</p>
<ul>
  <li>Building a CNN architecture</li>
  <li>Training with backpropagation</li>
  <li>Monitoring learning progress</li>
  <li>Visualizing learned features</li>
</ul>

<p>Our neural network achieved comparable accuracy to our Scikit-learn models (~99%), but this time on images with a higher resolution with the potential for even better performance through further optimization.</p>

<p><strong>Key takeaways:</strong></p>
<ol>
  <li>Neural networks can work directly with structured data like images</li>
  <li>Architecture design is crucial for good performance</li>
  <li>Training requires careful parameter selection</li>
  <li>Monitoring training helps detect problems early</li>
  <li>Visualizing learned features provides insights into model behavior</li>
</ol>

<p>In Part 3, we’ll explore more advanced machine learning concepts using Scikit-learn, focusing on regression problems and complex preprocessing pipelines.</p>

<p><a href="/blog/2023/01_scikit_simple">← Previous: Getting Started</a> or
<a href="/blog/2023/03_scikit_advanced">Next: Advanced Machine Learning →</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Building your first neural network for image classification]]></summary></entry><entry><title type="html">Getting Started with Machine Learning - Classification in Scikit-learn</title><link href="https://miykael.github.io/blog/2023/01_scikit_simple/" rel="alternate" type="text/html" title="Getting Started with Machine Learning - Classification in Scikit-learn" /><published>2023-10-23T12:00:00+00:00</published><updated>2023-10-23T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2023/01_scikit_simple</id><content type="html" xml:base="https://miykael.github.io/blog/2023/01_scikit_simple/"><![CDATA[<p>This post is part of a comprehensive machine learning series that takes you from basic classification to advanced neural networks. Throughout these tutorials, you’ll learn machine learning fundamentals using hands-on experience with real-world datasets and modern ML tools.</p>

<p>Have you ever wondered how to get started with machine learning? This series of posts will guide you through practical implementations using two of Python’s most popular frameworks: Scikit-learn and TensorFlow. Whether you’re a beginner looking to understand the basics or an experienced developer wanting to refresh your knowledge, we’ll progress from basic classification tasks to more advanced regression problems.</p>

<p>The series consists of four parts:</p>

<ol>
  <li><strong><a href="/blog/2023/01_scikit_simple">Getting Started with Classification using Scikit-learn</a></strong> (You are here)<br />Introduction to machine learning basics using the MNIST dataset</li>
  <li><strong><a href="/blog/2023/02_tensorflow_simple">Basic Neural Networks with TensorFlow</a></strong> (Part 2)<br />Building your first neural network for image classification</li>
  <li><strong><a href="/blog/2023/03_scikit_advanced">Advanced Machine Learning with Scikit-learn</a></strong> (Part 3)<br />Exploring complex regression problems and model optimization</li>
  <li><strong><a href="/blog/2023/04_tensorflow_advanced">Advanced Neural Networks with TensorFlow</a></strong> (Part 4)<br />Implementing sophisticated neural network architectures</li>
</ol>

<h3 id="why-these-tools">Why These Tools?</h3>

<p><a href="https://scikit-learn.org/stable/">Scikit-learn</a> is Python’s most popular machine learning library for a reason. It provides:</p>
<ul>
  <li>A consistent interface across different algorithms</li>
  <li>Extensive preprocessing capabilities</li>
  <li>Built-in model evaluation tools</li>
  <li>Excellent documentation and community support</li>
</ul>

<p><a href="https://www.tensorflow.org/">TensorFlow</a> complements Scikit-learn by offering:</p>
<ul>
  <li>Deep learning capabilities</li>
  <li>GPU acceleration</li>
  <li>Flexible model architecture design</li>
  <li>Production-ready deployment options</li>
</ul>

<p>In this first post, we’ll start with Scikit-learn and implement a basic classification task using the MNIST dataset. This will establish fundamental concepts that we’ll build upon in later posts.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Standard scientific Python imports
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
</code></pre></div></div>

<h2 id="1-load-dataset">1. Load dataset</h2>

<p>For our first machine learning task, we’ll use the famous MNIST dataset - a collection of handwritten digits that serves as a perfect introduction to image classification.  The MNIST dataset has become the “Hello World” of machine learning for good reason:</p>
<ul>
  <li>Simple to understand (handwritten digits from 0-9)</li>
  <li>Small enough to train quickly</li>
  <li>Complex enough to demonstrate real ML concepts</li>
  <li>Perfect for learning classification basics</li>
</ul>

<p>Let’s start by loading and exploring this dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load dataset
</span><span class="kn">from</span> <span class="n">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="n">digits</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">.</span><span class="nf">load_digits</span><span class="p">()</span>

<span class="c1"># Extract feature matrix X and target vector y
</span><span class="n">X</span> <span class="o">=</span> <span class="n">digits</span><span class="p">[</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">digits</span><span class="p">[</span><span class="sh">'</span><span class="s">target</span><span class="sh">'</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Dimension of X: </span><span class="si">{</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="se">\n</span><span class="s">Dimension of y: </span><span class="si">{</span><span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dimension of X: (1797, 64)
Dimension of y: (1797,)
</code></pre></div></div>

<p>Each of our 1,797 samples contains 64 features, representing an 8 x 8 pixel grid of an image. Let’s reshape these features into their original pixel grid format for visualization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">image</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">digits</span><span class="p">.</span><span class="n">images</span><span class="p">,</span> <span class="n">digits</span><span class="p">.</span><span class="n">target</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="sh">'</span><span class="s">nearest</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Label: %i</span><span class="sh">"</span> <span class="o">%</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_axis_off</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_digits_sample.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" />
    <div class="caption">
        Figure 1: Sample of MNIST digits showing different handwritten numbers from 0-9. Each image is an 8x8 pixel grayscale representation.
    </div>
</div>
<p><br /></p>

<h2 id="2-split-data-into-train-and-test-set">2. Split data into train and test set</h2>

<p>Next, we need to perform a train/test split so that we can validate the final performance of our trained model.
For the train/test split, we will use a 80:20 ratio. Furthermore, we will use the <code class="language-plaintext highlighter-rouge">stratify</code> parameter to
ensure that the class distribution in the train and test set is preserved.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="3-train-model">3. Train model</h2>

<p>For our first classification attempt, we’ll use a RandomForestClassifier. While there are many algorithms to choose from, Random Forests are an excellent starting point because they:</p>
<ul>
  <li>Handle both numerical and categorical data naturally</li>
  <li>Require minimal preprocessing</li>
  <li>Provide insights into feature importance</li>
  <li>Are relatively robust against overfitting</li>
  <li>Perform well even with default parameters</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Define type of classifier
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">()</span>

<span class="c1"># Train classifier on training data
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>

<span class="c1"># Evaluate model performance on training and test set
</span><span class="n">score_tr</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">score_te</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">Model accuracy on train data: </span><span class="si">{</span><span class="n">score_tr</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n\
</span><span class="s">Model accuracy on test data:  </span><span class="si">{</span><span class="n">score_te</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model accuracy on train data: 100.00%
Model accuracy on test data:  96.67%
</code></pre></div></div>

<p>The model’s performance metrics reveal several key insights:</p>
<ul>
  <li><strong>Perfect Training Accuracy (100%)</strong>: This suggests the model has completely memorized the training data, which could indicate overfitting.</li>
  <li><strong>Strong Test Accuracy (96.67%)</strong>: Despite potential overfitting, the model generalizes well to unseen data.</li>
  <li><strong>Train-Test Gap (3.33%)</strong>: The difference between train and test accuracy suggests some overfitting, but it’s within acceptable limits for this task.</li>
  <li><strong>Practical Impact</strong>: For digit recognition, 96.67% accuracy means the model would correctly identify about 967 out of 1000 handwritten digits, making it suitable for many real-world applications like postal code reading or form processing.</li>
</ul>

<p>As you can see, the model performed perfectly on the training set. No wonder, we tested the classifier’s
performance on the same data it was trained on. But is there way how we can improve the score on the test data?</p>

<p>Yes there is. But for this we need to fine-tune our random forest classifier. Because as of now we only used
the classifier with it’s default parameters.</p>

<h2 id="4-fine-tune-model">4. Fine-tune model</h2>

<p>To fine-tune our classifier model we need to split our dataset into a third part, the so called validation set.
In short, the <strong>training set</strong> is used to train the parameter of a model, the <strong>validation set</strong> is used to
fine-tune the hyperparameter of a model, and the <strong>test set</strong> is used to see how well the fine-tuned model
generalizes on never before seen data.</p>

<p>A common practice for model validation is k-fold <a href="https://scikit-learn.org/stable/modules/cross_validation.html">cross-validation</a>. In this approach, the
training data is iteratively split into training and validation sets, where each split (or fold) is used once
as the validation set.</p>

<p>Now, we also mentioned fine-tuning our model. One way to do this, is to perform a <a href="https://scikit-learn.org/stable/modules/grid_search.html">grid search</a>, i.e. running
the model with multiple parameter combinations and than deciding which ones work best.</p>

<p>Luckily, <code class="language-plaintext highlighter-rouge">scikit-learn</code> provides a neat routine that combines the cross-validation with the grid search, called
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"><code class="language-plaintext highlighter-rouge">GridSearchCV</code></a>.
So let’s go ahead and set everything up.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Define parameter grid
</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">max_depth</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span>  <span class="c1"># Controls tree depth - lower values reduce overfitting
</span>             <span class="sh">'</span><span class="s">n_estimators</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">]}</span>  <span class="c1"># Number of trees in forest - more trees = better generalization
</span>
<span class="c1"># Put parameter grid and classifier model into GridSearchCV
</span><span class="n">grid</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># 5-fold cross-validation for robust evaluation
</span>
<span class="c1"># Train classifier on training data
</span><span class="n">grid</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>

<span class="c1"># Evaluate model performance on training and test set
</span><span class="n">score_tr</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">score_te</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">Model accuracy on train data: </span><span class="si">{</span><span class="n">score_tr</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n\
</span><span class="s">Model accuracy on test data:  </span><span class="si">{</span><span class="n">score_te</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model accuracy on train data: 100.00%
Model accuracy on test data:  97.22%
</code></pre></div></div>

<p>Great, our score on the test set has improved. So let’s see which parameter combination seems to be the best.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show best random forest classifier
</span><span class="n">best_rf</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_rf</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>RandomForestClassifier(max_depth=25, n_estimators=200)
</code></pre></div></div>

<p>Now, to better understand how the different parameters relate to model performance, let’s plot the <code class="language-plaintext highlighter-rouge">max_depth</code>
and <code class="language-plaintext highlighter-rouge">n_estimators</code> with respect to the accuracy performance on the validation set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Put insights from cross-validation grid search into pandas dataframe
</span><span class="n">df_res</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">grid</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">df_res</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">mean_test_score|param_</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results in table (works only when we investigate two hyper-parameters).
</span><span class="n">result_table</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="nf">pivot</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="sh">'</span><span class="s">param_max_depth</span><span class="sh">'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="sh">'</span><span class="s">param_n_estimators</span><span class="sh">'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">result_table</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">.2f</span><span class="sh">'</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">RF Accuracy on validation set, based on model hyper-parameter</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">01_scikit_rf_heatmap.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_rf_heatmap.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" />
    <div class="caption">
        Figure 2: Heatmap showing model accuracy (%) for different combinations of SVM hyperparameters gamma and C. Darker colors indicate better performance.
    </div>
</div>
<p><br /></p>

<h2 id="5-change-model">5. Change model</h2>

<p>The great thing about <code class="language-plaintext highlighter-rouge">scikit-learn</code> is that the framework is very dynamic. The only thing we need to change to
do the same classification with a Support Vector Machine (SVM) for example, is changing the model and the
parameter grid we want to explore.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create support vector classifier object
</span><span class="kn">from</span> <span class="n">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="n">clf</span> <span class="o">=</span> <span class="nc">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="sh">'</span><span class="s">rbf</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Define parameter grid
</span><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">C</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span>  <span class="c1"># Regularization parameter - higher values = more complex decision boundary
</span>    <span class="sh">'</span><span class="s">gamma</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.00001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>  <span class="c1"># Kernel coefficient - higher values = more influence from nearby points
</span><span class="p">}</span>
</code></pre></div></div>

<p>That’s it! The rest can be used as before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Put parameter grid and classifier model into GridSearchCV
</span><span class="n">grid</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Train classifier on training data
</span><span class="n">grid</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>

<span class="c1"># Evaluate model performance on training and test set
</span><span class="n">score_tr</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">score_te</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="sh">"</span><span class="s">Model accuracy on train data: </span><span class="si">{</span><span class="n">score_tr</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="se">\n\
</span><span class="s">Model accuracy on test data:  </span><span class="si">{</span><span class="n">score_te</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span>
<span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model accuracy on train data: 100.00%
Model accuracy on test data:  98.89%
</code></pre></div></div>

<p>These results show significant improvements:</p>
<ul>
  <li><strong>Test Accuracy (98.89%)</strong>: The SVM correctly identifies 989 out of 1000 digits</li>
  <li><strong>Improvement (+2.22%)</strong>: Compared to Random Forest, SVM reduces errors by about 67%</li>
</ul>

<p>Nice, this is much better. It seems for this particular dataset, with the hyper-parameter’s we explored, SVM
is a better model type.</p>

<p>As before, let’s take a look at the model with the best parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show best SVM classifier
</span><span class="n">best_svm</span> <span class="o">=</span> <span class="n">grid</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_svm</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SVC(C=10.0, gamma=0.001)
</code></pre></div></div>

<p>And once more, how do these two hyper-parameters relate to the performance metric <code class="language-plaintext highlighter-rouge">accuracy</code> in the validation
set?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Put insights from cross-validation grid search into pandas dataframe
</span><span class="n">df_res</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">grid</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">df_res</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">mean_test_score|param_</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">df_res</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># Plot results in table (works only when we investigate two hyper-parameters).
</span><span class="n">result_table</span> <span class="o">=</span> <span class="n">df_res</span><span class="p">.</span><span class="nf">pivot</span><span class="p">(</span>
    <span class="n">index</span><span class="o">=</span><span class="sh">'</span><span class="s">param_gamma</span><span class="sh">'</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="sh">'</span><span class="s">param_C</span><span class="sh">'</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span>
<span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">result_table</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">.2f</span><span class="sh">'</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">SVM Accuracy on validation set, based on model hyper-parameter</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">01_scikit_svm_heatmap.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_svm_heatmap.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" />
    <div class="caption">
        Figure 3: Confusion matrix showing the model's prediction performance across all digit classes. Diagonal elements represent correct predictions.
    </div>
</div>
<p><br /></p>

<h2 id="6-post-model-investigation">6. Post-model investigation</h2>

<p>Last but certainly not least, let’s investigate the prediction quality of our classifier. Two great routines
that you can use for that are <code class="language-plaintext highlighter-rouge">scikit-learn</code>s
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"><code class="language-plaintext highlighter-rouge">classification_report</code></a> and
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"><code class="language-plaintext highlighter-rouge">confusion_matrix</code></a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Predict class predictions on the test set
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">best_svm</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Print classification report
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support
           0       1.00      1.00      1.00        36
           1       1.00      1.00      1.00        36
           2       1.00      1.00      1.00        35
           3       1.00      1.00      1.00        37
           4       1.00      1.00      1.00        36
           5       0.97      0.97      0.97        37
           6       1.00      1.00      1.00        36
           7       0.97      1.00      0.99        36
           8       0.97      1.00      0.99        35
           9       0.97      0.92      0.94        36
    accuracy                           0.99       360
   macro avg       0.99      0.99      0.99       360
weighted avg       0.99      0.99      0.99       360
</code></pre></div></div>

<p>As you can see, while the scores are comparable between classes, some clearly are harder to detect than others.
To help better understand which target classes are confused more often than others, we can look at the
confusion matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute confusion matrix
</span><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>

<span class="n">cm</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">),</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted Class</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">True Class</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">01_scikit_confusion_matrix.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_confusion_matrix.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" />
    <div class="caption">
        Figure 4: Feature importance heatmap showing which pixels in the 8x8 grid contribute most to the Random Forest's classification decisions.
    </div>
</div>
<p><br /></p>

<h2 id="7-additional-model-and-results-investigations">7. Additional model and results investigations</h2>

<p>Depending on the classifier model you chose, you can investigate many additional things, once your model is
trained.</p>

<p>For example, <code class="language-plaintext highlighter-rouge">RandomForest</code> model provide a <code class="language-plaintext highlighter-rouge">feature_importances_</code> attribute that allows you to investigate
which of your features is helping the most with the classification task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Collect feature importances from RF model
</span><span class="n">feat_import</span> <span class="o">=</span> <span class="n">best_rf</span><span class="p">.</span><span class="n">feature_importances_</span>

<span class="c1"># Putting the 64 feature importance values back into 8x8 pixel grid
</span><span class="n">feature_importance_image</span> <span class="o">=</span> <span class="n">feat_import</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>

<span class="c1"># Visualize the feature importance grid
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">feature_importance_image</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">RF Feature Importance</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">01_scikit_feature_importance.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_feature_importance.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" />
    <div class="caption">
        Figure 5: Most confidently predicted digits from the test set, showing examples where the model has highest prediction probabilities.
    </div>
</div>
<p><br /></p>

<p>As you can see, feature in the center of the 8x8 grid seem to be more important for the classification task.</p>

<p>Other interesting post-modeling tasks could be the investigation of the prediction probabilities per sample.
For example: What do images look like of digits with 100% prediction probability?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute prediction probabilities
</span><span class="n">y_prob</span> <span class="o">=</span> <span class="n">best_rf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>

<span class="c1"># Extract prediction probabilities of target class
</span><span class="n">target_prob</span> <span class="o">=</span> <span class="p">[</span><span class="n">e</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">e</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">y_prob</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)]</span>

<span class="c1"># Plot images of easiest to predict samples
</span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">target_prob</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">X_te</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="sh">'</span><span class="s">nearest</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_axis_off</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">01_scikit_confident_predictions.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_confident_predictions.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 6: Most confidently predicted digits from the test set, showing examples where the model has highest prediction probabilities.
</div>

<p>And what about the difficult cases? For which digits does the model strugle the most to get above chance level?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot images of easiest to predict samples
</span><span class="n">_</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">target_prob</span><span class="p">)):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">X_te</span><span class="p">[</span><span class="n">idx</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="p">.</span><span class="n">cm</span><span class="p">.</span><span class="n">gray_r</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="sh">'</span><span class="s">nearest</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_axis_off</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">'</span><span class="s">01_scikit_uncertain_predictions.png</span><span class="sh">'</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">'</span><span class="s">tight</span><span class="sh">'</span><span class="p">,</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/01_scikit_uncertain_predictions.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>
<div class="caption">
    Figure 7: Most challenging digits for the model to predict, showing examples where the model has lowest prediction confidence.
</div>

<h2 id="common-pitfalls-in-machine-learning-classification">Common Pitfalls in Machine Learning Classification</h2>

<p>Before wrapping up, let’s discuss some important pitfalls to avoid when working on classification tasks:</p>

<p><strong>Data Leakage</strong>: Always split your data before any preprocessing or feature engineering</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Wrong: Preprocessing before split
</span><span class="n">X_scaled</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Correct: Split first, then preprocess
</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">X_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">X_tr_scaled</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
<span class="n">X_te_scaled</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="p">.</span><span class="nf">scale</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Class Imbalance</strong>: Always check your class distribution</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show absolute and relative frequencies
</span><span class="n">class_dist</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="nf">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Class distribution (%):</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">class_dist</span><span class="p">.</span><span class="nf">mul</span><span class="p">(</span><span class="mi">100</span><span class="p">).</span><span class="nf">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Visualize distribution
</span><span class="n">class_dist</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="sh">'</span><span class="s">bar</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">'</span><span class="s">Class Distribution</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Class</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Frequency (%)</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Overfitting</strong>: Monitor these warning signs
    - Large gap between training and validation scores
    - Perfect training accuracy (like we saw with RandomForest)
    - Poor generalization to new data</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use cross-validation for robust estimates
</span><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>

<span class="n">scores</span> <span class="o">=</span> <span class="nf">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">CV Scores: </span><span class="si">{</span><span class="n">scores</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Mean: </span><span class="si">{</span><span class="n">scores</span><span class="p">.</span><span class="nf">mean</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s"> (±</span><span class="si">{</span><span class="n">scores</span><span class="p">.</span><span class="nf">std</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Memory Management</strong>: For large datasets, consider these approaches</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use n_jobs parameter for parallel processing
</span><span class="n">rf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># Use all available cores
</span>
<span class="c1"># Or batch processing with random forests
</span><span class="n">rf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">max_samples</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>  <span class="c1"># Use 80% of samples per tree
</span></code></pre></div></div>

<p><strong>Feature Scaling</strong>: Different algorithms have different scaling requirements</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># SVM requires scaling, Random Forests don't
</span><span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># For SVM
</span><span class="n">scaler</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">()</span>
<span class="n">X_tr_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
<span class="n">X_te_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>

<span class="c1"># Random Forests can handle unscaled data
</span><span class="n">rf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>  <span class="c1"># No scaling needed
</span></code></pre></div></div>

<p><strong>Model Selection Bias</strong>: Don’t use test set for model selection</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Wrong: Using test set for parameter tuning
</span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">parameters</span><span class="p">:</span>
    <span class="n">clf</span><span class="p">.</span><span class="nf">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">param</span><span class="p">)</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">).</span><span class="nf">score</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>  <span class="c1"># Don't do this!
</span>
<span class="c1"># Correct: Use cross-validation
</span><span class="n">grid</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">grid</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="c1"># Only use test set for final evaluation
</span></code></pre></div></div>

<p><strong>Model Troubleshooting Tips</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check for data issues first
</span><span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Missing values:</span><span class="sh">"</span><span class="p">,</span> <span class="n">X</span><span class="p">.</span><span class="nf">isnull</span><span class="p">().</span><span class="nf">sum</span><span class="p">().</span><span class="nf">sum</span><span class="p">())</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Infinite values:</span><span class="sh">"</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">isinf</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">).</span><span class="nf">sum</span><span class="p">())</span>

<span class="c1"># Verify predictions are valid
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
<span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">unique</span><span class="p">(</span><span class="n">y_pred</span><span class="p">))</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Warning: Model predicting single class!</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Check probability calibration
</span><span class="n">y_prob</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
<span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="nf">any</span><span class="p">(</span><span class="n">y_prob</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">)</span> <span class="ow">or</span> <span class="n">np</span><span class="p">.</span><span class="nf">any</span><span class="p">(</span><span class="n">y_prob</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Warning: Invalid probability predictions!</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Common Error Messages and Solutions</strong></p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">ValueError: Input contains NaN</code>: Clean your data before training</li>
  <li><code class="language-plaintext highlighter-rouge">MemoryError</code>: Reduce batch size or use data generators</li>
</ul>

<h2 id="summary-and-next-steps">Summary and Next Steps</h2>

<p>In this first tutorial, we’ve covered the fundamentals of machine learning with Scikit-learn:</p>
<ul>
  <li>Loading and visualizing data</li>
  <li>Splitting data into training and test sets</li>
  <li>Training a basic classifier</li>
  <li>Fine-tuning model parameters</li>
  <li>Evaluating model performance</li>
</ul>

<p>We’ve seen how Scikit-learn’s consistent API makes it easy to experiment with different algorithms and preprocessing techniques. The RandomForest classifier achieved 97.22% accuracy, while the SVM performed even better at 98.89%.</p>

<p>In the next post, we’ll tackle the same MNIST classification problem using TensorFlow, introducing neural networks and deep learning concepts. This will help you understand the differences between classical machine learning approaches and deep learning, and when to use each.</p>

<p><strong>Key takeaways:</strong></p>
<ol>
  <li>Even simple models can achieve good performance on well-structured problems</li>
  <li>Start with simple models and gradually increase complexity</li>
  <li>Cross-validation is crucial for reliable performance estimation</li>
  <li>Grid search helps find optimal parameters systematically</li>
  <li>Always keep a separate test set for final evaluation</li>
  <li>Look beyond accuracy to understand model performance</li>
</ol>

<p>In Part 2, we’ll explore how neural networks approach the same problem using TensorFlow, introducing deep learning concepts and comparing the two approaches.</p>

<p><a href="/blog/2023/02_tensorflow_simple">Next: Deep Learning Fundamentals →</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Getting started with machine learning using Scikit-learn's classification tools]]></summary></entry><entry><title type="html">Age prediction of a speaker’s voice</title><link href="https://miykael.github.io/blog/2022/audio_eda_and_modeling/" rel="alternate" type="text/html" title="Age prediction of a speaker’s voice" /><published>2022-02-16T12:00:00+00:00</published><updated>2022-02-16T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2022/audio_eda_and_modeling</id><content type="html" xml:base="https://miykael.github.io/blog/2022/audio_eda_and_modeling/"><![CDATA[<h2 id="how-to-perform-eda-and-data-modeling-on-audio-data">How to perform EDA and data modeling on audio data</h2>

<p><em>[Find the Jupyter Notebook to this article <a href="https://github.com/miykael/miykael.github.io/blob/master/assets/nb/04_audio_data_analysis/nb_audio_eda_and_modeling.ipynb">here</a>.]</em></p>

<hr />

<p>Most people are familiar with how to run a data science project on image, text or tabular data. But not many have experience with analyzing audio data. In this article, we will learn how we can do exactly that. How to prepare, explore and analyze audio data with the help of machine learning. In short: As for all other modalities (e.g. text or images) as well, the trick is to get the data into a machine interpretable format.</p>

<p>The interesting thing with audio data is that you can treat it as many different modalities:</p>

<ul>
  <li>You can extract <strong>high-level</strong> features and analyze the data like <strong>tabular</strong> data.</li>
  <li>You can compute <strong>frequency plots</strong> and analyze the data like <strong>image</strong> data.</li>
  <li>You can use <strong>temporal sensitive models</strong> and analyze the data like <strong>time-series</strong> data.</li>
  <li>You can use <strong>speech-to-text models</strong> and analyze the data like <strong>text</strong> data.</li>
</ul>

<p>In this article we will look at the first three approaches. But first, let’s take a closer look at what audio data actually looks like.</p>

<h1 id="1-the-many-facets-of-audio-data">1. The many facets of audio data</h1>

<p>While there are multiple Python libraries that allow you to work with audio data, for this example, we will be using <a href="https://librosa.org/doc/main/index.html">librosa</a>. So, let’s load an MP3 file and plot its content.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use this code snippet to suppress all 'librosa' related UserWarnings
</span><span class="kn">import</span> <span class="n">warnings</span>
<span class="n">warnings</span><span class="p">.</span><span class="nf">filterwarnings</span><span class="p">(</span><span class="sh">"</span><span class="s">ignore</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Import librosa
</span><span class="kn">import</span> <span class="n">librosa</span>

<span class="c1"># Loads mp3 file with a specific sampling rate, here 16kHz
</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">c4_sample-1.mp3</span><span class="sh">"</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>

<span class="c1"># Plot the signal stored in 'y'
</span><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">librosa.display</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Audio signal as waveform</span><span class="sh">"</span><span class="p">)</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">waveplot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_3_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>What you see here is the <strong>waveform</strong> representation of the spoken sentence: “<strong><em>he just got a new kite for his birthday</em></strong>”.</p>

<h2 id="11-waveform---signal-in-the-time-domain">1.1. Waveform - signal in the time-domain</h2>

<p>Before we called it time-series data, but now we name it waveform? Well, it’s both. This becomes clearer when we look only at a small segment of this audio file. The following illustration shows the same thing as above, but this time only 62.5 milliseconds of it.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="mi">17500</span><span class="p">:</span><span class="mi">18500</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_6_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>What you can see is a temporal signal that oscillates around the value 0 with different frequencies and amplitudes.This signal represents the air pressure change over time, or the physical displacement of a loud speaker’s membrane (or the membrane in your ear for that matter). That’s why this depiction of the audio data is also called <strong>waveform</strong>.</p>

<p>The <strong>frequency</strong> is the speed with which this signal oscillates. Low frequency, e.g. 60 Hz could be the sound of bass guitar, while a birds song could be in the higher frequency of 8000 Hz. Human speech is usually anywhere between that.</p>

<p>To know how quickly this signal needs to be interpret, we also need to know the <strong>sampling rate</strong> at which the data was recorded. In this case, the sampling rate per second was 16’000 or 16k Hz. Which means that the 1’000 time points we can see in the previous figure represents 62.5 milliseconds (1000/16000 = 0.0625) of audio signal.</p>

<h2 id="12-the-fourier-transform---signal-in-the-frequency-domain">1.2. The Fourier Transform - signal in the frequency domain</h2>

<p>While the previous visualization can tell us when something happens (i.e. around 2 seconds there seem to be a lot of waveforms), it cannot really tell us with what frequency it happens. Because the waveform shows us information about the when, this signal is also said to be in the <strong>time domain</strong>.</p>

<p>Using a fast fourier transformation, we can invert this issue and get a clear information about what frequencies are present, while loosing all information about the when. In such a case, the signal representation is said to be in the <strong>frequency domain</strong>.</p>

<p>Let’s see what our spoken sentence from before looks like represented in the frequency domain.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">scipy</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Applies fast fourier transformation to the signal and takes absolute values
</span><span class="n">y_freq</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">scipy</span><span class="p">.</span><span class="n">fftpack</span><span class="p">.</span><span class="nf">fft</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Establishes all possible frequency (dependent on the sampling rate and the length of the signal)
</span><span class="n">f</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sr</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">y_freq</span><span class="p">))</span>

<span class="c1"># Plot audio signal as frequency information.
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">semilogx</span><span class="p">(</span><span class="n">f</span><span class="p">[:</span> <span class="nf">len</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">],</span> <span class="n">y_freq</span><span class="p">[:</span> <span class="nf">len</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Frequency (Hz)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_9_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>What you can see here is that most of the signal is somewhere between ~100 and ~1000 Hz (i.e. between $10^2$ and  $10^3$). Plus there seem to be some additional stuff from 1’000 to 10’000 Hz.</p>

<h2 id="13-spectrogram">1.3. Spectrogram</h2>

<p>Luckily, we don’t always need to decide for either the time or frequency domain. Using a <strong>spectrogram</strong> plot, we can profit from both domains, while keeping most of their handicaps minimal. There are multiple ways how you can create such spectrogram plots, but for this article let’s take a look at three in particular.</p>

<h3 id="131-short-time-fourier-transform-stft">1.3.1. Short-time Fourier transform (STFT)</h3>

<p>Using a small adapted version of the fast fourier transformation before, namely the <strong>short-time fourier transformation</strong> (STFT), we can create such a spectrogram. The small trick that is applied here is that the FFT is computed for multiple small time windows (hence “short-time fourier”) in a sliding window manner.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">librosa.display</span>

<span class="c1"># Compute short-time Fourier Transform
</span><span class="n">x_stft</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">librosa</span><span class="p">.</span><span class="nf">stft</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>

<span class="c1"># Apply logarithmic dB-scale to spectrogram and set maximum to 0 dB
</span><span class="n">x_stft</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">amplitude_to_db</span><span class="p">(</span><span class="n">x_stft</span><span class="p">,</span> <span class="n">ref</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">)</span>

<span class="c1"># Plot STFT spectrogram
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">x_stft</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">log</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="sh">"</span><span class="s">%+2.0f dB</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_13_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>As in all spectrogram plots, the color represents the amount (loudness/volume) of a given frequency, at a given timepoint. +0dB is the loudest, and -80dB is close to silence. On the horizontal x-axis we can see the time, while on the vertical y-axis we can see the different frequencies.</p>

<h3 id="132-mel-spectrogram">1.3.2. Mel spectrogram</h3>

<p>As an alternative to the STFT, you can also compute the <strong>mel spectrogram</strong>, which is based on the <a href="https://en.wikipedia.org/wiki/Mel_scale">mel scale</a>. This scale accounts for the way we human perceive a sound’s pitch. The mel scale is calculated so that two pairs of frequencies separated by a delta in the mel scale are perceived by humans as having the same perceptual difference.</p>

<p>The mel spectrogram is computed very similar to the STFT, the main difference is just that the y-axis uses a different scale.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute the mel spectrogram
</span><span class="n">x_mel</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">melspectrogram</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>

<span class="c1"># Apply logarithmic dB-scale to spectrogram and set maximum to 0 dB
</span><span class="n">x_mel</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">power_to_db</span><span class="p">(</span><span class="n">x_mel</span><span class="p">,</span> <span class="n">ref</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">)</span>

<span class="c1"># Plot mel spectrogram
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">x_mel</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">mel</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="sh">"</span><span class="s">%+2.0f dB</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_16_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>The difference to the STFT might not be too obvious first, but if you take a closer look, you can see that in the STFT plot, the frequency from 0 to 512 Hz take much more space on the y-axis than in the mel plot.</p>

<h3 id="133-mel-frequency-cepstral-coefficients-mfccs">1.3.3. Mel-frequency cepstral coefficients (MFCCs)</h3>

<p>The <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">Mel-frequency cepstral coefficients</a> (MFCCs) are an alternative representation of the mel spectrogram from before. The advantage of the MFCCs over the mel-spectrogram are the rather small number of features (i.e. unique horizontal lines), usually ~20.</p>

<p>Due to the fact that the mel spectrogram is closer to the way we human perceive pitch and that the MFCCs only has a few number of component features, most machine learning practitioner prefer the MFCCs way of representing audio data in an ‘image way’. Which isn’t to say that for a given problem an STFT, mel or waveform representation might work better.</p>

<p>So, lets go ahead and compute the MFCCs and plot them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract 'n_mfcc' numbers of MFCCs components (here 20)
</span><span class="n">x_mfccs</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">mfcc</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">n_mfcc</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Plot MFCCs
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">x_mfccs</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_19_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h1 id="2-data-cleaning">2. Data cleaning</h1>

<p>Now that we understand a bit better what audio data looks like, let’s visualize a few more examples. <strong>Note:</strong> You can download these four examples via these links: <a href="https://www.dropbox.com/scl/fi/zk1pavpavoifw7r3ek8ac/c4_sample-1.mp3?rlkey=chvsev8wrkn4mrnwwz5o8kgnd&amp;dl=1">Audio 1</a>, <a href="https://www.dropbox.com/scl/fi/ls71sjgcc7j3jtsz4bbxp/c4_sample-2.mp3?rlkey=qbe9dh2548r7juji7xo7kh465&amp;dl=1">Audio 2</a>, <a href="https://www.dropbox.com/scl/fi/jyxonbiv82nsulmnc415h/c4_sample-3.mp3?rlkey=q7icmpyd5io0n3apnlk6thsys&amp;dl=1">Audio 3</a>, <a href="https://www.dropbox.com/scl/fi/b0mxezlc5wqgp8fqzjcfz/c4_sample-4.mp3?rlkey=dg94biwlrxr46rg3t43tofbna&amp;dl=1">Audio 4</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Visualization of four mp3 files
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
    <span class="n">fname</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c4_sample-%d.mp3</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>
    <span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">waveplot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_21_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>From these four examples, and more importantly, when listening to them, we can gather a few more insights about this audio dataset:</p>

<ol>
  <li>Most recordings have a long silence period at the beginning and the end of the recording (see sample 1 and 2). This is something we should take care of with ‘trimming’.</li>
  <li>However, in some cases, these silence period are interrupted by a ‘click’, due to the pressing and releasing of the recording buttons (see sample 2).</li>
  <li>Some audio recording don’t have such silence phase, i.e. a straight line (see sample 3 and 4). When listening to these recordings we can observe that this is due to a lot of background noise.</li>
</ol>

<p>To better understand how this is represented in the frequency domain, let’s look at the corresponding STFT spectrograms.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The code is the same as before, using the stft-spectrogram routine
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
    <span class="n">fname</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c4_sample-%d.mp3</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>
    <span class="n">x_stft</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">librosa</span><span class="p">.</span><span class="nf">stft</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">x_stft</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">amplitude_to_db</span><span class="p">(</span><span class="n">x_stft</span><span class="p">,</span> <span class="n">ref</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">)</span>
    <span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">x_stft</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">log</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_23_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>When we listen to the audio recordings we can observe that sample 3 has varying background noise covering multiple frequencies, while the background noise in sample 4 is rather constant. This is also what we see in the figures above. Sample 3 is very noisy throughout, while sample 4 is noisy only on a few frequencies (i.e. the thick horizontal lines). For now we won’t go into detail of how such noise could be removed, as this would be beyond the scope of this article.</p>

<p>So, let’s look into a ‘short-cut’ of how we could remove such noise, and trim the audio samples. While a more manual approach, using custom filtering functions, might be the best approach to remove noise from audio data, in our case we will go ahead and use the practical python package <a href="https://github.com/timsainb/noisereduce">noisereduce</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">noisereduce</span> <span class="k">as</span> <span class="n">nr</span>
<span class="kn">from</span> <span class="n">scipy.io</span> <span class="kn">import</span> <span class="n">wavfile</span>

<span class="c1"># Loop through all four samples
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>

    <span class="c1"># Load audio file
</span>    <span class="n">fname</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c4_sample-%d.mp3</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>

    <span class="c1"># Remove noise from audio sample
</span>    <span class="n">reduced_noise</span> <span class="o">=</span> <span class="n">nr</span><span class="p">.</span><span class="nf">reduce_noise</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">stationary</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># Save output in a wav file as mp3 cannot be saved to directly
</span>    <span class="n">wavfile</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">fname</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">.mp3</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">.wav</span><span class="sh">"</span><span class="p">),</span> <span class="n">sr</span><span class="p">,</span> <span class="n">reduced_noise</span><span class="p">)</span>
</code></pre></div></div>

<p>If you listen to the created wav files, you can hear that the noise is almost completely gone. Yes, we also introduced a few more artifacts, but overall, we hope that our noise removal approach did more good than harm.</p>

<p>For the trimming step we can use librosa’s <code class="language-plaintext highlighter-rouge">.effects.trim()</code> function. Note, each dataset might need a different <code class="language-plaintext highlighter-rouge">top_db</code> parameter for the trimming, so best is to try out a few versions and see what works well. In our case it is <code class="language-plaintext highlighter-rouge">top_db=20</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Loop through all four samples
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>

    <span class="c1"># Load audio file
</span>    <span class="n">fname</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c4_sample-%d.wav</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>

    <span class="c1"># Trim signal
</span>    <span class="n">y_trim</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">effects</span><span class="p">.</span><span class="nf">trim</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">top_db</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

    <span class="c1"># Overwrite previous wav file
</span>    <span class="n">wavfile</span><span class="p">.</span><span class="nf">write</span><span class="p">(</span><span class="n">fname</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">"</span><span class="s">.mp3</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">.wav</span><span class="sh">"</span><span class="p">),</span> <span class="n">sr</span><span class="p">,</span> <span class="n">y_trim</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s now take another look at the cleaned data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
    <span class="n">fname</span> <span class="o">=</span> <span class="sh">"</span><span class="s">c4_sample-%d.wav</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>
    <span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">waveplot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_29_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Much better!</p>

<h1 id="3-feature-extraction">3. Feature extraction</h1>

<p>Now that our data is clean, let’s go ahead and look into a few audio-specific feature that we could extract. But first, let’s load a file.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Load data for sample 1
</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">c4_sample-1.wav</span><span class="sh">"</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="31-onset-detection">3.1. Onset detection</h2>

<p>Looking at the waveform of a signal, librosa can reasonably well identify the onset of a new spoken word.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract onset timestamps of words
</span><span class="n">onsets</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">onset</span><span class="p">.</span><span class="nf">onset_detect</span><span class="p">(</span>
    <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">units</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">backtrack</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Plot onsets together with waveform plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">waveplot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">onsets</span><span class="p">:</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">"</span><span class="s">r</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Return number of onsets
</span><span class="n">number_of_words</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">onsets</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">number_of_words</span><span class="si">}</span><span class="s"> onsets were detected in this audio signal.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_34_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<pre><code class="language-code">7 onsets were detected in this audio signal
</code></pre>

<h2 id="32-length-of-an-audio-recording">3.2. Length of an audio recording</h2>

<p>Very much related to this is the length of an audio recording. The longer the recording, the more words can be spoken. So let’s compute the length of the recording and the speed at which words are spoken.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Computes duration in seconds
</span><span class="n">duration</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">/</span> <span class="n">sr</span>
<span class="n">words_per_second</span> <span class="o">=</span> <span class="n">number_of_words</span> <span class="o">/</span> <span class="n">duration</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"""</span><span class="s">The audio signal is </span><span class="si">{</span><span class="n">duration</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> seconds long,
with an average of </span><span class="si">{</span><span class="n">words_per_second</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> words per seconds.</span><span class="sh">"""</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">The audio signal is 1.70 seconds long,
with an average of 4.13 words per seconds.
</code></pre>

<h2 id="33-tempo">3.3. Tempo</h2>

<p>Language is a very melodic signal, and each of us has a unique way and speed of speaking. Therefore, another feature that we could extract is the tempo of our speech, i.e. the number of beats that can be detected in an audio signal.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Computes the tempo of a audio recording
</span><span class="n">tempo</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">beat</span><span class="p">.</span><span class="nf">tempo</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="p">,</span> <span class="n">start_bpm</span><span class="o">=</span><span class="mi">10</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">The audio signal has a speed of </span><span class="si">{</span><span class="n">tempo</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> bpm.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">The audio signal has a speed of 42.61 bpm.
</code></pre>

<h2 id="34-fundamental-frequency">3.4. Fundamental frequency</h2>

<p>The <a href="https://en.wikipedia.org/wiki/Fundamental_frequency">fundamental frequency</a> is the lowest frequency at which a periodic sound appears. In music this is also known as pitch. In the spectrogram plots that we saw before, the fundamental frequency (also called f0) is the lowest bright horizontal strip in the image. While the repetition of the strip pattern above this fundamental are called harmonics.</p>

<p>To better illustrate what we exactly mean, let’s extract the fundamental frequency and plot them in our spectrogram.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract fundamental frequency using a probabilistic approach
</span><span class="n">f0</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">pyin</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">fmin</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">fmax</span><span class="o">=</span><span class="mi">8000</span><span class="p">,</span> <span class="n">frame_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">)</span>

<span class="c1"># Establish timepoint of f0 signal
</span><span class="n">timepoints</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">duration</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">f0</span><span class="p">),</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Plot fundamental frequency in spectrogram plot
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">x_stft</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">librosa</span><span class="p">.</span><span class="nf">stft</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">x_stft</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">amplitude_to_db</span><span class="p">(</span><span class="n">x_stft</span><span class="p">,</span> <span class="n">ref</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">)</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">x_stft</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">log</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">timepoints</span><span class="p">,</span> <span class="n">f0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">cyan</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_40_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>The turquoise lines that you see around 100 Hz are the fundamental frequencies. So, this seems about write. But how can we now use that for feature engineering? Well, what we could do is compute specific characteristics of this f0.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Computes mean, median, 5%- and 95%-percentile value of fundamental frequency
</span><span class="n">f0_values</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">nanmean</span><span class="p">(</span><span class="n">f0</span><span class="p">),</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">nanmedian</span><span class="p">(</span><span class="n">f0</span><span class="p">),</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">nanstd</span><span class="p">(</span><span class="n">f0</span><span class="p">),</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">nanpercentile</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
    <span class="n">np</span><span class="p">.</span><span class="nf">nanpercentile</span><span class="p">(</span><span class="n">f0</span><span class="p">,</span> <span class="mi">95</span><span class="p">),</span>
<span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"""</span><span class="s">This audio signal has a mean of {:.2f}, a median of {:.2f}, a
std of {:.2f}, a 5-percentile at {:.2f} and a 95-percentile at {:.2f}.</span><span class="sh">"""</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="o">*</span><span class="n">f0_values</span><span class="p">))</span>
</code></pre></div></div>

<pre><code class="language-code">This audio signal has a mean of 81.98, a median of 80.46, a
std of 4.42, a 5-percentile at 76.57 and a 95-percentile at 90.64.
</code></pre>

<p><strong>Note:</strong> There are of course many more audio feature extraction techniques that you could explore. For a nice summary
of a few of them, check out <a href="https://musicinformationretrieval.com/#Signal-Analysis-and-Feature-Extraction">musicinformationretrieval.com</a>.</p>

<h1 id="4-exploratory-data-analysis-eda-on-audio-dataset">4. Exploratory data analysis (EDA) on audio dataset</h1>

<p>Now that we know what audio data looks like and how we can process it, let’s go a step further and conduct a proper EDA on it. To do so, let’s first download a dataset. <strong>Note</strong>, the dataset we will be using for this article was downloaded from the <a href="https://www.kaggle.com/mozillaorg/common-voice">Common Voice</a> repository from Kaggle. This 14 GB big dataset is only a small snapshot of a +70 GB big dataset from <a href="https://commonvoice.mozilla.org/en/datasets">Mozilla</a>. But don’t worry, for our example here we will use an ever smaller subsample of roughly ~9’000 audio files.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Download and unzip dataset
</span><span class="err">!</span><span class="n">wget</span> <span class="o">-</span><span class="n">qO</span> <span class="n">c4_audio_dataset</span><span class="p">.</span><span class="nb">zip</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="p">.</span><span class="n">dropbox</span><span class="p">.</span><span class="n">com</span><span class="o">/</span><span class="n">s</span><span class="o">/</span><span class="n">tkqpq16cu4i1oyd</span><span class="o">/</span><span class="n">c4_audio_dataset</span><span class="p">.</span><span class="nb">zip</span><span class="err">?</span><span class="n">dl</span><span class="o">=</span><span class="mi">1</span>
<span class="err">!</span><span class="n">unzip</span> <span class="o">-</span><span class="n">q</span> <span class="n">c4_audio_dataset</span><span class="p">.</span><span class="nb">zip</span>
<span class="err">!</span><span class="n">rm</span> <span class="n">c4_audio_dataset</span><span class="p">.</span><span class="nb">zip</span>
</code></pre></div></div>

<p>So let’s take a closer look at this dataset and some already extracted features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Load the csv-file which contains already extracted features
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">c4_common-voice_dataset.csv.zip</span><span class="sh">"</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>filename</th>
      <th>age</th>
      <th>gender</th>
      <th>nwords</th>
      <th>duration</th>
      <th>words_per_second</th>
      <th>tempo</th>
      <th>f0_mean</th>
      <th>f0_median</th>
      <th>f0_std</th>
      <th>f0_5perc</th>
      <th>f0_95perc</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sample_00001.mp3</td>
      <td>thirties</td>
      <td>male</td>
      <td>7</td>
      <td>2.628</td>
      <td>2.663</td>
      <td>25.000</td>
      <td>102.324</td>
      <td>98.498</td>
      <td>17.991</td>
      <td>80.418</td>
      <td>132.998</td>
    </tr>
    <tr>
      <td>sample_00002.mp3</td>
      <td>sixties</td>
      <td>male</td>
      <td>15</td>
      <td>2.916</td>
      <td>5.144</td>
      <td>27.173</td>
      <td>97.773</td>
      <td>96.799</td>
      <td>17.866</td>
      <td>70.626</td>
      <td>129.735</td>
    </tr>
    <tr>
      <td>sample_00003.mp3</td>
      <td>twenties</td>
      <td>female</td>
      <td>18</td>
      <td>3.528</td>
      <td>5.102</td>
      <td>25.000</td>
      <td>237.412</td>
      <td>234.253</td>
      <td>36.550</td>
      <td>185.338</td>
      <td>301.256</td>
    </tr>
    <tr>
      <td>sample_00004.mp3</td>
      <td>twenties</td>
      <td>male</td>
      <td>35</td>
      <td>6.516</td>
      <td>5.371</td>
      <td>21.306</td>
      <td>189.364</td>
      <td>110.553</td>
      <td>196.566</td>
      <td>90.317</td>
      <td>689.908</td>
    </tr>
    <tr>
      <td>sample_00005.mp3</td>
      <td>fourties</td>
      <td>female</td>
      <td>19</td>
      <td>5.040</td>
      <td>3.769</td>
      <td>19.531</td>
      <td>204.885</td>
      <td>202.755</td>
      <td>21.037</td>
      <td>177.839</td>
      <td>245.332</td>
    </tr>
  </tbody>
</table>
</div>

<p><br /></p>

<h2 id="41-investigation-of-features-distribution">4.1. Investigation of features distribution</h2>

<h3 id="411-target-features">4.1.1. Target features</h3>

<p>First, let’s look at the class distributions of our potential target classes <code class="language-plaintext highlighter-rouge">age</code> and <code class="language-plaintext highlighter-rouge">gender</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">]):</span>
    <span class="n">df</span><span class="p">[</span><span class="n">c</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="n">c</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_50_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p><br /></p>

<h3 id="412-extracted-features">4.1.2. Extracted features</h3>

<p>As a next step, let’s take a closer look at the value distributions of the extracted features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot value distributions of extracted features
</span><span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">]).</span><span class="nf">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_52_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Except for <code class="language-plaintext highlighter-rouge">words_per_second</code>, most of these feature distributions are right skewed and therefore could profit from a log-transformation. So let’s take care of that.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Applies log1p on features that are not age, gender, filename or words_per_second
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">x</span><span class="p">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">words_per_second</span><span class="sh">"</span><span class="p">]</span>
    <span class="k">else</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># Let's look at the distribution once more
</span><span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">]).</span><span class="nf">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_54_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Much better, but what is interesting is the fact that the <code class="language-plaintext highlighter-rouge">f0</code> features all seem to have a bimodal distribution. Let’s plot the same thing as before, but this time separated by gender.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">df</span><span class="p">.</span><span class="n">gender</span><span class="p">.</span><span class="nf">unique</span><span class="p">():</span>
    <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="n">g</span><span class="p">)][</span><span class="sh">"</span><span class="s">f0_median</span><span class="sh">"</span><span class="p">].</span><span class="nf">hist</span><span class="p">(</span>
        <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">g</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_56_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>As suspected, there seems to be a gender effect here! But what we can also see is that some <code class="language-plaintext highlighter-rouge">f0</code> scores (here in particular in males) are much lower and higher than they should be. These could potentially be outliers, due to bad feature extraction. Let’s take a closer look at all data points with the following figure.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot sample points for each feature individually
</span><span class="n">df</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">subplots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span>
        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mf">7.5</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_58_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Given the few number of features and the fact that we have rather nice looking distributions with pronounced tails, we could go through each of them and decide the outlier cut off threshold feature by feature. But to show you a more automated way, lets use a z-score approach instead.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">scipy.stats</span> <span class="kn">import</span> <span class="n">zscore</span>

<span class="c1"># Only select columns with numbers from the dataframe
</span><span class="n">df_num</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">number</span><span class="p">)</span>

<span class="c1"># Apply zscore to all numerical features
</span><span class="n">df_num</span> <span class="o">=</span> <span class="n">df_num</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">zscore</span><span class="p">)</span>

<span class="c1"># Identify all samples that are below a specific z-value
</span><span class="n">z_thresh</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">df_num</span><span class="p">.</span><span class="nf">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">z_thresh</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Only keep the values in the mask
</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">mask</span><span class="p">]</span>
<span class="n">df</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(8669, 12)
</code></pre>

<p>As you can see, this approach reduced our dataset roughly by 5%, which should be fine.</p>

<h2 id="42-feature-correlation">4.2. Feature correlation</h2>

<p>As a next step, let’s take a look at the correlation between all features. But before we can do that, let’s go ahead and also encode the non-numerical target features. Note, we could use scikit-learn’s <code class="language-plaintext highlighter-rouge">OrdinalEncoder</code> to do that, but that would potentially disrupt the correct order in the age feature. So let’s rather perform a manual mapping.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Map age to appropriate numerical value
</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">age</span><span class="sh">"</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">teens</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">twenties</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">thirties</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">fourties</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">fifties</span><span class="sh">"</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">sixties</span><span class="sh">"</span><span class="p">:</span> <span class="mi">5</span><span class="p">})</span>

<span class="c1"># Map gender to corresponding numerical value
</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">].</span><span class="nf">map</span><span class="p">({</span><span class="sh">"</span><span class="s">male</span><span class="sh">"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">"</span><span class="s">female</span><span class="sh">"</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</code></pre></div></div>

<p>Now we’re good to go to use pandas <code class="language-plaintext highlighter-rouge">.corr()</code> function together with seaborn’s <code class="language-plaintext highlighter-rouge">heatmap()</code> to gain more insight about the feature correlation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">df_corr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">corr</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">df_corr</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">.0f</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">df_corr</span><span class="p">)),</span> <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_65_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Interesting! What we can see is that our extracted <code class="language-plaintext highlighter-rouge">f0</code> features seem to have a rather strong relationship to <code class="language-plaintext highlighter-rouge">gender</code> target, while <code class="language-plaintext highlighter-rouge">age</code> doesn’t seem to correlate much with anything.</p>

<h2 id="43-spectrogram-features">4.3. Spectrogram features</h2>

<p>For now we haven’t looked at the actual audio recordings during our EDA. As we saw before, we have a lot of options (i.e. in waveform or as STFT, mel or mfccs spectrogram). For this exploration here, let’s go ahead look at the mel spectrograms.</p>

<p><strong>However</strong>, before we can do that we need to consider one thing: The audio samples are all of different length, meaning that the spectrograms will also have different length. Therefore, to normalize all recordings, let’s put cut them to a length of exactly 3 second. Meaning, samples that are too short will be filled up, while samples that are too long will be cut.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Two helper functions for audio data preparation
</span>
<span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">librosa</span>

<span class="k">def</span> <span class="nf">resize_spectrogram</span><span class="p">(</span><span class="n">spec</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">fact</span><span class="o">=-</span><span class="mi">80</span><span class="p">):</span>

    <span class="c1"># Create an empty canvas to put spectrogram into
</span>    <span class="n">canvas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="nf">len</span><span class="p">(</span><span class="n">spec</span><span class="p">),</span> <span class="n">length</span><span class="p">))</span> <span class="o">*</span> <span class="n">fact</span>

    <span class="k">if</span> <span class="n">spec</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">length</span><span class="p">:</span>
        <span class="n">canvas</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">spec</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">=</span> <span class="n">spec</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">canvas</span><span class="p">[:,</span> <span class="p">:</span><span class="n">length</span><span class="p">]</span> <span class="o">=</span> <span class="n">spec</span><span class="p">[:,</span> <span class="p">:</span><span class="n">length</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">canvas</span>

<span class="k">def</span> <span class="nf">compute_mel_spec</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="mi">16_000</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">duration</span><span class="o">=</span><span class="mf">3.0</span><span class="p">):</span>

    <span class="c1"># Loads the mp3 file
</span>    <span class="n">y</span><span class="p">,</span> <span class="n">sr</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">"</span><span class="s">audio_dataset</span><span class="sh">"</span><span class="p">,</span> <span class="n">filename</span><span class="p">),</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>

    <span class="c1"># Compute the mel spectrogram
</span>    <span class="n">x_mel</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="n">feature</span><span class="p">.</span><span class="nf">melspectrogram</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">)</span>

    <span class="c1"># Apply logarithmic dB-scale to spectrogram and set maximum to 0 dB
</span>    <span class="n">x_mel</span> <span class="o">=</span> <span class="n">librosa</span><span class="p">.</span><span class="nf">power_to_db</span><span class="p">(</span><span class="n">x_mel</span><span class="p">,</span> <span class="n">ref</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">)</span>

    <span class="c1"># Compute mean strength per frequency for mel spectrogram
</span>    <span class="n">mel_strength</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">x_mel</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Estimate the desired length of the spectrogram
</span>    <span class="n">length</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">duration</span> <span class="o">*</span> <span class="n">sr</span> <span class="o">/</span> <span class="n">hop_length</span><span class="p">)</span>

    <span class="c1"># Put mel spectrogram into the right shape
</span>    <span class="n">x_mel</span> <span class="o">=</span> <span class="nf">resize_spectrogram</span><span class="p">(</span><span class="n">x_mel</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">fact</span><span class="o">=-</span><span class="mi">80</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x_mel</span><span class="p">,</span> <span class="n">mel_strength</span>
</code></pre></div></div>

<p>Now that everything is ready, let’s extract the spectrograms for all audio samples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="c1"># Create arrays to store output into
</span><span class="n">spec_infos</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Loop through all files and extract spectrograms
</span><span class="n">sr</span> <span class="o">=</span> <span class="mi">16_000</span>
<span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">filename</span><span class="p">):</span>
    <span class="n">spec_infos</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">compute_mel_spec</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">))</span>

<span class="c1"># Aggregate feature types in common variable
</span><span class="n">mels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">spec_infos</span><span class="p">])</span>
<span class="n">mels_strengths</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">spec_infos</span><span class="p">])</span>
</code></pre></div></div>

<p>Now that we have these spectrogram features as well, let’s perform some EDA on them too! And because we saw that ‘gender’ seems to have a special relationship to our audio recordings, let’s visualize the average mel spectrogram for both gender separately, as well as their differences.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">librosa.display</span>

<span class="c1"># Creates a figure with two subplot
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Plots mel spectrogram for male speakers
</span><span class="n">mels_male</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">mels</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="mi">0</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">mels_male</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">mel</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">male</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plots mel spectrogram for female speakers
</span><span class="n">mels_female</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">mels</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">].</span><span class="nf">eq</span><span class="p">(</span><span class="mi">1</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span><span class="n">mels_female</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">mel</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">female</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Plot gender differences
</span><span class="n">librosa</span><span class="p">.</span><span class="n">display</span><span class="p">.</span><span class="nf">specshow</span><span class="p">(</span>
    <span class="n">mels_male</span> <span class="o">-</span> <span class="n">mels_female</span><span class="p">,</span> <span class="n">sr</span><span class="o">=</span><span class="n">sr</span><span class="p">,</span> <span class="n">x_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">time</span><span class="sh">"</span><span class="p">,</span> <span class="n">y_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">mel</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Difference</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_72_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>While it is difficult to see in the individual plot, the difference plot reveals that male speaker have on average lower voices than female. This can be seen by more strength in the lower frequencies (seeing in the red horizontal region) in the difference plot.</p>

<h1 id="5-machine-learning-models">5. Machine learning models</h1>

<p>Now, we’re ready for the modeling part. And as such, we have multiple options. With regards to <strong>models</strong>, we could …</p>

<ul>
  <li>train our own classical (i.e. shallow) machine learning models, such as LogisticRegression or SVC.</li>
  <li>train our own deep learning models, i.e. deep neural network.</li>
  <li>use a pretrained neural network from TensorflowHub for feature extraction and then train a shallow or deep model on these high-level features</li>
</ul>

<p>And with regards to <strong>data</strong>, we could use …</p>

<ul>
  <li>the data from the CSV file, combine it with the ‘mel strength’ features from the spectrograms and consider the data as a <em>tabular</em> data set</li>
  <li>the mel-spectrograms alone and consider them as a <em>image</em> data set</li>
  <li>the high-level features from TensorflowHub, combine them with the other tabular data and consider it as a <em>tabular</em> data set as well</li>
</ul>

<p>There are of course many different approaches and other ways to create the data set for the modeling part. For this article, let’s briefly explore one of them.</p>

<h2 id="classical-ie-shallow-machine-learning-model">Classical (i.e. shallow) machine learning model</h2>

<p>Let’s take the data from the CSV file and combine it with a simple <code class="language-plaintext highlighter-rouge">LogisticRegression</code> model and see how well we can predict the <code class="language-plaintext highlighter-rouge">age</code> of a speaker. So to start, let’s load the data and split it into train and test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Select target
</span><span class="n">target</span> <span class="o">=</span> <span class="sh">"</span><span class="s">age</span><span class="sh">"</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">target</span><span class="p">].</span><span class="n">values</span>

<span class="c1"># Select relevant features from the dataframe
</span><span class="n">features</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">filename</span><span class="sh">"</span><span class="p">,</span> <span class="n">target</span><span class="p">]).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Combine them with the mels strength features
</span><span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">((</span><span class="n">features</span><span class="p">,</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">mels_strengths</span><span class="p">)),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create train and test set
</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">x_te</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_te</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span>
<span class="p">)</span>

<span class="c1"># Plot size of dataset
</span><span class="nf">print</span><span class="p">(</span><span class="n">x_tr</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">(6935, 138)
</code></pre>

<p>Now that the data is ready to be trained, let’s create the model we would like to train. For this, let’s use a <code class="language-plaintext highlighter-rouge">Pipeline</code> object, so that we can explore the advantage of certain preprocessing routines (e.g. using scalers or PCA). Furthermore, let’s use <code class="language-plaintext highlighter-rouge">GridSearchCV</code> to explore different hyper-parameter combinations, as well to perform cross-validation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">RobustScaler</span><span class="p">,</span> <span class="n">PowerTransformer</span><span class="p">,</span> <span class="n">QuantileTransformer</span>
<span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="n">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="c1"># Create pipeline
</span><span class="n">pipe</span> <span class="o">=</span> <span class="nc">Pipeline</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">scaler</span><span class="sh">"</span><span class="p">,</span> <span class="nc">RobustScaler</span><span class="p">()),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">pca</span><span class="sh">"</span><span class="p">,</span> <span class="nc">PCA</span><span class="p">()),</span>
        <span class="p">(</span><span class="sh">"</span><span class="s">logreg</span><span class="sh">"</span><span class="p">,</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="sh">"</span><span class="s">balanced</span><span class="sh">"</span><span class="p">)),</span>
    <span class="p">]</span>
<span class="p">)</span>

<span class="c1"># Create grid
</span><span class="n">grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">scaler</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="nc">RobustScaler</span><span class="p">(),</span> <span class="nc">PowerTransformer</span><span class="p">(),</span> <span class="nc">QuantileTransformer</span><span class="p">()],</span>
    <span class="sh">"</span><span class="s">pca</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="nc">PCA</span><span class="p">(</span><span class="mf">0.99</span><span class="p">)],</span>
    <span class="sh">"</span><span class="s">logreg__C</span><span class="sh">"</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="nf">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">16</span><span class="p">),</span>
<span class="p">}</span>

<span class="c1"># Create GridSearchCV
</span><span class="n">grid_cv</span> <span class="o">=</span> <span class="nc">GridSearchCV</span><span class="p">(</span><span class="n">pipe</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Train GridSearchCV
</span><span class="n">model</span> <span class="o">=</span> <span class="n">grid_cv</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>

<span class="c1"># Collect results in a DataFrame
</span><span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">grid_cv</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">)</span>

<span class="c1"># Select the columns we are interested in
</span><span class="n">col_of_interest</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">param_scaler</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">param_pca</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">param_logreg__C</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">mean_train_score</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">std_test_score</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">std_train_score</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="n">col_of_interest</span><span class="p">]</span>

<span class="c1"># Show the dataframe sorted according to our performance metric
</span><span class="n">cv_results</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>
<pre><code class="language-code">Fitting 4 folds for each of 96 candidates, totalling 384 fits
</code></pre>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>param_scaler</th>
      <th>param_pca</th>
      <th>param_logreg__C</th>
      <th>mean_test_score</th>
      <th>mean_train_score</th>
      <th>std_test_score</th>
      <th>std_train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>PowerTransformer()</td>
      <td>None</td>
      <td>1.0</td>
      <td>0.439508</td>
      <td>0.485124</td>
      <td>0.005489</td>
      <td>0.005539</td>
    </tr>
    <tr>
      <td>PowerTransformer()</td>
      <td>None</td>
      <td>0.464159</td>
      <td>0.438499</td>
      <td>0.483538</td>
      <td>0.005958</td>
      <td>0.003447</td>
    </tr>
    <tr>
      <td>RobustScaler()</td>
      <td>None</td>
      <td>0.464159</td>
      <td>0.437203</td>
      <td>0.481663</td>
      <td>0.007420</td>
      <td>0.005240</td>
    </tr>
    <tr>
      <td>PowerTransformer()</td>
      <td>None</td>
      <td>0.1</td>
      <td>0.436482</td>
      <td>0.473059</td>
      <td>0.005968</td>
      <td>0.003246</td>
    </tr>
    <tr>
      <td>PowerTransformer()</td>
      <td>None</td>
      <td>0.215443</td>
      <td>0.436192</td>
      <td>0.478923</td>
      <td>0.005446</td>
      <td>0.004047</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>RobustScaler()</td>
      <td>PCA(0.99)</td>
      <td>0.001</td>
      <td>0.296178</td>
      <td>0.310118</td>
      <td>0.004719</td>
      <td>0.001384</td>
    </tr>
    <tr>
      <td>QuantileTransformer()</td>
      <td>None</td>
      <td>0.002154</td>
      <td>0.291420</td>
      <td>0.297573</td>
      <td>0.005419</td>
      <td>0.001818</td>
    </tr>
    <tr>
      <td>QuantileTransformer()</td>
      <td>PCA(0.99)</td>
      <td>0.002154</td>
      <td>0.290699</td>
      <td>0.296563</td>
      <td>0.005288</td>
      <td>0.002046</td>
    </tr>
    <tr>
      <td>QuantileTransformer()</td>
      <td>None</td>
      <td>0.001</td>
      <td>0.287959</td>
      <td>0.291613</td>
      <td>0.004569</td>
      <td>0.001804</td>
    </tr>
    <tr>
      <td>QuantileTransformer()</td>
      <td>PCA(0.99)</td>
      <td>0.001</td>
      <td>0.287670</td>
      <td>0.290988</td>
      <td>0.005001</td>
      <td>0.001787</td>
    </tr>
  </tbody>
</table>
<p>96 rows × 7 columns</p>
</div>

<p>As an addition to the DataFrame output above, we can also plot the performance score as a function of the explored hyperparameters. However, given that we have multiple scalers and PCA approaches, we need to create a separate plot for each separate combination of hyperparameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">product</span>

<span class="c1"># Establish combinations of different hyperparameters, that isn't the one
# we want to plot on the x-axis
</span><span class="n">combinations</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">product</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="sh">"</span><span class="s">scaler</span><span class="sh">"</span><span class="p">],</span> <span class="n">grid</span><span class="p">[</span><span class="sh">"</span><span class="s">pca</span><span class="sh">"</span><span class="p">]))</span>

<span class="c1"># Creates a figure with multiple subplot
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span>
    <span class="nf">len</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="sh">"</span><span class="s">scaler</span><span class="sh">"</span><span class="p">]),</span> <span class="nf">len</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="sh">"</span><span class="s">pca</span><span class="sh">"</span><span class="p">]),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extract useful information about max performance
</span><span class="n">max_score</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">].</span><span class="nf">max</span><span class="p">()</span>
<span class="n">c_values</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">param_logreg__C</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Loop through the subplots and populate them
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">combinations</span><span class="p">):</span>

    <span class="c1"># Select subplot relevant grid search results
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">logical_and</span><span class="p">(</span>
        <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">param_pca</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">str</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="nf">str</span><span class="p">(</span><span class="n">p</span><span class="p">),</span>
        <span class="n">cv_results</span><span class="p">[</span><span class="sh">"</span><span class="s">param_scaler</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">str</span><span class="sh">"</span><span class="p">)</span> <span class="o">==</span> <span class="nf">str</span><span class="p">(</span><span class="n">s</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">df_cv</span> <span class="o">=</span> <span class="n">cv_results</span><span class="p">[</span><span class="n">mask</span><span class="p">].</span><span class="nf">sort_values</span><span class="p">(</span><span class="sh">"</span><span class="s">param_logreg__C</span><span class="sh">"</span><span class="p">).</span><span class="nf">set_index</span><span class="p">(</span><span class="sh">"</span><span class="s">param_logreg__C</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Select relevant axis
</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()[</span><span class="n">i</span><span class="p">]</span>

    <span class="c1"># Plot train and test curves
</span>    <span class="n">df_cv</span><span class="p">[[</span><span class="sh">"</span><span class="s">mean_train_score</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">]].</span><span class="nf">plot</span><span class="p">(</span>
        <span class="n">logx</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">s</span><span class="si">}</span><span class="s"> | </span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span>
        <span class="n">df_cv</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
        <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">mean_train_score</span><span class="sh">"</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">std_train_score</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">mean_train_score</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">std_train_score</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">fill_between</span><span class="p">(</span>
        <span class="n">df_cv</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
        <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">]</span> <span class="o">-</span> <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">std_test_score</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">mean_test_score</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">df_cv</span><span class="p">[</span><span class="sh">"</span><span class="s">std_test_score</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,)</span>

    <span class="c1"># Plot best performance metric as dotted line
</span>    <span class="n">ax</span><span class="p">.</span><span class="nf">hlines</span><span class="p">(</span>
        <span class="n">max_score</span><span class="p">,</span> <span class="n">c_values</span><span class="p">.</span><span class="nf">min</span><span class="p">(),</span> <span class="n">c_values</span><span class="p">.</span><span class="nf">max</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">"</span><span class="s">dotted</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Limit y-axis
</span><span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mf">0.28</span><span class="p">,</span> <span class="mf">0.501</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_80_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Taking the extra step and visualizing the performance metrics as curves often give us relevant additional information, that we wouldn’t get when we just look at the pandas DataFrame.</p>

<p>In this plot we can see that overall, the models perform equally well. Some have a quicker ‘drop-off’ when we decrease the value of <code class="language-plaintext highlighter-rouge">C</code>, while other show a wider gap between train and test (here actually validation) score, especially when we don’t use <code class="language-plaintext highlighter-rouge">PCA</code>.</p>

<p>Having said all that, let’s just go ahead with the <code class="language-plaintext highlighter-rouge">best_estimator_</code> model and see how well it performs on the withheld test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute score of the best model on the withheld test set
</span><span class="n">best_clf</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">best_estimator_</span>
<span class="n">best_clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">x_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">0.4354094579008074
</code></pre>

<p>That’s already a very good score. But to better understand how well our classification model performed, let’s also look at the corresponding confusion matrix. To do this, let’s create a short helper function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>

<span class="k">def</span> <span class="nf">plot_confusion_matrices</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>

    <span class="c1"># Create two subplots
</span>    <span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1"># Specify labels
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">teens</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">twenties</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">thirties</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fourties</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">fifties</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">sixties</span><span class="sh">"</span><span class="p">]</span>

    <span class="c1"># Plots the standard confusion matrix
</span>    <span class="n">ax1</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix (counts)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_predictions</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">xticks_rotation</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax1</span><span class="p">)</span>

    <span class="c1"># Plots the normalized confusion matrix
</span>    <span class="n">ax2</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix (ratios)</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_predictions</span><span class="p">(</span>
        <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">xticks_rotation</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax2</span><span class="p">,</span>
        <span class="n">normalize</span><span class="o">=</span><span class="sh">"</span><span class="s">true</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Compute test set predictions
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">best_clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_te</span><span class="p">)</span>

<span class="c1"># Plot confusion matrices
</span><span class="nf">plot_confusion_matrices</span><span class="p">(</span><span class="n">y_te</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/04_audio_data_analysis/output_84_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>As you can see, while the model was able to detect more <code class="language-plaintext highlighter-rouge">twenties</code> samples than others (left confusion matrix), it overall it actually was better in classifying <code class="language-plaintext highlighter-rouge">teens</code> and <code class="language-plaintext highlighter-rouge">sixties</code> entries (e.g. with an accuracy of 59% and 55% respectively).</p>

<h1 id="summary">Summary</h1>

<p>In this unit we first saw what audio data looks like, in which different forms it can be transformed to, how it can be cleaned and explored and how it then can be used to train some machine learning models.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How to perform EDA and data modeling on audio data]]></summary></entry><entry><title type="html">Advanced exploratory data analysis (EDA)</title><link href="https://miykael.github.io/blog/2022/advanced_eda/" rel="alternate" type="text/html" title="Advanced exploratory data analysis (EDA)" /><published>2022-02-01T12:00:00+00:00</published><updated>2022-02-01T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2022/advanced_eda</id><content type="html" xml:base="https://miykael.github.io/blog/2022/advanced_eda/"><![CDATA[<h2 id="how-to-quickly-get-a-handle-on-almost-any-tabular-dataset">How to quickly get a handle on almost any tabular dataset</h2>

<p><em>[Find the Jupyter Notebook to this article <a href="https://github.com/miykael/miykael.github.io/blob/master/assets/nb/03_advanced_eda/nb_advanced_eda.ipynb">here</a>.]</em></p>

<hr />

<p>Getting a good feeling for a new dataset is not always easy, and takes time. However, a good and broad exploratory data analysis (EDA) can help a lot to understand your dataset, get a feeling for how things are connected and what needs to be done to properly process your dataset.</p>

<p>In this article, we will touch upon multiple useful EDA routines. However, to keep things short and compact we might not always dig deeper or explain all of the implications. But in reality, spending enough time on a proper EDA to fully understand your dataset is a key part of any good data science project. As a rule of thumb, you probably will spend 80% of your time in data preparation and exploration and only 20% in actual machine learning modeling.</p>

<p>Having said all this, let’s dive right into it!</p>
<h2 id="investigation-of-structure-quality-and-content">Investigation of structure, quality and content</h2>

<p>Overall, the EDA approach is very iterative. At the end of your investigation you might discover something that will require you to redo everything once more. That is normal! But to impose at least a little bit of structure, I propose the following structure for your investigations:</p>

<ol>
  <li><strong>Structure investigation</strong>: Exploring the general shape of the dataset, as well as the data types of your features.</li>
  <li><strong>Quality investigation</strong>: Get a feeling for the general quality of the dataset, with regards to duplicates, missing values and unwanted entries.</li>
  <li><strong>Content investigation</strong>: Once the structure and quality of the dataset is understood, we can go ahead and perform a more in-depth exploration on the features values and look at how different features relate to each other.</li>
</ol>

<p>But first we need to find an interesting dataset. Let’s go ahead and load the <a href="https://www.openml.org/d/42803">road safety dataset</a> from <a href="https://www.openml.org/search?type=data">OpenML</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_openml</span>

<span class="c1"># Download the dataset from openml
</span><span class="n">dataset</span> <span class="o">=</span> <span class="nf">fetch_openml</span><span class="p">(</span><span class="n">data_id</span><span class="o">=</span><span class="mi">42803</span><span class="p">,</span> <span class="n">as_frame</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extract feature matrix X and show 5 random samples
</span><span class="n">df_X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">frame</span><span class="sh">"</span><span class="p">]</span>
</code></pre></div></div>

<h1 id="1-structure-investigation">1. Structure Investigation</h1>

<p>Before looking at the content of our feature matrix $X$, let’s first look at the general structure of the dataset. For example, how many columns and rows does the dataset have?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Show size of the dataset
</span><span class="n">df_X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(363243, 67)
</code></pre>

<p>So we know that this dataset has 363’243 samples and 67 features. And how many different data types do these 67 features contain?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># Count how many times each data type is present in the dataset
</span><span class="n">pd</span><span class="p">.</span><span class="nf">value_counts</span><span class="p">(</span><span class="n">df_X</span><span class="p">.</span><span class="n">dtypes</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">float64    61
object      6
dtype: int64
</code></pre>

<h2 id="11-structure-of-non-numerical-features">1.1. Structure of non-numerical features</h2>

<p>Data types can be numerical and non-numerical. First, let’s take a closer look at the <strong>non-numerical</strong> entries.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Display non-numerical features
</span><span class="n">df_X</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="sh">"</span><span class="s">number</span><span class="sh">"</span><span class="p">).</span><span class="nf">head</span><span class="p">()</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Accident_Index</th>
      <th>Sex_of_Driver</th>
      <th>Date</th>
      <th>Time</th>
      <th>Local_Authority_(Highway)</th>
      <th>LSOA_of_Accident_Location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>201501BS70001</td>
      <td>1.0</td>
      <td>12/01/2015</td>
      <td>18:45</td>
      <td>E09000020</td>
      <td>E01002825</td>
    </tr>
    <tr>
      <th>1</th>
      <td>201501BS70002</td>
      <td>1.0</td>
      <td>12/01/2015</td>
      <td>07:50</td>
      <td>E09000020</td>
      <td>E01002820</td>
    </tr>
    <tr>
      <th>2</th>
      <td>201501BS70004</td>
      <td>1.0</td>
      <td>12/01/2015</td>
      <td>18:08</td>
      <td>E09000020</td>
      <td>E01002833</td>
    </tr>
    <tr>
      <th>3</th>
      <td>201501BS70005</td>
      <td>1.0</td>
      <td>13/01/2015</td>
      <td>07:40</td>
      <td>E09000020</td>
      <td>E01002874</td>
    </tr>
    <tr>
      <th>4</th>
      <td>201501BS70008</td>
      <td>1.0</td>
      <td>09/01/2015</td>
      <td>07:30</td>
      <td>E09000020</td>
      <td>E01002814</td>
    </tr>
  </tbody>
</table>
</div>

<p>Even though <code class="language-plaintext highlighter-rouge">Sex_of_Driver</code> is a numerical feature, it somehow was stored as a non-numerical one. This is sometimes due to some typo in data recording. So let’s take care of that:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Changes data type of 'Sex_of_Driver'
</span><span class="n">df_X</span><span class="p">[</span><span class="sh">"</span><span class="s">Sex_of_Driver</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">[</span><span class="sh">"</span><span class="s">Sex_of_Driver</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">float</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Using the <code class="language-plaintext highlighter-rouge">.describe()</code> function we can also investigate how many unique values each non-numerical feature has and with which frequency the most prominent value is present.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_X</span><span class="p">.</span><span class="nf">describe</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="sh">"</span><span class="s">number</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Accident_Index</th>
      <th>Date</th>
      <th>Time</th>
      <th>Local_Authority_(Highway)</th>
      <th>LSOA_of_Accident_Location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>363243</td>
      <td>319866</td>
      <td>319822</td>
      <td>319866</td>
      <td>298758</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>140056</td>
      <td>365</td>
      <td>1439</td>
      <td>204</td>
      <td>25979</td>
    </tr>
    <tr>
      <th>top</th>
      <td>201543P296025</td>
      <td>14/02/2015</td>
      <td>17:30</td>
      <td>E10000017</td>
      <td>E01028497</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1332</td>
      <td>2144</td>
      <td>2972</td>
      <td>8457</td>
      <td>1456</td>
    </tr>
  </tbody>
</table>
</div>

<p><br /></p>

<h2 id="12-structure-of-numerical-features">1.2. Structure of numerical features</h2>

<p>Next, let’s take a closer look at the numerical features. More precisely, let’s investigate how many unique values each of these feature has. This process will give us some insights about the number of <strong>binary</strong> (2 unique values), <strong>ordinal</strong> (3 to ~10 unique values) and <strong>continuous</strong> (more than 10 unique values) features in the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For each numerical feature compute number of unique entries
</span><span class="n">unique_values</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="sh">"</span><span class="s">number</span><span class="sh">"</span><span class="p">).</span><span class="nf">nunique</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">()</span>

<span class="c1"># Plot information with y-axis in log-scale
</span><span class="n">unique_values</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">logy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Unique values per feature</span><span class="sh">"</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_14_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="13-conclusion-of-structure-investigation">1.3. Conclusion of structure investigation</h2>

<p>At the end of this first investigation, we should have a better understanding of the general structure of our dataset. Number of samples and features, what kind of data type each feature has, and how many of them are binary, ordinal, categorical or continuous. For an alternative way to get such kind of information you could also use <code class="language-plaintext highlighter-rouge">df_X.info()</code> or <code class="language-plaintext highlighter-rouge">df_X.describe()</code>.</p>

<h1 id="2-quality-investigation">2. Quality Investigation</h1>

<p>Before focusing on the actual content stored in these features, let’s first take a look at the general quality of the dataset. The goal is to have a global view on the dataset with regards to things like duplicates, missing values and unwanted entries or recording errors.</p>

<h2 id="21-duplicates">2.1. Duplicates</h2>

<p>Duplicates are entries that represent the same sample point multiple times. For example, if a measurement was registered twice by two different people. Detecting such duplicates is not always easy, as each dataset might have a unique identifier (e.g. an index number or recording time that is unique to each new sample) which you might want to ignore first.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Check number of duplicates while ignoring the index feature
</span><span class="n">n_duplicates</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Accident_Index</span><span class="sh">"</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">duplicated</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">You seem to have </span><span class="si">{</span><span class="n">n_duplicates</span><span class="si">}</span><span class="s"> duplicates in your database.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">You seem to have 22 duplicates in your database.
</code></pre>

<p>To handle these duplicates you can just simply drop them with <code class="language-plaintext highlighter-rouge">.drop_duplicates()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#  Extract column names of all features, except 'Accident_Index'
</span><span class="n">columns_to_consider</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Accident_Index</span><span class="sh">"</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">columns</span>

<span class="c1"># Drop duplicates based on 'columns_to_consider'
</span><span class="n">df_X</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">drop_duplicates</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="n">columns_to_consider</span><span class="p">)</span>
<span class="n">df_X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(363221, 67)
</code></pre>

<h2 id="22-missing-values">2.2. Missing values</h2>

<p>Another quality issue worth to investigate are missing values. Having some missing values is normal. What we want to identify at this stage are big holes in the dataset, i.e. samples or features with a lot of missing values.</p>

<h3 id="221-per-sample">2.2.1. Per sample</h3>

<p>To look at number of missing values per sample we have multiple options. The most straight forward one is to simply visualize the output of <code class="language-plaintext highlighter-rouge">df_X.isna()</code>, with something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">df_X</span><span class="p">.</span><span class="nf">isna</span><span class="p">(),</span> <span class="n">aspect</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="sh">"</span><span class="s">nearest</span><span class="sh">"</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Column Number</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Sample Number</span><span class="sh">"</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_23_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>This figure shows on the y-axis each of the 360’000 individual samples, and on the x-axis if any of the 67 features contains a missing value. While this is already a useful plot, an even better approach is to use the <a href="https://github.com/ResidentMario/missingno">missingno</a> library, to get a plot like this one:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">missingno</span> <span class="k">as</span> <span class="n">msno</span>

<span class="n">msno</span><span class="p">.</span><span class="nf">matrix</span><span class="p">(</span><span class="n">df_X</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">sort</span><span class="o">=</span><span class="sh">"</span><span class="s">descending</span><span class="sh">"</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_25_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>From both of these plots we can see that the dataset has a huge whole, caused by some samples where more than 50% of the feature values are missing. For those samples, filling the missing values with some replacement values is probably not a good idea.</p>

<p>Therefore, let’s go ahead and drop samples that have more than 20% of missing values. The threshold is inspired by the information from the ‘Data Completeness’ column on the right of this figure.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_X</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">thresh</span><span class="o">=</span><span class="n">df_X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.80</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df_X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(319790, 67)
</code></pre>

<h3 id="222-per-feature">2.2.2. Per Feature</h3>

<p>As a next step, let’s now look at the number of missing values per feature. For this we can use some <code class="language-plaintext highlighter-rouge">pandas</code> trickery to quickly identify the ratio of missing values <strong>per feature</strong>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_X</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">mean</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">().</span><span class="nf">plot</span><span class="p">(</span>
    <span class="n">kind</span><span class="o">=</span><span class="sh">"</span><span class="s">bar</span><span class="sh">"</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
    <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Percentage of missing values per feature</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Ratio of missing values per feature</span><span class="sh">"</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_29_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>From this figure we can see that most features don’t contain any missing values. Nonetheless, features like <code class="language-plaintext highlighter-rouge">2nd_Road_Class</code>, <code class="language-plaintext highlighter-rouge">Junction_Control</code>, <code class="language-plaintext highlighter-rouge">Age_of_Vehicle</code> still contain quite a lot of missing values. So let’s go ahead and remove any feature with more than 15% of missing values.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_X</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">thresh</span><span class="o">=</span><span class="n">df_X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="mf">0.85</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(319790, 60)
</code></pre>

<h3 id="223-small-side-note">2.2.3. Small side note</h3>

<p><strong>Missing values</strong>: There is no strict order in removing missing values. For some datasets, tackling
first the features and than the samples might be better. Furthermore, the threshold at which you decide
to drop missing values per feature or sample changes from dataset to dataset, and depends on what you
intend to do with the dataset later on.</p>

<p><strong>Also</strong>, until now we only addressed the big holes in the dataset, not yet how we would fill the smaller gaps.
This is content for another post.</p>

<h2 id="23-unwanted-entries-and-recording-errors">2.3. Unwanted entries and recording errors</h2>

<p>Another source of quality issues in a dataset can be due to unwanted entries or recording errors. It’s important to distinguish such samples from simple outliers. While outliers are data points that are unusual for a given feature distribution, <strong>unwanted entries or recording errors are samples that shouldn’t be there in the first place</strong>.</p>

<p>For example, a temperature recording of 45°C in Switzerland might be an outlier (as in ‘very unusual’), while a recording at 90°C would be an error. Similarly, a temperature recording from the top of Mont Blanc might be physical possible, but most likely shouldn’t be included in a dataset about Swiss cities.</p>

<p>Of course, detecting such errors and unwanted entries and distinguishing them from outliers is not always straight forward and depends highly on the dataset. One approach to this is to take a global view on the dataset and see if you can identify some very unusual patterns.</p>

<h3 id="231-numerical-features">2.3.1. Numerical features</h3>

<p>To plot this global view of the dataset, at least for the numerical features, you can use pandas’ <code class="language-plaintext highlighter-rouge">.plot()</code> function and combine it with the following parameters:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">lw=0</code>: <code class="language-plaintext highlighter-rouge">lw</code> stands for line width. <code class="language-plaintext highlighter-rouge">0</code> means that we don’t want to show any lines</li>
  <li><code class="language-plaintext highlighter-rouge">marker="."</code>: Instead of lines, we tell the plot to use <code class="language-plaintext highlighter-rouge">.</code> as markers for each data point</li>
  <li><code class="language-plaintext highlighter-rouge">subplots=True</code>: <code class="language-plaintext highlighter-rouge">subplots</code> tells <code class="language-plaintext highlighter-rouge">pandas</code> to plot each feature in a separate subplot</li>
  <li><code class="language-plaintext highlighter-rouge">layout=(-1, 4)</code>: This parameter tells <code class="language-plaintext highlighter-rouge">pandas</code> how many rows and columns to use for the subplots. The <code class="language-plaintext highlighter-rouge">-1</code> means “as many as needed”, while the <code class="language-plaintext highlighter-rouge">2</code> means to use 2 columns per row.</li>
  <li><code class="language-plaintext highlighter-rouge">figsize=(15, 30), markersize=1</code>: To make sure that the figure is big enough we recommend to have a figure height of roughly the number of features, and to adjust the <code class="language-plaintext highlighter-rouge">markersize</code> accordingly.</li>
</ul>

<p>So what does this plot look like?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_X</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">subplots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span>
          <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">1</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_35_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Each point in this figure is a sample (i.e. a row) in our dataset and each subplot represents a different feature. The y-axis shows the feature value, while the x-axis is the sample index. These kind of plots can give you a lot of ideas for data cleaning and EDA. Usually it makes sense to invest as much time as needed until your happy with the output of this visualization.</p>

<h3 id="232-non-numerical-features">2.3.2. Non-numerical features</h3>

<p>Identifying <strong>unwanted entries</strong> or <strong>recording errors</strong> on non-numerical features is a bit more tricky. Given that at this point, we only want to investigate the general quality of the dataset. So what we can do is take a general look at how many unique values each of these non-numerical features contain, and how often their most frequent category is represented.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract descriptive properties of non-numerical features
</span><span class="n">df_X</span><span class="p">.</span><span class="nf">describe</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">number</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">datetime</span><span class="sh">"</span><span class="p">])</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Accident_Index</th>
      <th>Date</th>
      <th>Time</th>
      <th>Local_Authority_(Highway)</th>
      <th>LSOA_of_Accident_Location</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>319790</td>
      <td>319790</td>
      <td>319746</td>
      <td>319790</td>
      <td>298693</td>
    </tr>
    <tr>
      <th>unique</th>
      <td>123645</td>
      <td>365</td>
      <td>1439</td>
      <td>204</td>
      <td>25977</td>
    </tr>
    <tr>
      <th>top</th>
      <td>201543P296025</td>
      <td>14/02/2015</td>
      <td>17:30</td>
      <td>E10000017</td>
      <td>E01028497</td>
    </tr>
    <tr>
      <th>freq</th>
      <td>1332</td>
      <td>2144</td>
      <td>2969</td>
      <td>8457</td>
      <td>1456</td>
    </tr>
  </tbody>
</table>
</div>

<p>There are multiple ways for how you could potentially streamline the quality investigation for each individual non-numerical features. None of them is perfect, and all of them will require some follow up investigation. But for the purpose of showcasing one such a solution, what we could do is loop through all non-numerical features and plot for each of them the number of occurrences per unique value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create figure object with 3 subplots
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Identify non-numerical features
</span><span class="n">df_non_numerical</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">number</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">datetime</span><span class="sh">"</span><span class="p">])</span>

<span class="c1"># Loop through features and put each subplot on a matplotlib axis object
</span><span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">df_non_numerical</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()):</span>

    <span class="c1"># Selects one single feature and counts number of occurrences per unique value
</span>    <span class="n">df_non_numerical</span><span class="p">[</span><span class="n">col</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">plot</span><span class="p">(</span>

        <span class="c1"># Plots this information in a figure with log-scaled y-axis
</span>        <span class="n">logy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
    
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_40_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>We can see that the most frequent accident (i.e. <code class="language-plaintext highlighter-rouge">Accident_Index</code>), had more than 100 people involved. Digging a bit deeper (i.e. looking at the individual features of this accident), we could identify that this accident happened on February 24th, 2015 at 11:55 in Cardiff UK. A quick internet search reveals that this entry corresponds to a luckily non-lethal accident including a minibus full of pensioners.</p>

<p>The decision for what should be done with such rather unique entries is once more left in the the subjective hands of the person analyzing the dataset. Without any good justification for WHY, and only with the intention to show you the HOW - let’s go ahead and remove the 10 most frequent accidents from this dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Collect entry values of the 10 most frequent accidents
</span><span class="n">accident_ids</span> <span class="o">=</span> <span class="n">df_non_numerical</span><span class="p">[</span><span class="sh">"</span><span class="s">Accident_Index</span><span class="sh">"</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">().</span><span class="nf">head</span><span class="p">(</span><span class="mi">10</span><span class="p">).</span><span class="n">index</span>

<span class="c1"># Removes accidents from the 'accident_ids' list
</span><span class="n">df_X</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">[</span><span class="o">~</span><span class="n">df_X</span><span class="p">[</span><span class="sh">"</span><span class="s">Accident_Index</span><span class="sh">"</span><span class="p">].</span><span class="nf">isin</span><span class="p">(</span><span class="n">accident_ids</span><span class="p">)]</span>
<span class="n">df_X</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(317665, 60)
</code></pre>

<h2 id="24-conclusion-of-quality-investigation">2.4. Conclusion of quality investigation</h2>

<p>At the end of this second investigation, we should have a better understanding of the general quality of our dataset. We looked at duplicates, missing values and unwanted entries or recording errors. It is important to point out that we didn’t discuss yet how to address the remaining missing values or outliers in the dataset. This is a task for the next investigation, but won’t be covered in this article.</p>

<h1 id="3-content-investigation">3. Content Investigation</h1>

<p>Up until now we only looked at the general structure and quality of the dataset. Let’s now go a step further and take a look at the actual content. In an ideal setting, such an investigation would be done feature by feature. But this becomes very cumbersome once you have more than 20-30 features.</p>

<p>For this reason (and to keep this article as short as needed) we will explore three different approaches that can give you a very quick overview of the content stored in each feature and how they relate.</p>

<h2 id="31-feature-distribution">3.1. Feature distribution</h2>

<p>Looking at the value distribution of each feature is a great way to better understand the content of your data. Furthermore, it can help to guide your EDA, and provides a lot of useful information with regards to data cleaning and feature transformation. The quickest way to do this for numerical features is using histogram plots. Luckily, <code class="language-plaintext highlighter-rouge">pandas</code> comes with a builtin histogram function that allows the plotting of multiple features at once.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plots the histogram for each numerical feature in a separate subplot
</span><span class="n">df_X</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">25</span><span class="p">),</span> <span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">edgecolor</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_46_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>There are a lot of very interesting things visible in this plot. For example…</p>

<p><strong>Most frequent entry</strong>: Some features, such as <code class="language-plaintext highlighter-rouge">Towing_and_Articulation</code> or <code class="language-plaintext highlighter-rouge">Was_Vehicle_Left_Hand_Drive?</code> mostly contain entries of just one category. Using the <code class="language-plaintext highlighter-rouge">.mode()</code> function, we could for example extract the ratio of the most frequent entry for each feature and visualize that information.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Collects for each feature the most frequent entry
</span><span class="n">most_frequent_entry</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">mode</span><span class="p">()</span>

<span class="c1"># Checks for each entry if it contains the most frequent entry
</span><span class="n">df_freq</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">eq</span><span class="p">(</span><span class="n">most_frequent_entry</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Computes the mean of the 'is_most_frequent' occurrence
</span><span class="n">df_freq</span> <span class="o">=</span> <span class="n">df_freq</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># Show the 5 top features with the highest ratio of singular value content
</span><span class="nf">display</span><span class="p">(</span><span class="n">df_freq</span><span class="p">.</span><span class="nf">head</span><span class="p">())</span>

<span class="c1"># Visualize the 'df_freq' table
</span><span class="n">df_freq</span><span class="p">.</span><span class="n">plot</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">));</span>
</code></pre></div></div>

<pre><code class="language-code">Pedestrian_Crossing-Human_Control    0.995259
Was_Vehicle_Left_Hand_Drive?         0.990137
Carriageway_Hazards                  0.983646
Towing_and_Articulation              0.983221
Vehicle_Location-Restricted_Lane     0.982088
dtype: float64
</code></pre>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_48_1.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p><strong>Skewed value distributions</strong>: Certain kind of numerical features can also show strongly non-gaussian distributions. In that case you might want to think about how you can transform these values to make them more normal distributed. For example, for right skewed data you could use a log-transformation.</p>

<h2 id="32-feature-patterns">3.2. Feature patterns</h2>

<p>Next step on the list is the investigation of feature specific patterns. The goal of this part is two fold:</p>

<ol>
  <li>Can we identify particular patterns within a feature that will help us to decide if some entries need to be dropped or modified?</li>
  <li>Can we identify particular relationships between features that will help us to better understand our dataset?</li>
</ol>

<p>But before we dive into these two questions, let’s take a closer look at a few ‘randomly selected’ features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_X</span><span class="p">[[</span><span class="sh">"</span><span class="s">Location_Northing_OSGR</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">1st_Road_Number</span><span class="sh">"</span><span class="p">,</span>
      <span class="sh">"</span><span class="s">Journey_Purpose_of_Driver</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Pedestrian_Crossing-Physical_Facilities</span><span class="sh">"</span><span class="p">]].</span><span class="nf">plot</span><span class="p">(</span>
    <span class="n">lw</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="sh">"</span><span class="s">.</span><span class="sh">"</span><span class="p">,</span> <span class="n">subplots</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">markersize</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">6</span><span class="p">));</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_51_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>In the top row, we can see features with continuous values (e.g. seemingly any number from the number line), while in the bottom row we have features with discrete values (e.g. 1, 2, 3 but not 2.34).</p>

<p>While there are many ways we could explore our features for particular patterns, let’s simplify our option by deciding that we treat features with less than 25 unique features as <strong>discrete</strong> or <strong>ordinal</strong> features, and the other features as <strong>continuous</strong> features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Creates mask to identify numerical features with more or less than 25 unique features
</span><span class="n">cols_continuous</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="sh">"</span><span class="s">number</span><span class="sh">"</span><span class="p">).</span><span class="nf">nunique</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">25</span>
</code></pre></div></div>

<h3 id="321-continuous-features">3.2.1. Continuous features</h3>

<p>Now that we have a way to select the continuous features, let’s go ahead and use seaborn’s <code class="language-plaintext highlighter-rouge">pairplot</code> to visualize the relationships between these features. <strong>Important to note</strong>, seaborn’s pairplot routine can take a long time to create all subplots. Therefore we recommend to not use it for more than ~10 features at a time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a new dataframe which only contains the continuous features
</span><span class="n">df_continuous</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">[</span><span class="n">cols_continuous</span><span class="p">[</span><span class="n">cols_continuous</span><span class="p">].</span><span class="n">index</span><span class="p">]</span>
<span class="n">df_continuous</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(317665, 11)
</code></pre>

<p>Given that in our case we only have 11 features, we can go ahead with the pairplot. Otherwise, using something like <code class="language-plaintext highlighter-rouge">df_continuous.iloc[:, :5]</code> could help to reduce the number of features to plot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span><span class="n">df_continuous</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">plot_kws</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">s</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">});</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_57_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>There seems to be a strange relationship between a few features in the top left corner. <code class="language-plaintext highlighter-rouge">Location_Easting_OSGR</code> and <code class="language-plaintext highlighter-rouge">Longitude</code>, as well as <code class="language-plaintext highlighter-rouge">Location_Easting_OSGR</code> and <code class="language-plaintext highlighter-rouge">Latitude</code> seem to have a very strong linear relationship.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sns</span><span class="p">.</span><span class="nf">pairplot</span><span class="p">(</span>
    <span class="n">df_X</span><span class="p">,</span> <span class="n">plot_kws</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">s</span><span class="sh">"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="sh">"</span><span class="s">alpha</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">},</span> <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">Police_Force</span><span class="sh">"</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">Spectral</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">x_vars</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Location_Easting_OSGR</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Location_Northing_OSGR</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Longitude</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">y_vars</span><span class="o">=</span><span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_59_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Knowing that these features contain geographic information, a more in-depth EDA with regards to geolocation could be fruitful. However, for now we will leave the further investigation of this pairplot to the curious reader and continue with the exploration of the discrete and ordinal features.</p>

<h3 id="322-discrete-and-ordinal-features">3.2.2. Discrete and ordinal features</h3>

<p>Finding patterns in the discrete or ordinal features is a bit more tricky. But also here, some quick pandas and seaborn trickery can help us to get a general overview of our dataset. First, let’s select the columns we want to investigate.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a new dataframe which doesn't contain the numerical continuous features
</span><span class="n">df_discrete</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">[</span><span class="n">cols_continuous</span><span class="p">[</span><span class="o">~</span><span class="n">cols_continuous</span><span class="p">].</span><span class="n">index</span><span class="p">]</span>
<span class="n">df_discrete</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<pre><code class="language-code">(317665, 44)
</code></pre>

<p>As always, there are multiple way for how we could investigate all of these features. Let’s try one example, using seaborn’s <code class="language-plaintext highlighter-rouge">stripplot()</code> together with a handy <code class="language-plaintext highlighter-rouge">zip()</code> for-loop for subplots.</p>

<p><strong>Note</strong>, to spread the values out in the direction of the y-axis we need to chose one particular (hopefully informative) feature. While the ‘right’ feature can help to identify some interesting patterns, usually any continuous feature should do the trick. The main interest in this kind of plot is to see how many samples each discrete value contains.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Establish number of columns and rows needed to plot all features
</span><span class="n">n_cols</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_elements</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">df_discrete</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">n_rows</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">n_elements</span> <span class="o">/</span> <span class="n">n_cols</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">int</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># Specify y_value to spread data (ideally a continuous feature)
</span><span class="n">y_value</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">[</span><span class="sh">"</span><span class="s">Age_of_Driver</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Create figure object with as many rows and columns as needed
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="n">n_cols</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="n">n_rows</span> <span class="o">*</span> <span class="mf">2.5</span><span class="p">))</span>

<span class="c1"># Loop through features and put each subplot on a matplotlib axis object
</span><span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">df_discrete</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()):</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">stripplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_X</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_value</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">tab10</span><span class="sh">"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_64_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>There are too many things to comment here, so let’s just focus on a few. In particular, let’s focus on 6 features where the values appear in some particular pattern or where some categories seem to be much less frequent than others. And to shake things up a bit, let’s now use the <code class="language-plaintext highlighter-rouge">Longitude</code> feature to stretch the values over the y-axis.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Specify features of interest
</span><span class="n">selected_features</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Vehicle_Reference_df_res</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Towing_and_Articulation</span><span class="sh">"</span><span class="p">,</span>
                     <span class="sh">"</span><span class="s">Skidding_and_Overturning</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Bus_or_Coach_Passenger</span><span class="sh">"</span><span class="p">,</span>
                     <span class="sh">"</span><span class="s">Pedestrian_Road_Maintenance_Worker</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Age_Band_of_Driver</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Create a figure with 3 x 2 subplots
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Loop through these features and plot entries from each feature against `Latitude`
</span><span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">selected_features</span><span class="p">,</span> <span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()):</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">stripplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_X</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">df_X</span><span class="p">[</span><span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>
                  <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">tab10</span><span class="sh">"</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_66_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>These kind of plots are already very informative, but they obscure regions where there are a lot of data points at once. For example, there seems to be a high density of points in some of the plots at the 52nd latitude. So let’s take a closer look with an appropriate plot, such as <code class="language-plaintext highlighter-rouge">violineplot</code> ( or <code class="language-plaintext highlighter-rouge">boxenplot</code> or <code class="language-plaintext highlighter-rouge">boxplot</code> for that matter). And to go a step further, let’s also separate each visualization by <code class="language-plaintext highlighter-rouge">Urban_or_Rural_Area</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a figure with 3 x 2 subplots
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># Loop through these features and plot entries from each feature against `Latitude`
</span><span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">selected_features</span><span class="p">,</span> <span class="n">axes</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()):</span>
    <span class="n">sns</span><span class="p">.</span><span class="nf">violinplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df_X</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">df_X</span><span class="p">[</span><span class="sh">"</span><span class="s">Latitude</span><span class="sh">"</span><span class="p">],</span> <span class="n">palette</span><span class="o">=</span><span class="sh">"</span><span class="s">Set2</span><span class="sh">"</span><span class="p">,</span>
                   <span class="n">split</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="sh">"</span><span class="s">Urban_or_Rural_Area</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">();</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_68_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Interesting! We can see that some values on features are more frequent in urban, than in rural areas (and vice versa). Furthermore, as suspected, there seems to be a high density peak at latitude 51.5. This is very likely due to the more densely populated region around London (at 51.5074°).</p>

<h2 id="33-feature-relationships">3.3. Feature relationships</h2>

<p>Last, but not least, let’s take a look at relationships between features. More precisely how they correlate. The quickest way to do so is via pandas’ <code class="language-plaintext highlighter-rouge">.corr()</code> function. So let’s go ahead and compute the feature to feature correlation matrix for all numerical features.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Computes feature correlation
</span><span class="n">df_corr</span> <span class="o">=</span> <span class="n">df_X</span><span class="p">.</span><span class="nf">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">"</span><span class="s">pearson</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Note</strong>: Depending on the dataset and the kind of features (e.g. ordinal or continuous features) you might want to use the <code class="language-plaintext highlighter-rouge">spearman</code> method instead of the <code class="language-plaintext highlighter-rouge">pearson</code> method to compute the correlation. Whereas the <strong>Pearson</strong> correlation evaluates the linear relationship between two continuous variables, the <strong>Spearman</strong> correlation evaluates the monotonic relationship based on the ranked values for each feature. And to help with the interpretation of this correlation matrix, let’s use seaborn’s <code class="language-plaintext highlighter-rouge">.heatmap()</code> to visualize it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create labels for the correlation matrix
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">df_corr</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">0.75</span><span class="p">,</span> <span class="sh">"</span><span class="s">S</span><span class="sh">"</span><span class="p">,</span>
                  <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">df_corr</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">0.5</span><span class="p">,</span> <span class="sh">"</span><span class="s">M</span><span class="sh">"</span><span class="p">,</span>
                           <span class="n">np</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">df_corr</span><span class="p">)</span><span class="o">&gt;</span><span class="mf">0.25</span><span class="p">,</span> <span class="sh">"</span><span class="s">W</span><span class="sh">"</span><span class="p">,</span> <span class="sh">""</span><span class="p">)))</span>

<span class="c1"># Plot correlation matrix
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">df_corr</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">df_corr</span><span class="p">)),</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">''</span><span class="p">,</span> <span class="n">linewidths</span><span class="o">=</span><span class="p">.</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">vlag</span><span class="sh">"</span><span class="p">,</span> <span class="n">cbar_kws</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">shrink</span><span class="sh">"</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">});</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/03_advanced_eda/output_73_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>This looks already very interesting. We can see a few very strong correlations between some of the features. Now, if you’re interested actually ordering all of these different correlations, you could do something like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#  Creates a mask to remove the diagonal and the upper triangle.
</span><span class="n">lower_triangle_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">df_corr</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">bool</span><span class="sh">"</span><span class="p">)</span>

<span class="c1">#  Stack all correlations, after applying the mask
</span><span class="n">df_corr_stacked</span> <span class="o">=</span> <span class="n">df_corr</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">lower_triangle_mask</span><span class="p">).</span><span class="nf">stack</span><span class="p">().</span><span class="nf">sort_values</span><span class="p">()</span>

<span class="c1">#  Showing the lowest and highest correlations in the correlation matrix
</span><span class="nf">display</span><span class="p">(</span><span class="n">df_corr_stacked</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">Local_Authority_(District)  Longitude                -0.509343
                            Location_Easting_OSGR    -0.502919
Police_Force                Longitude                -0.471327
                            Location_Easting_OSGR    -0.461112
Speed_limit                 1st_Road_Class           -0.438931
                                                        ...   
Age_Band_of_Casualty        Age_of_Casualty           0.974397
Age_Band_of_Driver          Age_of_Driver             0.979019
Local_Authority_(District)  Police_Force              0.984819
Longitude                   Location_Easting_OSGR     0.999363
Latitude                    Location_Northing_OSGR    0.999974
Length: 1485, dtype: float64
</code></pre>

<p>As you can see, the investigation of feature correlations can be very informative. But looking at everything at once can sometimes be more confusing than helpful. So focusing only on one feature with something like <code class="language-plaintext highlighter-rouge">df_X.corrwith(df_X["Speed_limit"])</code> might be a better approach.</p>

<p>Furthermore, correlations can be deceptive if a feature still contains a lot of missing values or extreme outliers. Therefore, it is always important to first make sure that your feature matrix is properly prepared before investigating these correlations.</p>

<h2 id="34-conclusion-of-content-investigation">3.4. Conclusion of content investigation</h2>

<p>At the end of this third investigation, we should have a better understanding of the content in our dataset. We looked at value distribution, feature patterns and feature correlations. However, these are certainly not all possible content investigation and data cleaning steps you could do. Additional steps would for example be outlier detection and removal, feature engineering and transformation, and more.</p>

<h1 id="take-home-message">Take home message</h1>

<p>A proper and detailed EDA takes time! It is a very iterative process that often makes you go back to the start, after you addressed another flaw in the dataset. This is normal! It’s the reason why we often say that 80% of any data science project is data preparation and EDA.</p>

<p>But keep also in mind that an in-depth EDA can consume a lot of time. And just because something seems interesting doesn’t mean that you need to follow up on it. Always remind yourself what the dataset will be used for and tailor your investigations to support that goal. And sometimes it is also ok, to just do a quick-and-dirty data preparation and exploration. So that you can move on to the data modeling part rather quickly, and to establish a few preliminary baseline models perform some informative results investigation.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How to quickly get a handle on almost any tabular dataset]]></summary></entry><entry><title type="html">Color engineering for special images</title><link href="https://miykael.github.io/blog/2021/color_engineering_medmnist/" rel="alternate" type="text/html" title="Color engineering for special images" /><published>2021-12-31T12:00:00+00:00</published><updated>2021-12-31T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2021/color_engineering_medmnist</id><content type="html" xml:base="https://miykael.github.io/blog/2021/color_engineering_medmnist/"><![CDATA[<h2 id="how-to-improve-color-encoding-of-unnatural-images">How to improve color encoding of unnatural images</h2>

<p><em>[Find the Jupyter Notebook to this article <a href="https://github.com/miykael/miykael.github.io/blob/master/assets/nb/02_color_engineering_medmnist/nb_color_engineering_medmnist.ipynb">here</a>.]</em></p>

<hr />

<p><strong>Not all images that are colorful, should be colorful.</strong> Or in other words, not all images that use RGB (red, green, blue) encoding should be using exactly these colors! In this article we will explore how different ways of feature engineering - i.e. spreading the original color values out - can help with improving the classification performance of a convolutional neural network.</p>

<p>Note, there are numerous ways of how you can change and adapt the color encoding of RGB images (e.g. transforming RGB to HSV, LAB or XYZ values; <code class="language-plaintext highlighter-rouge">scikit-image</code> offers <a href="https://scikit-image.org/docs/stable/auto_examples/#manipulating-exposure-and-color-channels">many great routines</a> to do this) - however this article is not about that. It’s much more about thinking about what the data tries to capture and how to exploit that.</p>

<p>To better highlight what I mean by that, let’s take a look at the following three datasets (each image shows 100s of individual images from that dataset):</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/medmnist.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p><em>These three datasets are part of the <a href="https://medmnist.com">MedMNIST</a> dataset - images are taken from the <a href="https://arxiv.org/pdf/2010.14925.pdf">corresponding paper</a>.</em></p>

<p>What these datasets have in common is the fact that the individual images from a given dataset very much stick to a specific color range. While there are fluctuations in pink or red tones, for most of these images it is more important how the contrast differs between images than what the actual RGB color value represents.</p>

<p>This offers us a unique opportunity for feature engineering. Instead of sticking with the original RGB color values, we can investigate if an adaptation of the dataset specific color space can help and improve our data science investigations.</p>

<h1 id="the-data-set">The data set</h1>

<p>To explore this topic, let’s use the augmented blood cell dataset (see original <a href="https://www.sciencedirect.com/science/article/pii/S2352340920303681">paper</a>) from <a href="https://medmnist.com">MedMNIST</a>. This dataset contains roughly 17’000 individual images from 10 different blood cell types.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/blood_cell.jpg" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p><em>Image is taken from the original <a href="https://www.sciencedirect.com/science/article/pii/S2352340920303681">paper</a> and depicts the ten types of blood cell types that conform the dataset.</em></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Download dataset
</span><span class="err">!</span><span class="n">wget</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">zenodo</span><span class="p">.</span><span class="n">org</span><span class="o">/</span><span class="n">record</span><span class="o">/</span><span class="mi">5208230</span><span class="o">/</span><span class="n">files</span><span class="o">/</span><span class="n">bloodmnist</span><span class="p">.</span><span class="n">npz</span>

<span class="c1"># Load packages
</span><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">glob</span> <span class="kn">import</span> <span class="n">glob</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="n">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="n">sns</span><span class="p">.</span><span class="nf">set_context</span><span class="p">(</span><span class="sh">"</span><span class="s">talk</span><span class="sh">"</span><span class="p">)</span>
<span class="o">%</span><span class="n">config</span> <span class="n">InlineBackend</span><span class="p">.</span><span class="n">figure_format</span> <span class="o">=</span> <span class="sh">'</span><span class="s">retina</span><span class="sh">'</span>

<span class="c1"># Load data set
</span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">bloodmnist.npz</span><span class="sh">"</span><span class="p">)</span>
<span class="n">X_tr</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">train_images</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">X_va</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">val_images</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">X_te</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test_images</span><span class="sh">"</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
<span class="n">y_tr</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">train_labels</span><span class="sh">"</span><span class="p">]</span>
<span class="n">y_va</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">val_labels</span><span class="sh">"</span><span class="p">]</span>
<span class="n">y_te</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">test_labels</span><span class="sh">"</span><span class="p">]</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">basophils</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">eosinophils</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">erythroblasts</span><span class="sh">"</span><span class="p">,</span>
          <span class="sh">"</span><span class="s">granulocytes_immature</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">lymphocytes</span><span class="sh">"</span><span class="p">,</span>
          <span class="sh">"</span><span class="s">monocytes</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">neutrophils</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">platelets</span><span class="sh">"</span><span class="p">]</span>

<span class="n">labels_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_tr</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()])</span>
<span class="n">labels_va</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_va</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()])</span>
<span class="n">labels_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()])</span>
</code></pre></div></div>

<p>So let’s take a look at a few images from this dataset!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Helper function to visualize first few images of a dataset.</span><span class="sh">"""</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">15.5</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">.</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># Plot dataset
</span><span class="nf">plot_dataset</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_9_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p><strong>What you can see is the following:</strong> The background color, as well as the main target object color is in most of the cases the same (but not always)! To better understand why this offers us an opportunity for color value engineering, let’s take a look at the RGB color space that these images occupy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract a few RGB color values
</span><span class="n">X_colors</span> <span class="o">=</span> <span class="n">X_tr</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)[::</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># Plot color values in 3D space
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Loop through 3 different views
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">view</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([[</span><span class="o">-</span><span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">3d</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_colors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_colors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_colors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">X_colors</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">G</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">elev</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Colors in RGB space</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_11_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>These are three different views on the same RGB color space of the original dataset. What you can see is that this dataset only covers a small section of the full cube, i.e. all 16’777’216 possible color values. This offers us now three unique opportunities:</p>

<ol>
  <li>We could <strong>reduce the image complexity</strong> by transforming the RGB colors to <strong>grayscale images</strong>.</li>
  <li>We could <strong>realign and stretch the color values</strong> so that the RGB values better fill the RGB color space.</li>
  <li>We could <strong>reorient the color values</strong> so that the three cube axis stretch into the direction of highest variance. This is best done via a PCA approach.</li>
</ol>

<p><strong>Note:</strong> There are of course multiple other ways how we could manipulate our color values, but for this article, we will go with the three mentioned here.</p>

<h1 id="data-set-augmentation">Data set augmentation</h1>

<h2 id="1-grayscale-transformation">1. Grayscale transformation</h2>

<p>First things first, let’s transform the RGB images to grayscale images (i.e. go from a 3D to a 1D data set). Note, grayscale images are not just a simple averaging of RGB, but a slight imbalanced weighting thereof. We will use <code class="language-plaintext highlighter-rouge">scikit-image</code>’s <code class="language-plaintext highlighter-rouge">rgb2gray</code> to perform this transformation. Furthermore, we will stretch the grayscale values to fully cover the 0 to 255 value range of images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Install scikit-image if not already done
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">scikit</span><span class="o">-</span><span class="n">image</span>

<span class="kn">from</span> <span class="n">skimage.color</span> <span class="kn">import</span> <span class="n">rgb2gray</span>

<span class="c1"># Create grayscale images
</span><span class="n">X_tr_gray</span> <span class="o">=</span> <span class="nf">rgb2gray</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)[...,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">X_va_gray</span> <span class="o">=</span> <span class="nf">rgb2gray</span><span class="p">(</span><span class="n">X_va</span><span class="p">)[...,</span> <span class="bp">None</span><span class="p">]</span>
<span class="n">X_te_gray</span> <span class="o">=</span> <span class="nf">rgb2gray</span><span class="p">(</span><span class="n">X_te</span><span class="p">)[...,</span> <span class="bp">None</span><span class="p">]</span>

<span class="c1"># Stretch color range to training min, max
</span><span class="n">gmin_tr</span> <span class="o">=</span> <span class="n">X_tr_gray</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span>
<span class="n">X_tr_gray</span> <span class="o">-=</span> <span class="n">gmin_tr</span>
<span class="n">X_va_gray</span> <span class="o">-=</span> <span class="n">gmin_tr</span>
<span class="n">X_te_gray</span> <span class="o">-=</span> <span class="n">gmin_tr</span>

<span class="n">gmax_tr</span> <span class="o">=</span> <span class="n">X_tr_gray</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>
<span class="n">X_tr_gray</span> <span class="o">/=</span> <span class="n">gmax_tr</span>
<span class="n">X_va_gray</span> <span class="o">/=</span> <span class="n">gmax_tr</span>
<span class="n">X_te_gray</span> <span class="o">/=</span> <span class="n">gmax_tr</span>

<span class="n">X_va_gray</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_va_gray</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_te_gray</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_te_gray</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Let’s take a look at how these grayscale color values are positioned in the previous RGB color space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Put 1D values into 3D space
</span><span class="n">X_tr_show</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">X_tr_gray</span><span class="p">,</span> <span class="n">X_tr_gray</span><span class="p">,</span> <span class="n">X_tr_gray</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Extract a few grayscale color values
</span><span class="n">X_grays</span> <span class="o">=</span> <span class="n">X_tr_show</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)[::</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># Plot color values in 3D space
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Loop through 3 different views
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">view</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([[</span><span class="o">-</span><span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">3d</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_grays</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_grays</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_grays</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">X_grays</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">G</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">elev</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Colors in Grayscale space</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_17_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Not surprising, the grayscale color values lay exactly on the cube diagonal. As such we were able to reduce our three dimensional dataset to one dimension. Let’s take a look at how the cell images look like in grayscale format.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot dataset
</span><span class="nf">plot_dataset</span><span class="p">(</span><span class="n">X_tr_gray</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_19_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="2-color-realigning-and-stretching">2. Color realigning and stretching</h2>

<p>In the first RGB cube plot we saw that the color values of this dataset occupies only a part of the full cube. However, when comparing that point cloud to the black-white diagonal line from the second RGB cube plot, we can see that the original colors are off axis and slightly bend.</p>

<p>To better illustrate what we mean by that, let’s try to find the equally spaced ‘cloud centroids’ (or center of masses) that support this point cloud.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get RGB color values
</span><span class="n">X_colors</span> <span class="o">=</span> <span class="n">X_tr</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Get distance of all color values to black (0,0,0)
</span><span class="n">dist_origin</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">X_colors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Find index of 0.1% smallest entry
</span><span class="n">perc_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">dist_origin</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">dist_origin</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)))</span>

<span class="c1"># Find centroid of lowest 0.1% RGBs
</span><span class="n">centroid_low</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X_colors</span><span class="p">[</span><span class="n">perc_sorted</span><span class="p">][:</span> <span class="nf">len</span><span class="p">(</span><span class="n">X_colors</span><span class="p">)</span> <span class="o">//</span> <span class="mi">1000</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Order all RGB values with regards to distance to low centroid
</span><span class="n">order_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">X_colors</span> <span class="o">-</span> <span class="n">centroid_low</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>

<span class="c1"># According to this order, divide all RGB values into N equal sized chunks
</span><span class="n">nth</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array_split</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">order_idx</span><span class="p">)),</span> <span class="n">nth</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Compute centroids, i.e. RGB mean values of each segment
</span><span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="nf">median</span><span class="p">(</span><span class="n">X_colors</span><span class="p">[</span><span class="n">order_idx</span><span class="p">][</span><span class="n">s</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">splits</span><span class="p">)])</span>

<span class="c1"># Only keep centroids that are spaced enough
</span><span class="n">new_centers</span> <span class="o">=</span> <span class="p">[</span><span class="n">centroids</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">centroids</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">new_centers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.03</span><span class="p">:</span>
        <span class="n">new_centers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">new_centers</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">new_centers</span><span class="p">)</span>
</code></pre></div></div>

<p>Once we have these centers, as before, let’s visualize them in the RGB color cube. And to support our point, let’s also add the grayscale diagonal to this plot.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot centroids in 3D space
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Loop through 3 different views
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">view</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([[</span><span class="o">-</span><span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">3d</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_grays</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_grays</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_grays</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">X_grays</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
        <span class="n">new_centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">new_centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">new_centers</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">facecolors</span><span class="o">=</span><span class="n">new_centers</span><span class="p">,</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">G</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">elev</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Color centroids in RGB space</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_23_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>So how can we use these two information (the cloud centroids and the gray scale diagonal) to <strong>realign and stretch</strong> our original dataset? Well, one way to do so is the following:</p>

<ol>
  <li>For each color value in the original dataset we compute the distance vector to the closest cloud centroid.</li>
  <li>Then we add this distance vector to the grayscale diagonal (i.e. realign the cloud centroids).</li>
  <li>And as last step we stretch and clip color values to make sure that 99.9% all values are in the desired color range.</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create grayscale diagonal centroids (equal numbers to cloud centroids)
</span><span class="n">steps</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">new_centers</span><span class="p">))</span>

<span class="c1"># Realign and stretch images
</span><span class="n">X_tr_stretch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))]</span>
     <span class="o">+</span> <span class="n">steps</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))]</span>
     <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">X_tr</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))])</span>
<span class="n">X_va_stretch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))]</span>
     <span class="o">+</span> <span class="n">steps</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))]</span>
     <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">X_va</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))])</span>
<span class="n">X_te_stretch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
    <span class="p">[</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))]</span>
     <span class="o">+</span> <span class="n">steps</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmin</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">norm</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">new_centers</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))]</span>
     <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">X_te</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))])</span>

<span class="c1"># Stretch and clip data
</span><span class="n">xmin_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_tr_stretch</span> <span class="o">-=</span> <span class="n">xmin_tr</span>
<span class="n">X_va_stretch</span> <span class="o">-=</span> <span class="n">xmin_tr</span>
<span class="n">X_te_stretch</span> <span class="o">-=</span> <span class="n">xmin_tr</span>
<span class="n">xmax_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">,</span> <span class="mf">99.95</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_tr_stretch</span> <span class="o">/=</span> <span class="n">xmax_tr</span>
<span class="n">X_va_stretch</span> <span class="o">/=</span> <span class="n">xmax_tr</span>
<span class="n">X_te_stretch</span> <span class="o">/=</span> <span class="n">xmax_tr</span>
<span class="n">X_tr_stretch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_va_stretch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_va_stretch</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_te_stretch</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_te_stretch</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>So, what do the original RGB color values look like once they were realigned and stretched?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot color values in 3D space
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">stretch_colors</span> <span class="o">=</span> <span class="n">X_tr_stretch</span><span class="p">[::</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># Loop through 3 different views
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">view</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([[</span><span class="o">-</span><span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">3d</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span>
        <span class="n">stretch_colors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">stretch_colors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">stretch_colors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span>
        <span class="n">facecolors</span><span class="o">=</span><span class="n">stretch_colors</span><span class="p">,</span>
        <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">R</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">G</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">B</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">elev</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Colors in realigned and stretched space</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_27_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>That looks already very promising. The point cloud is much better aligned to the cube diagonal and it seems the point cloud is a bit more stretched into all directions. And what do the cellular images look like in this new color encoding?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert data back into image space
</span><span class="n">X_tr_stretch</span> <span class="o">=</span> <span class="n">X_tr_stretch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_tr</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X_va_stretch</span> <span class="o">=</span> <span class="n">X_va_stretch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_va</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X_te_stretch</span> <span class="o">=</span> <span class="n">X_te_stretch</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_te</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot dataset
</span><span class="nf">plot_dataset</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_29_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="3-pca-transformation">3. PCA transformation</h2>

<p>Last but certainly not least of our three options, let’s use a PCA approach to transform the original RGB color values into a new 3D space, where each of these three new axis explains as much variance as possible. <strong>Note:</strong> For this approach we will use the original RGB color values, but we could of course also use the just realigned and stretched values as well.</p>

<p>So what do the original RGB color values look like in this new PCA color space?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train PCA decomposition on original RGB values
</span><span class="kn">from</span> <span class="n">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>

<span class="n">pca</span> <span class="o">=</span> <span class="nc">PCA</span><span class="p">()</span>
<span class="n">pca</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_tr</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Transform all data sets into new PCA space
</span><span class="n">X_tr_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_tr</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">X_va_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_va</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">X_te_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_te</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Stretch and clip data
</span><span class="n">xmin_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_tr_pca</span> <span class="o">-=</span> <span class="n">xmin_tr</span>
<span class="n">X_va_pca</span> <span class="o">-=</span> <span class="n">xmin_tr</span>
<span class="n">X_te_pca</span> <span class="o">-=</span> <span class="n">xmin_tr</span>
<span class="n">xmax_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">,</span> <span class="mf">99.95</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_tr_pca</span> <span class="o">/=</span> <span class="n">xmax_tr</span>
<span class="n">X_va_pca</span> <span class="o">/=</span> <span class="n">xmax_tr</span>
<span class="n">X_te_pca</span> <span class="o">/=</span> <span class="n">xmax_tr</span>
<span class="n">X_tr_pca</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_va_pca</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_va_pca</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_te_pca</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">X_te_pca</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Flip first component
</span><span class="n">X_tr_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">X_tr_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">X_va_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">X_va_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">X_te_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">X_te_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Extract a few RGB color values
</span><span class="n">X_colors</span> <span class="o">=</span> <span class="n">X_tr_pca</span><span class="p">[::</span><span class="mi">100</span><span class="p">].</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Plot color values in 3D space
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="c1"># Loop through 3 different views
</span><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">view</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">([[</span><span class="o">-</span><span class="mi">45</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">80</span><span class="p">],</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">10</span><span class="p">]]):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="p">.</span><span class="nf">add_subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="sh">"</span><span class="s">3d</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">X_colors</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_colors</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">X_colors</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">X_colors</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PC1</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PC2</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">PC3</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">yaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">zaxis</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">view_init</span><span class="p">(</span><span class="n">azim</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">elev</span><span class="o">=</span><span class="n">view</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">vertical_axis</span><span class="o">=</span><span class="sh">"</span><span class="s">z</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sh">"</span><span class="s">Colors in PCA space</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_31_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Interesting, the stretching worked very well! But what about the images? Let’s see.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Convert data back into image space
</span><span class="n">X_tr_pca</span> <span class="o">=</span> <span class="n">X_tr_pca</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_tr</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X_va_pca</span> <span class="o">=</span> <span class="n">X_va_pca</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_va</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">X_te_pca</span> <span class="o">=</span> <span class="n">X_te_pca</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">X_te</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot dataset
</span><span class="nf">plot_dataset</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_33_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>That looks interesting as well. Unique colors, e.g. background, nucleus and the thing that surrounds the nucleus all have unique colors. However, our PCA transformation also brought forward an artifact in the images - the cross like color border in the middle of the image. It is unclear where this is coming from, but I assume that it’s due to the dataset manipulation that MedMNIST introduced by downsampling the original dataset.</p>

<h1 id="feature-correlation">Feature correlation</h1>

<p>Before moving on to the next part of our investigation (i.e. testing if these color manipulation help a convolutional neural network to classify the 10 target classes), let’s take a quick look at how these new color values correlate to each other.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Combine all images in one big dataframe
</span><span class="n">X_tr_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">X_tr</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_tr_gray</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_tr_stretch</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_tr_pca</span><span class="p">.</span><span class="n">T</span><span class="p">]).</span><span class="n">T</span>
<span class="n">X_va_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">X_va</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_va_gray</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_va_stretch</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_va_pca</span><span class="p">.</span><span class="n">T</span><span class="p">]).</span><span class="n">T</span>
<span class="n">X_te_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">X_te</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_te_gray</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_te_stretch</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_te_pca</span><span class="p">.</span><span class="n">T</span><span class="p">]).</span><span class="n">T</span>

<span class="c1"># Compute correlation matrix between all color features
</span><span class="n">corr_all</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">corrcoef</span><span class="p">(</span><span class="n">X_tr_all</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">X_tr_all</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="n">T</span><span class="p">)</span>
<span class="n">cols</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">Red</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Green</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Blue</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Gray</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">Stretch1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stretch2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stretch3</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">PC1</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PC2</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PC3</span><span class="sh">"</span><span class="p">]</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span>
    <span class="mi">100</span> <span class="o">*</span> <span class="n">corr_all</span><span class="p">,</span>
    <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">.0f</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">xticklabels</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span>
    <span class="n">yticklabels</span><span class="o">=</span><span class="n">cols</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_36_0.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>As we can see, many of the new color features correlate highly with the original RGB values (except for the 2nd and 3rd PCA features). So let’s see if any of our color manipulation help with the image classification.</p>

<h1 id="testing-color-manipulations-in-image-classification">Testing color manipulations in image classification</h1>

<p>Let’s see if our color manipulation is helping a convolutional neural network to classify the 8 target classes. To do so, let’s create a ‘smallish’ ResNet model and train it on all 4 versions of our dataset (i.e. original, grayscale, stretched, and PCA).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The code for this ResNet architecture was adapted from here:
# https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba
</span><span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="p">(</span><span class="n">Input</span><span class="p">,</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">ReLU</span><span class="p">,</span> <span class="n">BatchNormalization</span><span class="p">,</span> <span class="n">Add</span><span class="p">,</span>
                                     <span class="n">AveragePooling2D</span><span class="p">,</span> <span class="n">Flatten</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">)</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>

<span class="k">def</span> <span class="nf">relu_bn</span><span class="p">(</span><span class="n">inputs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">relu</span> <span class="o">=</span> <span class="nc">ReLU</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">bn</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">relu</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">bn</span>

<span class="k">def</span> <span class="nf">residual_block</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">downsample</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span> <span class="n">filters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">downsample</span> <span class="k">else</span> <span class="mi">2</span><span class="p">),</span>
        <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nf">relu_bn</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)(</span><span class="n">y</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">downsample</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nc">Add</span><span class="p">()([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">])</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nf">relu_bn</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">create_res_net</span><span class="p">(</span><span class="n">in_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>

    <span class="n">inputs</span> <span class="o">=</span> <span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">in_shape</span><span class="p">)</span>
    <span class="n">num_filters</span> <span class="o">=</span> <span class="mi">32</span>

    <span class="n">t</span> <span class="o">=</span> <span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="nc">Conv2D</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">)(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="nf">relu_bn</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">num_blocks_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">num_blocks_list</span><span class="p">)):</span>
        <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">num_blocks_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
            <span class="n">t</span> <span class="o">=</span> <span class="nf">residual_block</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">downsample</span><span class="o">=</span><span class="p">(</span><span class="n">j</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">),</span> <span class="n">filters</span><span class="o">=</span><span class="n">num_filters</span><span class="p">)</span>
        <span class="n">num_filters</span> <span class="o">*=</span> <span class="mi">2</span>

    <span class="n">t</span> <span class="o">=</span> <span class="nc">AveragePooling2D</span><span class="p">(</span><span class="mi">4</span><span class="p">)(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="nc">Flatten</span><span class="p">()(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">t</span> <span class="o">=</span> <span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="nc">Dense</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">)(</span><span class="n">t</span><span class="p">)</span>

    <span class="n">model</span> <span class="o">=</span> <span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

    <span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="o">=</span><span class="sh">"</span><span class="s">adam</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">"</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">run_resnet</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">X_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Support function to train ResNet model</span><span class="sh">"""</span>

    <span class="c1"># Create Model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nf">create_res_net</span><span class="p">(</span><span class="n">in_shape</span><span class="o">=</span><span class="n">X_tr</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>

    <span class="c1"># Creates 'EarlyStopping' callback
</span>    <span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>

    <span class="n">earlystopping_cb</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">EarlyStopping</span><span class="p">(</span>
        <span class="n">patience</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">restore_best_weights</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># Train model
</span>    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
        <span class="n">X_tr</span><span class="p">,</span>
        <span class="n">y_tr</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">earlystopping_cb</span><span class="p">],</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">history</span>

<span class="k">def</span> <span class="nf">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Support function to plot model history</span><span class="sh">"""</span>

    <span class="c1"># Plots neural network performance metrics for train and validation
</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>
    <span class="n">results</span><span class="p">[[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">val_accuracy</span><span class="sh">"</span><span class="p">]].</span><span class="nf">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">results</span><span class="p">[[</span><span class="sh">"</span><span class="s">loss</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">val_loss</span><span class="sh">"</span><span class="p">]].</span><span class="nf">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">logy</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">plot_classification_report</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">model</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Support function to plot classification report</span><span class="sh">"""</span>

    <span class="c1"># Show classification report
</span>    <span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_pred</span><span class="p">))</span>

    <span class="c1"># Show confusion matrix
</span>    <span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
    <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_predictions</span><span class="p">(</span>
        <span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">colorbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">inferno_r</span><span class="sh">"</span><span class="p">)</span>

    <span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>

    <span class="n">ConfusionMatrixDisplay</span><span class="p">.</span><span class="nf">from_predictions</span><span class="p">(</span>
        <span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(),</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="sh">"</span><span class="s">true</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">values_format</span><span class="o">=</span><span class="sh">"</span><span class="s">.1f</span><span class="sh">"</span><span class="p">,</span> <span class="n">colorbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">inferno_r</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="classification-performance-on-original-data-set">Classification performance on original data set</h2>

<p>Let’s establish a baseline by first training a ResNet model on the original dataset. The following shows the model’s performance (on accuracy and loss) during training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train model
</span><span class="n">model_orig</span><span class="p">,</span> <span class="n">history_orig</span> <span class="o">=</span> <span class="nf">run_resnet</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">X_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>

<span class="c1"># Show model performance during training
</span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history_orig</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_41_1.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Now that the model is trained, let’s see how well it’s capable in detecting the 8 target classes, by looking at the corresponding confusion matrices. On the left the confusion matrix shows the number of correctly/falsely identified samples, while on the right it shows the proportional values per target class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate Model
</span><span class="n">loss_orig_tr</span><span class="p">,</span> <span class="n">acc_orig_tr</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">loss_orig_va</span><span class="p">,</span> <span class="n">acc_orig_va</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_va</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>
<span class="n">loss_orig_te</span><span class="p">,</span> <span class="n">acc_orig_te</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="c1"># Report classification report and confusion matrix
</span><span class="nf">plot_classification_report</span><span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">model_orig</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">Train score: loss = 0.0537 - accuracy = 0.9817
Valid score: loss = 0.1816 - accuracy = 0.9492
Test score:  loss = 0.1952 - accuracy = 0.9421
</code></pre>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_43_1.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="classification-performance-on-grayscale-data-set">Classification performance on grayscale data set</h2>

<p>Now let’s do the same for the gray scale transformed images. How does the model performance during training look like?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train model
</span><span class="n">model_gray</span><span class="p">,</span> <span class="n">history_gray</span> <span class="o">=</span> <span class="nf">run_resnet</span><span class="p">(</span><span class="n">X_tr_gray</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">X_va_gray</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>

<span class="c1"># Show model performance during training
</span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history_gray</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_45_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>And what about the confusion matrices?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate Model
</span><span class="n">loss_gray_tr</span><span class="p">,</span> <span class="n">acc_gray_tr</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_tr_gray</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">loss_gray_va</span><span class="p">,</span> <span class="n">acc_gray_va</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_va_gray</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>
<span class="n">loss_gray_te</span><span class="p">,</span> <span class="n">acc_gray_te</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_te_gray</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="c1"># Report classification report and confusion matrix
</span><span class="nf">plot_classification_report</span><span class="p">(</span><span class="n">X_te_gray</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">model_gray</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">Train score: loss = 0.1118 - accuracy = 0.9619
Valid score: loss = 0.2255 - accuracy = 0.9287
Test score:  loss = 0.2407 - accuracy = 0.9220
</code></pre>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_47_1.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="classification-performance-on-realigned-and-stretched-data-set">Classification performance on realigned and stretched data set</h2>

<p>Now let’s do the same for the realigned and stretched images. How does the model performance during training look like?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train model
</span><span class="n">model_stretch</span><span class="p">,</span> <span class="n">history_stretch</span> <span class="o">=</span> <span class="nf">run_resnet</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">X_va_stretch</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>

<span class="c1"># Show model performance during training
</span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history_stretch</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_49_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>And what about the confusion matrices?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate Model
</span><span class="n">loss_stretch_tr</span><span class="p">,</span> <span class="n">acc_stretch_tr</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">loss_stretch_va</span><span class="p">,</span> <span class="n">acc_stretch_va</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_va_stretch</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>
<span class="n">loss_stretch_te</span><span class="p">,</span> <span class="n">acc_stretch_te</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_te_stretch</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="c1"># Report classification report and confusion matrix
</span><span class="nf">plot_classification_report</span><span class="p">(</span><span class="n">X_te_stretch</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">model_stretch</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">Train score: loss = 0.0229 - accuracy = 0.9921
Valid score: loss = 0.1672 - accuracy = 0.9533
Test score:  loss = 0.1975 - accuracy = 0.9491
</code></pre>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_51_1.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="classification-performance-on-pca-transformed-data-set">Classification performance on PCA transformed data set</h2>

<p>Now let’s do the same for the PCA transformed images. How does the model performance during training look like?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Train model
</span><span class="n">model_pca</span><span class="p">,</span> <span class="n">history_pca</span> <span class="o">=</span> <span class="nf">run_resnet</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">X_va_pca</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>

<span class="c1"># Show model performance during training
</span><span class="nf">plot_history</span><span class="p">(</span><span class="n">history_pca</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_53_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>And what about the confusion matrices?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Evaluate Model
</span><span class="n">loss_pca_tr</span><span class="p">,</span> <span class="n">acc_pca_tr</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">)</span>
<span class="n">loss_pca_va</span><span class="p">,</span> <span class="n">acc_pca_va</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_va_pca</span><span class="p">,</span> <span class="n">y_va</span><span class="p">)</span>
<span class="n">loss_pca_te</span><span class="p">,</span> <span class="n">acc_pca_te</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">X_te_pca</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>

<span class="c1"># Report classification report and confusion matrix
</span><span class="nf">plot_classification_report</span><span class="p">(</span><span class="n">X_te_pca</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">model_pca</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">Train score: loss = 0.0289 - accuracy = 0.9918
Valid score: loss = 0.1459 - accuracy = 0.9509
Test score:  loss = 0.1898 - accuracy = 0.9448
</code></pre>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_55_1.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h1 id="model-investigation">Model investigation</h1>

<p>Now that we trained all of these individual models, let’s see how they relate and how they differ. First, let’s compare in what way these different models predict the same values by plotting their individual predictions for the test set next to each others.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute model specific predictions
</span><span class="n">y_pred_orig</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred_gray</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te_gray</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred_stretch</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te_stretch</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_pred_pca</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te_pca</span><span class="p">).</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Aggregate all model predictions
</span><span class="n">target</span> <span class="o">=</span> <span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">y_pred_orig</span><span class="p">,</span> <span class="n">y_pred_gray</span><span class="p">,</span> <span class="n">y_pred_stretch</span><span class="p">,</span> <span class="n">y_pred_pca</span><span class="p">])[</span>
    <span class="p">:,</span> <span class="n">np</span><span class="p">.</span><span class="nf">argsort</span><span class="p">(</span><span class="n">target</span><span class="p">)]</span>

<span class="c1"># Plot model individual predictions
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="sh">"</span><span class="s">auto</span><span class="sh">"</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="sh">"</span><span class="s">nearest</span><span class="sh">"</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">rainbow</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Predictions for all </span><span class="si">{</span><span class="n">predictions</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s"> test samples</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Model</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">(</span><span class="n">ticks</span><span class="o">=</span><span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">Orig</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Gray</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stretched</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PCA</span><span class="sh">"</span><span class="p">]);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_57_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>What we can see is that, except for the original dataset, none of the other models make mistakes in the 8th target class (here in red). So our manipulations seem to have been useful. And amongs the three approaches, the “realigned and stretched” dataset seems perform the best. To support this claim, let’s take a look at the test accuracies of our four models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Collect accuracies
</span><span class="n">accs_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">acc_orig_te</span><span class="p">,</span> <span class="n">acc_gray_te</span><span class="p">,</span> <span class="n">acc_stretch_te</span><span class="p">,</span> <span class="n">acc_pca_te</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>

<span class="c1"># Plot accuracies
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy for our four models</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">([</span><span class="sh">"</span><span class="s">Orig</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Gray</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stretched</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PCA</span><span class="sh">"</span><span class="p">],</span> <span class="n">accs_te</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">hlines</span><span class="p">(</span><span class="n">accs_te</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">3.4</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">"</span><span class="s">dotted</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="mi">98</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_59_0.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h1 id="model-stacking">Model stacking</h1>

<p>Seeing that our 4 models all perform slightly different, let’s try to go one step further and train a “meta” model that uses the predictions of our 4 models as input.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Compute prediction probabilities for all models and data sets
</span><span class="n">y_prob_tr_orig</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_tr</span><span class="p">)</span>
<span class="n">y_prob_tr_gray</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_tr_gray</span><span class="p">)</span>
<span class="n">y_prob_tr_stretch</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_tr_stretch</span><span class="p">)</span>
<span class="n">y_prob_tr_pca</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_tr_pca</span><span class="p">)</span>

<span class="n">y_prob_va_orig</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_va</span><span class="p">)</span>
<span class="n">y_prob_va_gray</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_va_gray</span><span class="p">)</span>
<span class="n">y_prob_va_stretch</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_va_stretch</span><span class="p">)</span>
<span class="n">y_prob_va_pca</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_va_pca</span><span class="p">)</span>

<span class="n">y_prob_te_orig</span> <span class="o">=</span> <span class="n">model_orig</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te</span><span class="p">)</span>
<span class="n">y_prob_te_gray</span> <span class="o">=</span> <span class="n">model_gray</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te_gray</span><span class="p">)</span>
<span class="n">y_prob_te_stretch</span> <span class="o">=</span> <span class="n">model_stretch</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te_stretch</span><span class="p">)</span>
<span class="n">y_prob_te_pca</span> <span class="o">=</span> <span class="n">model_pca</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_te_pca</span><span class="p">)</span>

<span class="c1"># Combine prediction probabilities into meta data sets
</span><span class="n">y_prob_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">y_prob_tr_orig</span><span class="p">,</span> <span class="n">y_prob_tr_gray</span><span class="p">,</span> <span class="n">y_prob_tr_stretch</span><span class="p">,</span> <span class="n">y_prob_tr_pca</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_prob_va</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">y_prob_va_orig</span><span class="p">,</span> <span class="n">y_prob_va_gray</span><span class="p">,</span> <span class="n">y_prob_va_stretch</span><span class="p">,</span> <span class="n">y_prob_va_pca</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y_prob_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">(</span>
    <span class="p">[</span><span class="n">y_prob_te_orig</span><span class="p">,</span> <span class="n">y_prob_te_gray</span><span class="p">,</span> <span class="n">y_prob_te_stretch</span><span class="p">,</span> <span class="n">y_prob_te_pca</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Combine training and validation dataset
</span><span class="n">y_prob_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">y_prob_tr</span><span class="p">,</span> <span class="n">y_prob_va</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">y_va</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="nf">ravel</span><span class="p">()</span>
</code></pre></div></div>

<p>There are many different classification models to choose from, but to keep it short and compact, let’s quickly train a multi-layer perceptron classifier and compare it’s score to the other four models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="c1"># Create MLP classifier
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">MLPClassifier</span><span class="p">(</span>
    <span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="sh">"</span><span class="s">adam</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.42</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="sh">"</span><span class="s">adaptive</span><span class="sh">"</span><span class="p">,</span> <span class="n">learning_rate_init</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">early_stopping</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.15</span><span class="p">)</span>

<span class="c1"># Train model
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">y_prob_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Compute prediction accuracy of meta classifier
</span><span class="n">acc_meta_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">y_prob_te</span><span class="p">)</span> <span class="o">==</span> <span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">())</span>

<span class="c1"># Collect accuracies
</span><span class="n">accs_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">acc_orig_te</span><span class="p">,</span> <span class="n">acc_gray_te</span><span class="p">,</span> <span class="n">acc_stretch_te</span><span class="p">,</span> <span class="n">acc_pca_te</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">accs_meta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">acc_meta_te</span><span class="p">])</span> <span class="o">*</span> <span class="mi">100</span>

<span class="c1"># Plot accuracies
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Test accuracy for all five models</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">([</span><span class="sh">"</span><span class="s">Orig</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Gray</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stretched</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PCA</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Meta</span><span class="sh">"</span><span class="p">],</span> <span class="n">accs_te</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">([</span><span class="sh">"</span><span class="s">Orig</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Gray</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Stretched</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">PCA</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">Meta</span><span class="sh">"</span><span class="p">],</span> <span class="n">accs_meta</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">hlines</span><span class="p">(</span><span class="n">accs_te</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="sh">"</span><span class="s">black</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">"</span><span class="s">dotted</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="mi">98</span><span class="p">);</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/02_color_engineering_medmnist/output_63_0.png" data-zoomable="" width="750px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Awesome! It worked! Training four different models on the original and three color transformed data sets, and then using these prediction probabilities to train a new meta classifier helped us to improve the initial prediction accuracy from 94% up to 96.4%!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[How to improve color encoding of unnatural images]]></summary></entry><entry><title type="html">Watch how AI learns to classify images</title><link href="https://miykael.github.io/blog/2021/AI_classification_under_the_hood/" rel="alternate" type="text/html" title="Watch how AI learns to classify images" /><published>2021-12-27T12:00:00+00:00</published><updated>2021-12-27T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2021/AI_classification_under_the_hood</id><content type="html" xml:base="https://miykael.github.io/blog/2021/AI_classification_under_the_hood/"><![CDATA[<h2 id="looking-under-the-hood-of-machine-learning-routines-while-they-learn-about-things">Looking under the hood of machine learning routines while they learn about things.</h2>

<p><em>[Find the Jupyter Notebook to this article <a href="https://github.com/miykael/miykael.github.io/blob/master/assets/nb/01_ai_classifier_under_hood/nb_ai_classification_under_the_hood.ipynb">here</a>.]</em></p>

<hr />

<p>There exist already many tutorial and demos that show nicely how machine learning routines can learn from images and do wondrous tasks. And so, this article is not about what they can do, nor about the code to get there, but about what’s happening while the machines are learning.</p>

<p>I hope that the animations in this article can show in an intuitive way, how current machine learning routines start from randomness and learn very quickly how to extract meaningful features from data to solve a given task in a efficient and impressive way. So let’s jump right into it!</p>

<h2 id="the-data-set">The data set</h2>

<p>First things first, we need a data set and a problem. Mixing things up slightly, lets use the <a href="https://github.com/zalandoresearch/fashion-mnist">Fashion MNIST</a> dataset and a classification task. In short, the Fashion MNIST contains 70’000 Zalando clothing items, where each item is a 28 x 28 pixel, gray-scaled image, belonging to 10 different clothing categories.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Load and prepare dataset
</span><span class="n">fashion_mnist</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">fashion_mnist</span>
<span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">),</span> <span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span> <span class="o">=</span> <span class="n">fashion_mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">T-shirt/top</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Trouser</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Pullover</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Dress</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Coat</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Sandal</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Shirt</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Sneaker</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Bag</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">Ankle boot</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">labels_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_tr</span><span class="p">])</span>
<span class="n">labels_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_te</span><span class="p">])</span>
</code></pre></div></div>

<p>Before going any further, let’s take a quick look at a few of these images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Plot a few random clothing items
</span><span class="n">size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">y_tr</span><span class="p">)),</span> <span class="n">size</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">labels_tr</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">X_tr</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">binary</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_6_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Looking at these images we can see that some categories look rather homogenious, while others are quite diverse. So let’s take a closer look at these target categories (i.e. classes) and how they might relate to each other.</p>

<p>A simple way to do this is by looking at the class averages and investigating how they correlate to each other.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Look at class averages
</span><span class="n">class_averages</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">class_average</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">X_tr</span><span class="p">[</span><span class="n">y_tr</span> <span class="o">==</span> <span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span>
    <span class="n">class_averages</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">class_average</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">class_average</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">binary</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_8_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>As we assumed, categories like <code class="language-plaintext highlighter-rouge">Trouser</code>, <code class="language-plaintext highlighter-rouge">Pullover</code> and <code class="language-plaintext highlighter-rouge">T-shirt/top</code> are rather heterogenous (i.e. sharp), while <code class="language-plaintext highlighter-rouge">Dress</code>, <code class="language-plaintext highlighter-rouge">Sandal</code> and <code class="language-plaintext highlighter-rouge">Bag</code> are a bit vague.</p>

<p>And how do these classes relate to each other? For this we will just simply investigate how correlated each of these class averages are, and then order this correlation matrix, by using seaborn’s hierarchically-clustered heatmap <code class="language-plaintext highlighter-rouge">sns.clustermap()</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="c1"># Correlate class averages and plot hierarchically-clustered heatmap
</span><span class="n">corr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">corrcoef</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">class_averages</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">df_corr</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">corr</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">sns</span><span class="p">.</span><span class="nf">clustermap</span><span class="p">(</span>
    <span class="mi">100</span> <span class="o">*</span> <span class="n">df_corr</span><span class="p">,</span>
    <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">fmt</span><span class="o">=</span><span class="sh">"</span><span class="s">.0f</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">cm</span><span class="p">.</span><span class="n">ax_row_dendrogram</span><span class="p">.</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">cm</span><span class="p">.</span><span class="n">ax_col_dendrogram</span><span class="p">.</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">cm</span><span class="p">.</span><span class="n">cax</span><span class="p">.</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_10_0.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Great, there seems to be some structure. Given this correlation matrix, we could assume that a machine learning model will have more problem to differentiate <code class="language-plaintext highlighter-rouge">Shirt</code>, <code class="language-plaintext highlighter-rouge">Pullover</code> and <code class="language-plaintext highlighter-rouge">Coat</code> from one another than from other classes. Similar thing for <code class="language-plaintext highlighter-rouge">Sandal</code> and <code class="language-plaintext highlighter-rouge">Sneakers</code>, etc.</p>

<h1 id="unsupervised-learning">Unsupervised Learning</h1>

<p>Now that we know our data set a bit better and have an intuition for when our machine learning models might struggle, we can go ahead and train them. First, let’s start with an unsupervised learning approach, specifically with <a href="https://umap-learn.readthedocs.io/en/latest/">UMAP</a>, i.e. Uniform Manifold Approximation and Projection for Dimension Reduction.</p>

<p>The details about how UMAP works are not important here, but what you should know is the following: This unsupervised learning method tries to find a lower-dimensional manifold on which our high-dimensional data set lies. Once this manifold is known (or estimated), we can try to unfold and flatten it onto a 2-dimensional plane and visualize it in a human-understandable form. In contrast to other similar approaches (e.g. t-SNE), UMAP allows the reduction to more than just two dimensions and also contains a <code class="language-plaintext highlighter-rouge">.transfrom()</code> function, meaning that the mapping can even be reversed for new samples in the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">umap</span>

<span class="c1"># Get samples into a 1D shape
</span><span class="n">data</span> <span class="o">=</span> <span class="n">X_tr</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">28</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># Train UMAP model
</span><span class="n">transformer</span> <span class="o">=</span> <span class="n">umap</span><span class="p">.</span><span class="nc">UMAP</span><span class="p">(</span>
    <span class="n">n_neighbors</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
    <span class="n">random_state</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span>
    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span>  <span class="c1"># Lower than usual to slow down model improvement for visualization
</span>    <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>  <span class="c1"># Chosen to lead to a nice final spread of point cloud
</span>    <span class="n">n_epochs</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>  <span class="c1"># Keeping it sequential to extract epochs one after the other
</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p>As many such machine learning models, the model is initiated with more or less random parameters and then slowly optimizes to an impressive end result. To fully appreciate how well this works, it is important to understand that this routine doesn’t know about the target classes, and as such only works with similarities in the high-dimensional space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.patheffects</span> <span class="k">as</span> <span class="n">PathEffects</span>


<span class="k">def</span> <span class="nf">plot_umap</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">aspect</span><span class="o">=</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">embedding</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">Spectral</span><span class="sh">"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">title_txt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Original data</span><span class="sh">"</span>
    <span class="k">elif</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">121</span><span class="p">:</span>
        <span class="n">title_txt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Transformed Data</span><span class="sh">"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">title_txt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Iteration Step: %03d</span><span class="sh">"</span> <span class="o">%</span> <span class="n">step</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="n">title_txt</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">28</span><span class="p">)</span>

    <span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">colorbar</span><span class="p">(</span><span class="n">boundaries</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">cbar</span><span class="p">.</span><span class="nf">set_ticks</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
    <span class="n">cbar</span><span class="p">.</span><span class="nf">set_ticklabels</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># We add the labels for each class
</span>    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">labels</span><span class="p">):</span>
        <span class="c1"># Position of each label
</span>        <span class="n">xtext</span><span class="p">,</span> <span class="n">ytext</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">median</span><span class="p">(</span><span class="n">embedding</span><span class="p">[</span><span class="n">y_tr</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">txt</span> <span class="o">=</span> <span class="n">ax</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">xtext</span><span class="p">,</span> <span class="n">ytext</span><span class="p">,</span> <span class="nf">str</span><span class="p">(</span><span class="n">t</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">)</span>
        <span class="n">txt</span><span class="p">.</span><span class="nf">set_path_effects</span><span class="p">(</span>
            <span class="p">[</span><span class="n">PathEffects</span><span class="p">.</span><span class="nc">Stroke</span><span class="p">(</span><span class="n">linewidth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">foreground</span><span class="o">=</span><span class="sh">"</span><span class="s">w</span><span class="sh">"</span><span class="p">),</span> <span class="n">PathEffects</span><span class="p">.</span><span class="nc">Normal</span><span class="p">()]</span>
        <span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Optimize UMAP 128 times and store each step in an image
</span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="nf">plot_umap</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="sh">"</span><span class="s">emb_%04d.npz</span><span class="sh">"</span> <span class="o">%</span> <span class="n">n</span><span class="p">)[</span><span class="sh">"</span><span class="s">embedding</span><span class="sh">"</span><span class="p">],</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">fig_%04d.jpeg</span><span class="sh">"</span> <span class="o">%</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">clf</span><span class="p">()</span>

<span class="c1"># Remove first figure as it is confusing
</span><span class="err">!</span><span class="n">rm</span> <span class="n">fig_0000</span><span class="p">.</span><span class="n">jpeg</span>

<span class="c1"># Duplicate first and last figure to have a "resting phase" impression
</span><span class="err">!</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">{</span><span class="mf">1.</span><span class="p">.</span><span class="mi">12</span><span class="p">};</span> <span class="n">do</span> <span class="n">cp</span> <span class="n">fig_0001</span><span class="p">.</span><span class="n">jpeg</span> <span class="n">fig_0001_</span><span class="err">$</span><span class="n">i</span><span class="p">.</span><span class="n">jpeg</span><span class="p">;</span> <span class="n">done</span>
<span class="err">!</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">{</span><span class="mf">1.</span><span class="p">.</span><span class="mi">12</span><span class="p">};</span> <span class="n">do</span> <span class="n">cp</span> <span class="n">fig_0127</span><span class="p">.</span><span class="n">jpeg</span> <span class="n">fig_0127_</span><span class="err">$</span><span class="n">i</span><span class="p">.</span><span class="n">jpeg</span><span class="p">;</span> <span class="n">done</span>

<span class="c1"># Transform jpgs into video
</span><span class="err">!</span><span class="n">ffmpeg</span> <span class="o">-</span><span class="n">y</span> <span class="o">-</span><span class="n">framerate</span> <span class="mi">12</span> <span class="o">-</span><span class="n">pattern_type</span> <span class="n">glob</span> <span class="o">-</span><span class="n">i</span> <span class="sh">'</span><span class="s">fig*.jpeg</span><span class="sh">'</span> <span class="n">umap_animation</span><span class="p">.</span><span class="n">gif</span>

<span class="c1"># Remove temporary files
</span><span class="err">!</span><span class="n">rm</span> <span class="n">fig_</span><span class="o">*</span><span class="p">.</span><span class="n">jpeg</span> <span class="n">emb_</span><span class="o">*</span><span class="p">.</span><span class="n">npz</span>

<span class="c1"># Visualize gif
</span><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="nc">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="sh">"</span><span class="s">umap_animation.gif</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/umap_animation.gif" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Nice, isn’t it? What is especially nice to see is that the UMAP projection supports our assumption from the hierarchically-clustered heatmap from before. Or in other words, target classes that are highly correlated with each other also seem to overlap on the lower-dimensional manifold.</p>

<div class="alert alert-success">
  <strong>Note:</strong> To create this animation, you need to slightly adapt UMAP's source code (<a href="https://github.com/lmcinnes/umap/blob/c060b65c208f3fdfd8abf1296adb4d9882cdac4e/umap/layouts.py#L396">here</a>) and save each <code>head_embedding</code> to a new file (e.g. <code>np.savez(filename, embedding=head_embedding)</code>.
</div>

<h1 id="supervised-learning">Supervised Learning</h1>

<p>Let’s now explore what we can do if we chose a supervised learning approach and provide the machine learning model with the target class labels. While this would also be possible with a UMAP approach, let’s switch gears a bit and use a <strong>convolutional neural network</strong>, or short CNN, instead. A CNN is a particular kind of neural network that is specialized for analyzing data with spatial structure (e.g. images or time-series data).</p>

<p>In short, what a CNN tries to do is the following: Find meaningful patterns in an image that help to differentiate target classes. For example, does one class of images have more straight lines than others, or more “wavy bits”? Do some images have more dots or wrinkles, do they have handles or shoe laces, etc. The CNN tries to find specific image patterns that are unique to some classes. Once it knows what kind of patterns can be found, it can use this information to predict if an image looks more than one class or another.</p>

<h2 id="another-data-set">Another data set</h2>

<p>To better illustrate the wonder of such CNNs, let’s also go to a bit more naturalistic dataset: The <a href="https://en.wikipedia.org/wiki/CIFAR-10">CIFAR-10</a>. Similar to the Fashion-MNIST dataset, this data set contains 70’000 color images, with a pixel resolution of 32 x 32, of 10 different target classes: <code class="language-plaintext highlighter-rouge">airplane</code>, <code class="language-plaintext highlighter-rouge">automobile</code>, <code class="language-plaintext highlighter-rouge">bird</code>, <code class="language-plaintext highlighter-rouge">cat</code>, <code class="language-plaintext highlighter-rouge">deer</code>, <code class="language-plaintext highlighter-rouge">dog</code>, <code class="language-plaintext highlighter-rouge">frog</code>, <code class="language-plaintext highlighter-rouge">horse</code>, <code class="language-plaintext highlighter-rouge">ship</code> and <code class="language-plaintext highlighter-rouge">truck</code>. Let’s have a quick look at this dataset:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="c1"># Load and prepare dataset
</span><span class="p">(</span><span class="n">X_tr</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">),</span> <span class="p">(</span><span class="n">X_te</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">cifar10</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">y_tr</span> <span class="o">=</span> <span class="n">y_tr</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>
<span class="n">y_te</span> <span class="o">=</span> <span class="n">y_te</span><span class="p">.</span><span class="nf">ravel</span><span class="p">()</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span>
    <span class="sh">"</span><span class="s">airplane</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">automobile</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">bird</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">cat</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">deer</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">dog</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">frog</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">horse</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">ship</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">truck</span><span class="sh">"</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">labels_tr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_tr</span><span class="p">])</span>
<span class="n">labels_te</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">labels</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">y_te</span><span class="p">])</span>

<span class="c1"># Prepare dataset for CNN
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">X_tr</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">X_te</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">"</span><span class="s">float32</span><span class="sh">"</span><span class="p">)</span> <span class="o">/</span> <span class="mf">127.5</span> <span class="o">-</span> <span class="mi">1</span>

<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Split train and validation set
</span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_valid</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_va</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_tr</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,)</span>

<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Plot a few random clothing items
</span><span class="n">size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">y_tr</span><span class="p">)),</span> <span class="n">size</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="n">labels_tr</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">X_tr</span><span class="p">[</span><span class="n">idx</span><span class="p">][</span><span class="n">i</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">binary</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_aspect</span><span class="p">(</span><span class="sh">"</span><span class="s">equal</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_23_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<h2 id="model-creation">Model creation</h2>

<p>Next, let’s create a simple convolutional neural network, using TensorFlow:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>

<span class="c1"># Create Conv Net
</span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="c1"># Entry block
</span><span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nc">TruncatedNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Activation</span><span class="p">(</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">same</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nc">TruncatedNormal</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.01</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">kernel_initializer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">initializers</span><span class="p">.</span><span class="nc">VarianceScaling</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">2</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">BatchNormalization</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">softmax</span><span class="sh">"</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">),</span>
    <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<pre><code class="language-code">Model: "model_1"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_1  (InputLayer)          [(None, 32, 32, 3)]    0         
 conv2d_2  (Conv2D)              (None, 32, 32, 64)    4864      
 activation_3 (Activation)       (None, 32, 32, 64)    0         
 max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 64)    0         
 conv2d_5 (Conv2D)               (None, 16, 16, 64)    36928     
 max_pooling2d_6 (MaxPooling2D)  (None, 8, 8, 64)      0         
 flatten_7 (Flatten)             (None, 4096)          0         
 dropout_8 (Dropout)             (None, 4096)          0         
 dense_9 (Dense)                 (None, 256)           1048832   
 batch_normalization_10 (Batch   (None, 256)           1024      
 Normalization)                                                  
 dense_11 (Dense)                (None, 10)            2570      
=================================================================
Total params: 1,094,218
Trainable params: 1,093,706
Non-trainable params: 512
_________________________________________________________________
</code></pre>

<p>This is certainly not the most efficient, nor the best performing network, but it allows us to observe the inner workings of a neural network. But before we go ahead and train the network, let’s take a look at the ‘raw’ and untrained version of it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">plot_first_layer_kernels</span><span class="p">(</span><span class="n">kernels</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>

    <span class="c1"># Create a grid of subplots
</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

    <span class="c1"># Remove gaps between suplots
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Plot the 64 kernels from the first convolutional layer
</span>    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
        <span class="c1"># Get i-th kernel (shape: 5x5x3)
</span>        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernels</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span>

        <span class="c1"># Rescale values between 0 and 1
</span>        <span class="n">kernel</span> <span class="o">-=</span> <span class="n">kernel</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span>  <span class="c1"># Rescale between 0 and max
</span>        <span class="n">kernel</span> <span class="o">/=</span> <span class="n">kernel</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>  <span class="c1"># Rescale between 0 and 1
</span>
        <span class="c1"># Plot kernel with imshow()
</span>        <span class="n">axis</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">axis</span><span class="p">.</span><span class="nf">get_xaxis</span><span class="p">().</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># disable x-axis
</span>        <span class="n">axis</span><span class="p">.</span><span class="nf">get_yaxis</span><span class="p">().</span><span class="nf">set_visible</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>  <span class="c1"># disable y-axis
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="nf">plot_first_layer_kernels</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_31_0.png" data-zoomable="" width="400px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>This is a picture of the first 16 convolutional kernels (or filters). In other words, what the CNN sees (or looks for) in the very first input layer.</p>

<p>Right now, all of the convolutional neural network’s parameter are randomly set. In other words, the ConvNet doesn’t know what to look for, its ‘eyes’ are still untrained. And so, if we give this untrained neural network an image to classify, it performs very poorly, and doesn’t really know which of the 10 classes to pick from.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">predict_single_image</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

    <span class="c1"># Compute predictions
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

    <span class="c1"># Prepare image
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">x_test</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">img</span> <span class="o">-=</span> <span class="n">img</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span>
    <span class="n">img</span> <span class="o">/=</span> <span class="n">img</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>

    <span class="c1"># Get class probabilities
</span>    <span class="n">probability</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="mi">100</span>

    <span class="c1"># Get class names
</span>    <span class="n">class_names</span> <span class="o">=</span> <span class="n">labels</span>

    <span class="c1"># Get predicted and true label of image
</span>    <span class="n">predicted_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span>
    <span class="n">predicted_prob</span> <span class="o">=</span> <span class="n">probability</span><span class="p">[</span><span class="n">predicted_idx</span><span class="p">]</span>
    <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">class_names</span><span class="p">[</span><span class="n">predicted_idx</span><span class="p">]</span>

    <span class="kn">import</span> <span class="n">matplotlib.gridspec</span> <span class="k">as</span> <span class="n">gridspec</span>

    <span class="c1"># Plot overview figure
</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">13</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="nc">GridSpec</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">width_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Plot image
</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Image</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">binary</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>

    <span class="c1"># Add information text to image
</span>    <span class="n">info_txt</span> <span class="o">=</span> <span class="sh">"</span><span class="se">\n</span><span class="s">This is to {:.02f}% a {}!</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">predicted_prob</span><span class="p">,</span> <span class="n">predicted_label</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="n">info_txt</span><span class="p">,</span> <span class="n">fontdict</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">size</span><span class="sh">"</span><span class="p">:</span> <span class="mi">21</span><span class="p">})</span>

    <span class="c1"># Plot prediction probabilities
</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Prediction Probability</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probability</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">probability</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">#BFBFBF</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Set y-label text
</span>    <span class="n">y_label_text</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">{}: {:5.2f}%</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">probability</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">e</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">class_names</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_yticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_yticklabels</span><span class="p">(</span><span class="n">y_label_text</span><span class="p">)</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">vlines</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="o">*</span><span class="n">ylim</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">(</span><span class="n">ylim</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">predict_single_image</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_34_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>The illustration above shows with which class probability this untrained CNN would predict the dog shown in the image.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">matplotlib.gridspec</span> <span class="k">as</span> <span class="n">gridspec</span>

<span class="k">def</span> <span class="nf">predict_multiple_images</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">nimg</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">title_idx</span><span class="o">=</span><span class="bp">None</span>
<span class="p">):</span>

    <span class="c1"># Set random seed
</span>    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

    <span class="c1"># Compute predictions
</span>    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

    <span class="kn">from</span> <span class="n">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>

    <span class="n">imgs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">img_ids</span> <span class="o">=</span> <span class="nf">shuffle</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">x_test</span><span class="p">)))</span>

    <span class="c1"># Plot first N image prediction information
</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">nimg</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="nc">GridSpec</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">nimg</span><span class="p">,</span> <span class="n">height_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">i_pos</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">img_ids</span><span class="p">[:</span><span class="n">nimg</span><span class="p">]):</span>

        <span class="c1"># Get image
</span>        <span class="n">img</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="c1"># Get probability
</span>        <span class="n">probability</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="c1"># Get predicted and true label of image
</span>        <span class="n">predicted_label</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span>
        <span class="n">true_label</span> <span class="o">=</span> <span class="n">y_te</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

        <span class="c1"># Get class names
</span>        <span class="n">class_names</span> <span class="o">=</span> <span class="n">labels</span>

        <span class="c1"># Identify the text color
</span>        <span class="k">if</span> <span class="n">predicted_label</span> <span class="o">==</span> <span class="n">true_label</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">#004CFF</span><span class="sh">"</span>
            <span class="n">info_txt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">{} {:2.1f}%</span><span class="se">\n</span><span class="s">Correct!</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                <span class="n">class_names</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">],</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">probability</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">#F50000</span><span class="sh">"</span>
            <span class="n">info_txt</span> <span class="o">=</span> <span class="sh">"</span><span class="s">{} {:2.1f}%</span><span class="se">\n</span><span class="s">Actually: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span>
                <span class="n">class_names</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">],</span>
                <span class="mi">100</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">probability</span><span class="p">),</span>
                <span class="n">class_names</span><span class="p">[</span><span class="n">true_label</span><span class="p">],</span>
            <span class="p">)</span>

        <span class="c1"># Plot prediction probabilities
</span>        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i_pos</span><span class="p">])</span>
        <span class="n">pred_plot</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">bar</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">probability</span><span class="p">)),</span> <span class="n">probability</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sh">"</span><span class="s">#BFBFBF</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">pred_plot</span><span class="p">[</span><span class="n">predicted_label</span><span class="p">].</span><span class="nf">set_color</span><span class="p">(</span><span class="sh">"</span><span class="s">#FF4747</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">pred_plot</span><span class="p">[</span><span class="n">true_label</span><span class="p">].</span><span class="nf">set_color</span><span class="p">(</span><span class="sh">"</span><span class="s">#477EFF</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">xlim</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">())</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">hlines</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">probability</span><span class="p">),</span> <span class="o">*</span><span class="n">xlim</span><span class="p">,</span> <span class="n">linestyles</span><span class="o">=</span><span class="sh">"</span><span class="s">:</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">xlim</span><span class="p">(</span><span class="n">xlim</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Class Probability</span><span class="sh">"</span><span class="p">)</span>

        <span class="c1"># Plot image
</span>        <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">i_pos</span><span class="p">])</span>
        <span class="n">img</span> <span class="o">-=</span> <span class="n">img</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span>
        <span class="n">img</span> <span class="o">/=</span> <span class="n">img</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">binary</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">grid</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">xticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="p">.</span><span class="nf">yticks</span><span class="p">([])</span>

        <span class="c1"># Add information text to image
</span>        <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="n">info_txt</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">title_idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">pred_%03d_%04d.jpg</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">seed</span><span class="p">,</span> <span class="n">title_idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">clf</span><span class="p">()</span>

<span class="nf">predict_multiple_images</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">nimg</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">title_idx</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_35_0.jpg" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>This is the same plot as with the dog above, but this time for six different images. The blue bar shows the probability of the class it should have predicted and the red one, the class probability of the wrongly predicted class. For now, all of these probabilities are at 10%, i.e. chance level.</p>

<h2 id="model-training">Model training</h2>

<p>Let’s now go ahead and train this model. There’s an endless depth to explaining how this actually works, but in simple terms, the model looks at a few images (i.e., a batch), tries to predict the target classes of these images, looks at how wrong it was and corrects its internal parameters accordingly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create some helper lists for results storage
</span><span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">kernels</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">activations</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Specify how many epochs to train for
</span><span class="n">N</span> <span class="o">=</span> <span class="mi">48</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
        <span class="n">x_train</span><span class="p">,</span>
        <span class="n">y_tr</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="mi">6</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_valid</span><span class="p">,</span> <span class="n">y_va</span><span class="p">),</span>
        <span class="n">steps_per_epoch</span><span class="o">=</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="mi">5</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Store kernels in array
</span>    <span class="n">kernels</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># Store activation maps in array
</span>    <span class="n">img</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">[</span><span class="mi">16</span><span class="p">]</span>
    <span class="n">model_conv</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Model</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">output</span><span class="p">)</span>
    <span class="n">activations</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">model_conv</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">img</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="p">...]))</span>

    <span class="c1"># Stores scores in array
</span>    <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>

    <span class="c1"># Save predictions to image
</span>    <span class="k">for</span> <span class="n">sidx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">12</span><span class="p">):</span>
        <span class="nf">predict_multiple_images</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">nimg</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">sidx</span><span class="p">,</span> <span class="n">title_idx</span><span class="o">=</span><span class="n">i</span>
        <span class="p">)</span>

<span class="c1"># Save relevant outputs in numpy and pandas frameworks
</span><span class="n">kernels</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">kernels</span><span class="p">)</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">]).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot_first_layer_view</span><span class="p">(</span><span class="n">activation_maps</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Helper function to plot kernels of first conv layer</span><span class="sh">"""</span>

    <span class="c1"># Create a grid of subplots
</span>    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

    <span class="c1"># Remove gaps between suplots
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hspace</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Plot the activation maps of the 1st conv. layer for the sample image
</span>    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">axis</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axes</span><span class="p">.</span><span class="nf">flatten</span><span class="p">()):</span>
        <span class="c1"># Get activation map of the i-th filter
</span>        <span class="n">activation</span> <span class="o">=</span> <span class="n">activation_maps</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">j</span><span class="p">]</span>

        <span class="c1"># Rescale values between 0 and 1
</span>        <span class="n">activation</span> <span class="o">-=</span> <span class="n">activation</span><span class="p">.</span><span class="nf">min</span><span class="p">()</span>  <span class="c1"># Rescale between 0 and max
</span>        <span class="n">activation</span> <span class="o">/=</span> <span class="n">activation</span><span class="p">.</span><span class="nf">max</span><span class="p">()</span>  <span class="c1"># Rescale between 0 and 1
</span>
        <span class="c1"># Plot it with imshow()
</span>        <span class="n">axis</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">activation</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">"</span><span class="s">gray</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">axis</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>

<span class="c1"># Loop through results and create informative plots
</span><span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)):</span>

    <span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

    <span class="c1"># Plot first layer kernels
</span>    <span class="n">fig_kernel</span> <span class="o">=</span> <span class="nf">plot_first_layer_kernels</span><span class="p">(</span><span class="n">kernels</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">fig_kernel</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">temp.jpg</span><span class="sh">"</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">"</span><span class="s">tight</span><span class="sh">"</span><span class="p">,</span> <span class="n">pad_inches</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="sh">"</span><span class="s">temp.jpg</span><span class="sh">"</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Kernel</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Plot first layer kernels
</span>    <span class="n">fig_activation</span> <span class="o">=</span> <span class="nf">plot_first_layer_view</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">fig_activation</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">temp.jpg</span><span class="sh">"</span><span class="p">,</span> <span class="n">bbox_inches</span><span class="o">=</span><span class="sh">"</span><span class="s">tight</span><span class="sh">"</span><span class="p">,</span> <span class="n">pad_inches</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">imshow</span><span class="p">(</span><span class="n">plt</span><span class="p">.</span><span class="nf">imread</span><span class="p">(</span><span class="sh">"</span><span class="s">temp.jpg</span><span class="sh">"</span><span class="p">))</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">axis</span><span class="p">(</span><span class="sh">"</span><span class="s">off</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">View</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># Plot validation curves
</span>    <span class="n">scores</span><span class="p">[[</span><span class="sh">"</span><span class="s">accuracy</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">val_accuracy</span><span class="sh">"</span><span class="p">]].</span><span class="n">iloc</span><span class="p">[:</span> <span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:].</span><span class="nf">plot</span><span class="p">(</span>
        <span class="n">xlabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Epochs</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">ylabel</span><span class="o">=</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.8</span><span class="p">),</span>
        <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
    <span class="p">)</span>
    <span class="n">axes</span><span class="p">[</span><span class="mi">2</span><span class="p">].</span><span class="nf">set_title</span><span class="p">(</span><span class="sh">"</span><span class="s">Learning Curve</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">acc</span> <span class="o">=</span> <span class="n">scores</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span> <span class="sh">"</span><span class="s">val_accuracy</span><span class="sh">"</span><span class="p">]</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Epoch: </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s"> | Accuracy </span><span class="si">{</span><span class="n">acc</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">savefig</span><span class="p">(</span><span class="sh">"</span><span class="s">conv_%04d.jpg</span><span class="sh">"</span> <span class="o">%</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">fig</span><span class="p">.</span><span class="nf">clf</span><span class="p">()</span>

<span class="c1"># Duplicate first and last figure to have a "resting phase" impression
</span><span class="err">!</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">{</span><span class="mf">1.</span><span class="p">.</span><span class="mi">4</span><span class="p">};</span> <span class="n">do</span> <span class="n">cp</span> <span class="n">conv_0001</span><span class="p">.</span><span class="n">jpg</span> <span class="n">conv_0001_</span><span class="err">$</span><span class="n">i</span><span class="p">.</span><span class="n">jpg</span><span class="p">;</span> <span class="n">done</span>
<span class="err">!</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">{</span><span class="mf">1.</span><span class="p">.</span><span class="mi">4</span><span class="p">};</span> <span class="n">do</span> <span class="n">cp</span> <span class="n">conv_0048</span><span class="p">.</span><span class="n">jpg</span> <span class="n">conv_0048_</span><span class="err">$</span><span class="n">i</span><span class="p">.</span><span class="n">jpg</span><span class="p">;</span> <span class="n">done</span>

<span class="c1"># Concatenate images into a gif
</span><span class="err">!</span><span class="n">ffmpeg</span> <span class="o">-</span><span class="n">y</span> <span class="o">-</span><span class="n">framerate</span> <span class="mi">4</span> <span class="o">-</span><span class="n">pattern_type</span> <span class="n">glob</span> <span class="o">-</span><span class="n">i</span> <span class="sh">'</span><span class="s">conv*.jpg</span><span class="sh">'</span> <span class="n">cnn_training</span><span class="p">.</span><span class="n">gif</span>

<span class="c1"># Show gif
</span><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="nc">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="sh">"</span><span class="s">cnn_training.gif</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/cnn_training.gif" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>What we can see here is the learning of a convolutional neural network model, in action. On the <strong>left</strong>, we can see the different convolutions (also called kernels), the neural network is learning. These are the particular features the network tries to find (e.g. horizontal lines, color shifts from red to green, circular patterns, etc.). In the <strong>middle</strong> we can see how each of these convolutions sees the puppy image from before, and on the <strong>right</strong> we can see how the overall accuracy of the model (on the training and validation set) slowly increases through the 48 epochs we trained the network for.</p>

<p>To verify that the model improved its prediction capability, let’s take another look at the puppy classification from before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">predict_single_image</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">idx</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/output_46_0.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>

<p>Much better! Actually, when we compute the overall accuracy this model would reach on a never before seen test set of 10’000 images, it reaches an average prediction accuracy of 65.89%.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_te</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy on test set: %f</span><span class="sh">"</span> <span class="o">%</span> <span class="n">acc</span><span class="p">)</span>
</code></pre></div></div>

<pre><code class="language-code">313/313 [==========] - 3s 11ms/step - loss: 1.1656 - accuracy: 0.6589
Accuracy on test set: 0.658900
</code></pre>

<p>Last but not least, and for fun, let’s select a few images from this test set and see how the individual prediction probabilities become more and more opinionated during training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Duplicate first and last figure to have a "resting phase" impression
</span><span class="err">!</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">{</span><span class="mf">1.</span><span class="p">.</span><span class="mi">4</span><span class="p">};</span> <span class="n">do</span> <span class="n">cp</span> <span class="n">pred_000_0000</span><span class="p">.</span><span class="n">jpg</span> <span class="n">pred_000_0000_</span><span class="err">$</span><span class="n">i</span><span class="p">.</span><span class="n">jpg</span><span class="p">;</span> <span class="n">done</span>
<span class="err">!</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="p">{</span><span class="mf">1.</span><span class="p">.</span><span class="mi">4</span><span class="p">};</span> <span class="n">do</span> <span class="n">cp</span> <span class="n">pred_000_0048</span><span class="p">.</span><span class="n">jpg</span> <span class="n">pred_000_0048_</span><span class="err">$</span><span class="n">i</span><span class="p">.</span><span class="n">jpg</span><span class="p">;</span> <span class="n">done</span>

<span class="c1"># Concatenate images into a gif
</span><span class="err">!</span><span class="n">ffmpeg</span> <span class="o">-</span><span class="n">y</span> <span class="o">-</span><span class="n">framerate</span> <span class="mi">4</span> <span class="o">-</span><span class="n">pattern_type</span> <span class="n">glob</span> <span class="o">-</span><span class="n">i</span> <span class="sh">'</span><span class="s">pred_000_*.jpg</span><span class="sh">'</span> <span class="n">cnn_predictions</span><span class="p">.</span><span class="n">gif</span>

<span class="c1"># Removes temporary files
</span><span class="err">!</span><span class="n">rm</span> <span class="n">conv_0</span><span class="o">*</span><span class="p">.</span><span class="n">jpg</span> <span class="n">pred_0</span><span class="o">*</span><span class="p">.</span><span class="n">jpg</span> <span class="n">temp</span><span class="p">.</span><span class="n">jpg</span>

<span class="c1"># Show gif
</span><span class="kn">from</span> <span class="n">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="nc">Image</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="sh">"</span><span class="s">cnn_predictions.gif</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/nb/01_ai_classifier_under_hood/cnn_predictions.gif" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px" /></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Looking under the hood of machine learning routines while they learn about things.]]></summary></entry><entry><title type="html">Noah 7777</title><link href="https://miykael.github.io/blog/2021/noah_7777/" rel="alternate" type="text/html" title="Noah 7777" /><published>2021-10-25T12:00:00+00:00</published><updated>2021-10-25T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2021/noah_7777</id><content type="html" xml:base="https://miykael.github.io/blog/2021/noah_7777/"><![CDATA[<h2 id="everyday">Everyday</h2>

<p><a href="http://www.noahkalina.com/">Noah Kalina</a> takes a photo of himself, every day, since 2001. And roughly every decade, he uploads the full collection of his daily selfies in a video on YouTube. The result is a fascinating rendition of him aging, accompanied with some great music. Given the nature of this project, it fittingly carries the name <strong>Everyday</strong>!</p>

<iframe width="480" height="360" src="https://www.youtube.com/embed/wAIZ36GI4p8" frameborder="0" allowfullscreen=""></iframe>

<p><br /></p>

<p>I first came across Noah’s work in 2007 when he published his first video, covering 6.5 years of his life. And so, when I saw his work come up again, half of my lifetime later, I felt the urge to use my programming skills and create something with it. Which resulted in the following tweet:</p>

<p><br /></p>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">See how <a href="https://twitter.com/noahkalina?ref_src=twsrc%5Etfw">@noahkalina</a> gets 20 years older in 20 seconds! No GAN, CGI or photoshop used. Only good old data wrangling!<br /><br />And if you want to see what I (or yourself) would look like as &quot;Noah&quot;, or see an infinite mosaic of Noah pictures, check out this notebook <a href="https://t.co/F3t81kYU18">https://t.co/F3t81kYU18</a> <a href="https://t.co/bFPdr9iv5Y">https://t.co/bFPdr9iv5Y</a></p>&mdash; Michael Notter (@miyka_el) <a href="https://twitter.com/miyka_el/status/1380061075425652737?ref_src=twsrc%5Etfw">April 8, 2021</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><br /></p>

<h2 id="7777-days">7’777 days</h2>

<p>To my delight and surprise, this tweet got Noah’s attention. He reached out to me and was curious to see what I could do with the original raw data.</p>

<p>Not letting such a chance pass by, and excited to be part of his journey, I used face recognition AI, and simple image manipulation approaches to realign his images to a perfect stare into the camera. And the end result became the following video, covering now 7777 days of Noah’s life - where every second represents Noah aging 2-months!</p>

<iframe width="480" height="360" src="https://www.youtube.com/embed/DC1KHAxE7mo" frameborder="0" allowfullscreen=""></iframe>

<p><br /></p>

<h2 id="exploration">Exploration</h2>

<p>Going a bit further, I used Noah’s 21 years of selfies and created a collage of 21 images, where each image represents one year of his life.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/blog_noah_7777_yearly.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #000" /></p>

<p>And using a variational autoencoder (VAE) model, i.e. an AI that can decompose and recreate visual images, I created something called the Noah multiverse.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/blog_noah_7777_multiverse.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #000" /></p>

<p><br /></p>

<h2 id="background">Background</h2>

<p>If you’re interested how all of this is done, check out my github repository <a href="https://github.com/miykael/noah_ages">here</a> and if you want to know a bit more about how this all came to be, read <a href="https://mailchi.mp/noahkalina/newsletter94">Noah’s newsletter</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The story between the art collaboration.]]></summary></entry><entry><title type="html">How to use fMRI multi-slice acquisition correctly</title><link href="https://miykael.github.io/blog/2021/mri_multislice_time/" rel="alternate" type="text/html" title="How to use fMRI multi-slice acquisition correctly" /><published>2021-06-23T12:00:00+00:00</published><updated>2021-06-23T12:00:00+00:00</updated><id>https://miykael.github.io/blog/2021/mri_multislice_time</id><content type="html" xml:base="https://miykael.github.io/blog/2021/mri_multislice_time/"><![CDATA[<h2 id="fmri-multi-slice-acquisition">fMRI multi-slice acquisition</h2>

<p>Up until a few years back, functional MRI images of a human’s brain had to be recorded volume by volume, slice by slice. Meaning, that for each time-point, i.e. each sampling point, in an fMRI dataset, the MRI scanner had to first record ~40 consecutive slices (covering the full brain), one after the other. So, if one slice takes 50m to record, that means one full brain volume will take 2000ms to record. Or in other words, the fMRI signal was recorded with a sampling rate of 2 seconds.</p>

<p>While this technological achievement provided already enough detail to establish many great neuroimaging studies, it nonetheless was one of the reasons why fMRI data was considered a ‘rather slow’ neuroimaging method. Luckily, new advancements in the field, allowed the acceleration of the data acquisition by establish a method where multiple slices can be acquired in parallel. So, if we acquired 2 slices in parallel, the acceleration factor became 2, and if it’s 4 slices, the acceleration factor is 4. This acceleration factor also directly effects the sampling rate, which means that a temporal resolution of 1 second (using acceleration of 2) or 500ms (using acceleration of 4) became possible.</p>

<p>For a visual comparison between different approaches, see the following gif (note, top left is the acquisition without any acceleration).</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/blog_slice_time0.gif" data-zoomable="" width="400px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #000" /></p>

<p>I’ve created this nice-looking gif for my PhD defense, but thought that others might also be interested in seeing the striking difference in temporal resolution as well. I therefore shared my gif via twitter, and to my delight, the tweet was met with lots of shared excitement and fascination:</p>

<p><br /></p>

<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="en" dir="ltr">Ever wondered what the difference between single- and multi-slice acquisition would look like? Here&#39;s a visualization of a 🧠 volume with an acceleration factor of 1, 2, 3 or 4, leading to a TR of 2s, 1s, 0.66s and 0.5s. <a href="https://t.co/cLMHykZIMi">pic.twitter.com/cLMHykZIMi</a></p>&mdash; Michael Notter (@miyka_el) <a href="https://twitter.com/miyka_el/status/1407703118691942401?ref_src=twsrc%5Etfw">June 23, 2021</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p><br /></p>

<h2 id="the-drawback">The drawback</h2>

<p>The usage of such multi-slice approaches obviously also comes with drawbacks. If the acceleration factor is too high, the data quality (or signal-to-noise ratio) decreases too much. But if it’s done just right, sub-second recording of fMRI becomes possible.</p>

<p>However, this is not the only drawback with regards to improved sampling rate. For my PhD thesis, I recorded a huge 17 subject big fMRI dataset with six 5min long functional recordings, at a temporal resolution of 600ms. And while preprocessing my data as usual, I suddenly stumbled over something rather unusual.</p>

<p>The following three figures on the left show the recorded average signal activation throughout the brain (i.e. in the ‘total volume’ = TV), plus two of the six estimated motion parameters (rotation around x-axis = Rotation01; translation in z-direction = Translation03). And on the right, you can see the corresponding frequency power spectrum.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/blog_slice_time1.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #fff" /></p>

<p>What is striking is that all of these three methods have a particular oscillation in the higher frequency of ~0.3 Hz. Which perfectly corresponds to the respiratory frequency (i.e. breathing) that people usually have in the fMRI scanner.</p>

<p>In other words, due to improved sampling rate through multi-slice acquisition, fMRI recording now was capable of recording nuance components such as respiratory or cardiac (at 1 to 1.7 Hz) signal (for more see <a href="https://www.sciencedirect.com/science/article/pii/S1053811918300132">Viessman et al., 2018</a>).</p>

<h2 id="the-problem">The problem</h2>

<p>While this might be nice to have, problems actually start arising when we preprocess such “temporal high-res” data as we normally would (see <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/hbm.24528">Lindquist et al., 2019</a>). If during the preprocessing we apply standard motion correction and potential temporal filtering with a low-pass filter, we might reintroduce previously cleaned noise components.</p>

<p>To counter this issue, I developed a method that allows the orthogonal cleaning of such fMRI data with the effect of properly removing respiratory (and cardiac) artefacts. So let’s take another look at the three signal curves we had from before after we preprocessed the data with my new approach.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/blog_slice_time2.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #fff" /></p>

<p>The colored signal shows the cleaned signal, while the gray signal was the one from before the cleaning. As you can see, the noise component was nicely removed.</p>

<h2 id="the-good">The good</h2>

<p>While this might be a nice thing to do, the actual advantage of doing it the right way becomes more obvious once we look at the output of a standard fMRI analysis.</p>

<p>The following illustration shows a single subject’s statistical map of the brain activation differences during audio-visual versus audio only stimulation, using 5 different analysis methods. Panel A) shows my newly developed approach using a temporal low-pass filter at 0.2Hz, panel B) shows the same approach without the temporal low-pass filter and panel C) to E) show the comparable state of the art neuroimaging toolboxes fMRIPrep, FSL and SPM.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/blog_slice_time3.png" data-zoomable="" width="600px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #fff" /></p>

<p>What is clearly visible is that my approach in panel A), using an appropriate low-pass filter with orthogonal preprocessing leads to much stronger statistical results, even just in the single subject (i.e 1st-level) analysis.</p>

<h2 id="fmriflows">fMRIflows</h2>

<p>I packaged this improved fMRI preprocessing technique and much more into my neuroimaging toolbox <strong>fMRIflows</strong>.</p>

<p><img class="img-fluid rounded z-depth-1" src="/assets/img/research_fmriflows.png" data-zoomable="" width="400px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px; background-color: #fff" /></p>

<p>For more about fMRIflows and this approach, feel free to take a look at the <a href="/assets/pdf/Paper_2021_Notter.pdf">corresponding scientific paper</a> or an explanation of the even brother implications in <a href="/assets/pdf/Thesis_2021_PhD_Notter.pdf">my PhD thesis</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A brief visual explanation of why sub-second sampling rate in fMRI should be done correctly.]]></summary></entry></feed>