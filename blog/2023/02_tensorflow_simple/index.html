<!DOCTYPE html>
<html lang="en">

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Michael P. Notter


  | Deep Learning Fundamentals - Building Neural Networks with TensorFlow

</title>
<meta name="description" content="Personal homepage of Michael P. Notter.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/monokai.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üë®‚Äçüíª</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="https://miykael.github.io/blog/2023/02_tensorflow_simple/">


<!-- Dark Mode -->
<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://miykael.github.io/">
       <span class="font-weight-bold">Michael</span> P.  Notter
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/">
              Blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/background/">
                Background
                
              </a>
          </li>
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                Projects
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                Teaching
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      





<div class="post">

  <header class="post-header">
    <h1 class="post-title">Deep Learning Fundamentals - Building Neural Networks with TensorFlow</h1>
    <p class="post-meta">October 23, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>
      

      

    </p>
  </header>

  <article class="post-content">
    <p>In this second part of our machine learning series, we‚Äôll implement the same MNIST classification task using <a href="https://www.tensorflow.org/" target="_blank" rel="noopener noreferrer">TensorFlow</a>. While Scikit-learn excels at classical machine learning, TensorFlow shines when building neural networks. We‚Äôll see how deep learning approaches differ from traditional methods and learn the basic concepts of neural network architecture.</p>

<p>The complete code for this tutorial can be found in the <a href="/assets/scripts/02_tensorflow_simple.py">02_tensorflow_simple.py</a> script.</p>

<h3 id="why-neural-networks">Why Neural Networks?</h3>

<p>While our Scikit-learn models performed well in Part 1, neural networks offer several key advantages for image classification:</p>
<ul>
  <li>
<strong>Automatic feature learning</strong>: No need to manually engineer features</li>
  <li>
<strong>Scalability</strong>: Can handle much larger datasets efficiently</li>
  <li>
<strong>Complex pattern recognition</strong>: Especially good at finding hierarchical patterns in data</li>
  <li>
<strong>State-of-the-art performance</strong>: Currently the best approach for many computer vision tasks</li>
</ul>

<p>Let‚Äôs see these advantages in action by building our own neural network for digit classification.</p>

<p>Let‚Äôs start by importing the necessary packages:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">seaborn</span> <span class="k">as</span> <span class="n">sns</span>

<span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
<span class="kn">from</span> <span class="n">tensorflow.keras</span> <span class="kn">import</span> <span class="n">layers</span>
</code></pre></div></div>

<h2 id="1-load-and-prepare-dataset">1. Load and Prepare Dataset</h2>

<p>Unlike Scikit-learn, TensorFlow‚Äôs MNIST dataset comes in a slightly different format. We‚Äôll keep the images in their original 2D shape (28x28 pixels) since neural networks can work directly with this structure - another advantage over traditional methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Model parameters
</span><span class="n">num_classes</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># One class for each digit (0-9)
</span><span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># Height, width, and channels (1 for grayscale)
</span>
<span class="c1"># Load dataset, already pre-split into train and test set
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">mnist</span><span class="p">.</span><span class="nf">load_data</span><span class="p">()</span>

<span class="c1"># Scale pixel values to range [0,1] - this helps with training stability
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>

<span class="c1"># Add channel dimension required by Conv2D layers
# Shape changes from (samples, height, width) to (samples, height, width, channels)
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">x_train shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">x_test shape:</span><span class="sh">"</span><span class="p">,</span> <span class="n">x_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>x_train shape: (60000, 28, 28, 1)
x_test shape: (10000, 28, 28, 1)
</code></pre></div></div>

<p>Our dataset dimensions represent:</p>
<ul>
  <li>
<strong>60,000 training samples</strong>: Much larger than scikit-learn‚Äôs version for better learning</li>
  <li>
<strong>28x28 pixels</strong>: Higher resolution images than Part 1‚Äôs 8x8 grid</li>
  <li>
<strong>1 channel</strong>: Grayscale images (RGB would be 3 channels)</li>
  <li>
<strong>10,000 test samples</strong>: Large test set for robust evaluation</li>
</ul>

<p>The final dimension (1) represents the color channel. Since MNIST contains grayscale images, we only need one channel, unlike RGB images which would have 3 channels.</p>

<p>Now that the data is loaded and scaled to appropriate range, we can go ahead and create the neural network
model. Given that our input are images, let‚Äôs go ahead and train a convolutional neural network. There are
multiple ways how we can set this up.</p>

<h2 id="2-create-neural-network-model">2. Create Neural Network Model</h2>

<p>For image classification, we‚Äôll use a Convolutional Neural Network (CNN). CNNs are specifically designed to work with image data through specialized layers:</p>

<ul>
  <li>
<strong>Convolutional layers</strong>: Extract spatial features like edges, textures, and shapes</li>
  <li>
<strong>Pooling layers</strong>: Reduce spatial dimensions while preserving important features</li>
  <li>
<strong>Dense layers</strong>: Combine extracted features for final classification</li>
  <li>
<strong>Dropout layers</strong>: Prevent overfitting by randomly deactivating neurons during training</li>
</ul>

<p>There are multiple ways to define a model in TensorFlow. Let‚Äôs explore two common approaches:</p>

<h3 id="21-sequential-api">2.1. Sequential API</h3>
<p>The Sequential API is the simplest way to build neural networks - layers are stacked linearly, one after another:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define model architecture using Sequential API
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="c1"># First Convolutional Block (32 filters, each 3x3 in size, detect basic patterns)
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">),</span>

        <span class="c1"># MaxPooling2D: Reduces spatial dimensions by half while preserving features
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>

        <span class="c1"># Second Convolutional Block (64 filters, detect more complex patterns)
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>

        <span class="c1"># MaxPooling2D: Reduces spatial dimensions by half while preserving features
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>

        <span class="c1"># Flatten 3D feature maps to 1D feature vector
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>

        <span class="c1"># Dense layers for final classification
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>  <span class="c1"># Prevents overfitting by randomly dropping 50% of connections
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>  <span class="c1"># Hidden layer combines features
</span>
        <span class="c1"># Output layer for classification
</span>        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<h3 id="22-layer-by-layer-sequential-api">2.2. Layer-by-Layer Sequential API</h3>
<p>For more explicit control, we can separate each layer and activation:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># More precise and sequential approach
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">keras</span><span class="p">.</span><span class="nc">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_shape</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">32</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="n">num_classes</span><span class="p">),</span>
        <span class="n">layers</span><span class="p">.</span><span class="nc">Softmax</span><span class="p">(),</span>
    <span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<p>The two models are functionally identical, but the layer-by-layer approach offers several advantages:</p>
<ul>
  <li>Makes it easier to insert additional layers like BatchNormalization</li>
  <li>Provides more explicit activation functions</li>
  <li>Makes the data flow more transparent</li>
  <li>Allows finer control over layer parameters</li>
</ul>

<p>Next to this sequential API, there‚Äôs also a functional API.We‚Äôll explore this more flexible approach in our advanced TensorFlow tutorial, which allows for:</p>
<ul>
  <li>Multiple inputs and outputs</li>
  <li>Layer sharing</li>
  <li>Non-sequential layer connections</li>
  <li>Complex architectures like residual networks</li>
</ul>

<p>Once the model is created, you can use the <code class="language-plaintext highlighter-rouge">summary()</code> method to get an overview of the network‚Äôs architecture
and the number of trainable and non-trainable parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Model: "sequential"
_________________________________________________________________
Layer (type)                    Output Shape              Param #
=================================================================
conv2d (Conv2D)                 (None, 26, 26, 32)        320
re_lu (ReLU)                    (None, 26, 26, 32)        0
max_pooling2d (MaxPooling2D)    (None, 13, 13, 32)        0
conv2d_1 (Conv2D)               (None, 11, 11, 64)        18496
re_lu_1 (ReLU)                  (None, 11, 11, 64)        0
max_pooling2d_1 (MaxPooling2D)  (None, 5, 5, 64)          0
flatten (Flatten)               (None, 1600)              0
dropout (Dropout)               (None, 1600)              0
dense (Dense)                   (None, 32)                51232
re_lu_2 (ReLU)                  (None, 32)                0
dropout_1 (Dropout)             (None, 32)                0
dense_1 (Dense)                 (None, 10)                330
softmax (Softmax)               (None, 10)                0
=================================================================
Total params: 70,378 (274.91 KB)
Trainable params: 70,378 (274.91 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</code></pre></div></div>

<p>This summary tells us several important things:</p>
<ol>
  <li>Our model has 70,378 trainable parameters - relatively small by modern standards</li>
  <li>The input image (28x28x1) is progressively reduced in size through pooling (see the Output Shape column)</li>
  <li>The final dense layer has 10 outputs - one for each digit class</li>
  <li>Most parameters are in the dense layers, not the convolutional layers</li>
</ol>

<h2 id="3-train-tensorflow-model">3. Train TensorFlow model</h2>

<p>Before we can train the model we need to provide a few additional information:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">batch_size</code>: How many samples the model should look at once before performing the gradient descent.</li>
  <li>
<code class="language-plaintext highlighter-rouge">epochs</code>: For how many times the model should go through the full dataset.</li>
  <li>
<code class="language-plaintext highlighter-rouge">loss</code>: Which loss function the model should optimize for.</li>
  <li>
<code class="language-plaintext highlighter-rouge">metrics</code>: Which performance metrics the model should keep track of. By default this includes the loss metric.</li>
  <li>
<code class="language-plaintext highlighter-rouge">optimizer</code>: Which optimizer strategy the model should use. This could involve additional optimzation
parameters, such as the learning rate.</li>
  <li>
<code class="language-plaintext highlighter-rouge">validation_split</code> or <code class="language-plaintext highlighter-rouge">validation_data</code>: This parameter allows you to automatically split the training set
into a training and validation set (with <code class="language-plaintext highlighter-rouge">validation_split</code>) or you can also provide a specific validation
set with <code class="language-plaintext highlighter-rouge">validation_data</code>.</li>
</ul>

<p>Finding the right parameters for any of that, as well as establishing the right model architecture, is the
black arts of any deep learning practitioners. For this example, let‚Äôs just go with some proven default
parameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Training configuration
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>  <span class="c1"># Number of samples processed before model update
</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>       <span class="c1"># Number of complete passes through the dataset
</span>
<span class="c1"># Compile model with appropriate loss function and optimizer
</span><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
    <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">sparse_categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span>  <span class="c1"># Appropriate for integer labels
</span>    <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span>                        <span class="c1"># Adaptive learning rate optimizer
</span>    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">]</span>                     <span class="c1"># Track accuracy during training
</span><span class="p">)</span>

<span class="c1"># Train the model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span>  <span class="c1"># Use 10% of training data for validation
</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 1/10
422/422 [==============================] - 4s 9ms/step - loss: 0.5902 - accuracy: 0.8117 - val_loss: 0.1014 - val_accuracy: 0.9700
Epoch 2/10
422/422 [==============================] - 4s 8ms/step - loss: 0.2183 - accuracy: 0.9364 - val_loss: 0.0674 - val_accuracy: 0.9808
Epoch 3/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1663 - accuracy: 0.9512 - val_loss: 0.0499 - val_accuracy: 0.9860
Epoch 4/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1390 - accuracy: 0.9599 - val_loss: 0.0462 - val_accuracy: 0.9875
Epoch 5/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1166 - accuracy: 0.9674 - val_loss: 0.0433 - val_accuracy: 0.9888
Epoch 6/10
422/422 [==============================] - 4s 8ms/step - loss: 0.1046 - accuracy: 0.9693 - val_loss: 0.0370 - val_accuracy: 0.9902
Epoch 7/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0950 - accuracy: 0.9722 - val_loss: 0.0394 - val_accuracy: 0.9892
Epoch 8/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0891 - accuracy: 0.9742 - val_loss: 0.0400 - val_accuracy: 0.9895
Epoch 9/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0865 - accuracy: 0.9750 - val_loss: 0.0342 - val_accuracy: 0.9907
Epoch 10/10
422/422 [==============================] - 4s 8ms/step - loss: 0.0775 - accuracy: 0.9773 - val_loss: 0.0355 - val_accuracy: 0.9905
</code></pre></div></div>

<p>Let‚Äôs analyze the training progression:</p>
<ul>
  <li>
<strong>Initial Performance (Epoch 1)</strong>:
    <ul>
      <li>Training: 81.17% accuracy, loss of 0.5902</li>
      <li>Validation: 97.00% accuracy, loss of 0.1014</li>
      <li>Shows rapid initial learning</li>
    </ul>
  </li>
  <li>
<strong>Final Performance (Epoch 10)</strong>:
    <ul>
      <li>Training: 97.73% accuracy, loss of 0.0775</li>
      <li>Validation: 99.05% accuracy, loss of 0.0355</li>
      <li>Excellent convergence with validation outperforming training</li>
    </ul>
  </li>
  <li>
<strong>Key Observations</strong>:
    <ul>
      <li>Consistent improvement across epochs</li>
      <li>Lower validation loss than training loss suggests good generalization</li>
      <li>Final accuracy exceeds our Scikit-learn model from Part 1</li>
      <li>No signs of overfitting as validation metrics remain stable</li>
    </ul>
  </li>
</ul>

<h2 id="4-model-investigation">4. Model investigation</h2>

<p>If we stored the <code class="language-plaintext highlighter-rouge">model.fit()</code> output in a <code class="language-plaintext highlighter-rouge">history</code> variable, we can easily access and visualize the different
model metrics during training.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Store history in a dataframe
</span><span class="n">df_history</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">)</span>

<span class="c1"># Visualize training history
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">df_history</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">df_history</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">)].</span><span class="nf">plot</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Loss during training</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">df_history</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">df_history</span><span class="p">.</span><span class="n">columns</span><span class="p">.</span><span class="nb">str</span><span class="p">.</span><span class="nf">contains</span><span class="p">(</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">)].</span><span class="nf">plot</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="sh">"</span><span class="s">Accuracy during training</span><span class="sh">"</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [#]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Epoch [#]</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Loss</span><span class="sh">"</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">set_ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/02_tensorflow_training_history.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px"></p>
<div class="caption">
    Figure 1: Training metrics over time showing model loss (left) and Mean Absolute Error (right) for both training and validation sets. The logarithmic scale helps visualize improvement across different magnitudes.
</div>

<p>Once the model is trained we can also compute its score on the test set. For this we can use the <code class="language-plaintext highlighter-rouge">evaluate()</code>
method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">score</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test loss:     </span><span class="si">{</span><span class="n">score</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Test accuracy: </span><span class="si">{</span><span class="n">score</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Test loss:     0.032
Test accuracy: 98.93%
</code></pre></div></div>

<p>And if you‚Äôre interested in the individual predictions, you can use the <code class="language-plaintext highlighter-rouge">predict()</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_pred</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(10000, 10)
</code></pre></div></div>

<p>Given that our last layer uses a softmax activation, we actually don‚Äôt get just the class label back, but the
probability score for each class. To get to the class prediction, we therefore need to apply an argmax routine.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transform class probabilities to prediction labels
</span><span class="n">predictions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Create confusion matrix
</span><span class="n">cm</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">math</span><span class="p">.</span><span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="c1"># Visualize confusion matrix
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="p">.</span><span class="nf">heatmap</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">square</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="sh">'</span><span class="s">d</span><span class="sh">'</span><span class="p">,</span> <span class="n">cbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<div style="text-align: center">
    <img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/02_tensorflow_confusion_matrix.png" data-zoomable="" width="500px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px">
</div>
<div class="caption">
    Figure 2: Confusion matrix showing model predictions across all digit classes. The strong diagonal pattern indicates high accuracy across all digits.
</div>

<h2 id="5-model-parameters">5. Model parameters</h2>

<p>And if you‚Äôre interested in the model parameters of the trained neural network, you can directly access them
via <code class="language-plaintext highlighter-rouge">model.layers</code>. One advantage of neural networks is their ability to learn hierarchical features. Let‚Äôs examine what our first convolutional layer learned:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Extract first hidden convolutional layers
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Transform the layer weights to a numpy array
</span><span class="n">weights</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">.</span><span class="n">weights</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nf">numpy</span><span class="p">()</span>

<span class="c1"># Visualize the 32 kernels from the first convolutional layer
</span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="nf">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ravel</span><span class="p">(</span><span class="n">axs</span><span class="p">)</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">axs</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">set_title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Kernel </span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">imshow</span><span class="p">(</span><span class="n">weights</span><span class="p">[...,</span> <span class="n">idx</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="sh">'</span><span class="s">binary</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="nf">axis</span><span class="p">(</span><span class="sh">'</span><span class="s">off</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img class="img-fluid rounded z-depth-1" src="/assets/ex_plots/02_tensorflow_conv_kernels.png" data-zoomable="" width="800px" style="padding-top: 20px; padding-right: 20px; padding-bottom: 20px; padding-left: 20px"></p>
<div class="caption">
    Figure 3: Visualization of the 32 convolutional kernels learned by the first layer. Each 3x3 kernel acts as a feature detector, learning to identify basic patterns like edges, corners, and textures that are useful for digit recognition.
</div>

<h3 id="common-deep-learning-pitfalls">Common Deep Learning Pitfalls</h3>
<p>When starting with TensorFlow and neural networks, watch out for these common issues:</p>

<p><strong>Data Preparation</strong></p>
<ul>
  <li>(Almost) always scale input data (like we did with <code class="language-plaintext highlighter-rouge">/255.0</code>)</li>
  <li>Check for missing or invalid values</li>
  <li>Ensure consistent data types</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of proper data preparation
</span><span class="n">x_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">x_test</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="sh">'</span><span class="s">float32</span><span class="sh">'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span>
</code></pre></div></div>

<p><strong>Model Architecture</strong></p>
<ul>
  <li>Start simple, add complexity only if needed</li>
  <li>Match output layer to your task (softmax for classification)</li>
  <li>Use appropriate layer sizes</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example of clear, progressive architecture
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
    <span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">)</span>  <span class="c1"># 10 classes
</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>Training Issues</strong></p>
<ul>
  <li>Monitor training metrics (loss not decreasing)</li>
  <li>Watch for overfitting (validation loss increasing)</li>
  <li>Use appropriate batch sizes</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Add validation monitoring during training
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</code></pre></div></div>

<p><strong>Memory Management</strong></p>
<ul>
  <li>Clear unnecessary variables</li>
  <li>Use appropriate data types</li>
  <li>Watch batch sizes on limited hardware</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Free memory after training
</span><span class="kn">import</span> <span class="n">gc</span>
<span class="n">gc</span><span class="p">.</span><span class="nf">collect</span><span class="p">()</span>
<span class="n">keras</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="nf">clear_session</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="summary-and-next-steps">Summary and Next Steps</h2>

<p>In this tutorial, we‚Äôve introduced neural networks using TensorFlow:</p>
<ul>
  <li>Building a CNN architecture</li>
  <li>Training with backpropagation</li>
  <li>Monitoring learning progress</li>
  <li>Visualizing learned features</li>
</ul>

<p>Our neural network achieved comparable accuracy to our Scikit-learn models (~99%), but this time on images with a higher resolution with the potential for even better performance through further optimization.</p>

<p><strong>Key takeaways:</strong></p>
<ol>
  <li>Neural networks can work directly with structured data like images</li>
  <li>Architecture design is crucial for good performance</li>
  <li>Training requires careful parameter selection</li>
  <li>Monitoring training helps detect problems early</li>
  <li>Visualizing learned features provides insights into model behavior</li>
</ol>

<p>In Part 3, we‚Äôll explore more advanced machine learning concepts using Scikit-learn, focusing on regression problems and complex preprocessing pipelines.</p>

<p><a href="/blog/2023/01_scikit_simple">‚Üê Previous: Getting Started</a> or
<a href="/blog/2023/03_scikit_advanced">Next: Advanced Machine Learning ‚Üí</a></p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    ¬© Copyright 2025 Michael P. Notter.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme.

    
    
  </div>
</footer>



  </body>

  <!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

  
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-126030922-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-126030922-1');
</script>






</html>
